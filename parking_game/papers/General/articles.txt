ML:

1) https://www.coursera.org/articles/types-of-machine-learning

2) https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861

3) https://www.sas.com/en_gb/insights/articles/analytics/machine-learning-algorithms.html


Comparison of RL algorithms:

1) https://smartlabai.medium.com/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc

2) https://www.v7labs.com/blog/deep-reinforcement-learning-guide

3) https://cse.buffalo.edu/~avereshc/rl_fall20/Comparison_of_RL_Algorithms_vvelivel_sudhirya.pdf

How do you compare different deep reinforcement Learning (RL) algorithms like DQN, PPO, A3C, etc.?
Comparing different Deep Reinforcement Learning (RL) algorithms like Deep Q-Network (DQN), Proximal Policy Optimization (PPO), Asynchronous Advantage Actor-Critic (A3C), and others requires considering various factors such as their learning mechanisms, performance, computational requirements, and suitability for different types of problems. Here's a comparative overview of these algorithms:

1. Deep Q-Network (DQN)

Learning Mechanism: Combines Q-Learning with deep neural networks. Uses a technique called experience replay and target networks to stabilize learning.
Strengths: Good at learning policies in high-dimensional spaces using raw pixels as input.
Limitations: Struggles with continuous action spaces and is less sample-efficient.
Use Cases: Effective in discrete action spaces, like playing Atari games.

2. Proximal Policy Optimization (PPO)

Learning Mechanism: A policy gradient method that optimizes a surrogate objective function to prevent the policy from changing too much at each iteration, enhancing training stability.
Strengths: Strikes a balance between sample efficiency, ease of implementation, and training stability. Works well in both discrete and continuous action spaces.
Limitations: Can be slower to converge in environments with very high-dimensional action spaces.
Use Cases: Suitable for a wide range of problems, from robotic control to playing complex games like Dota 2.

3. Asynchronous Advantage Actor-Critic (A3C)

Learning Mechanism: Uses multiple agents exploring different copies of the environment simultaneously. It updates a global network asynchronously, leading to diverse experience and faster learning.
Strengths: More robust to hyperparameter changes. Can learn more diverse policies due to asynchronous updates.
Limitations: Asynchronous updates can make training less stable and harder to debug.
Use Cases: Useful in complex environments where exploration is crucial.

4. Trust Region Policy Optimization (TRPO)

Learning Mechanism: A policy gradient method designed to take the largest possible step in improving the policy while remaining within a certain trust region to avoid performance collapse.
Strengths: Provides monotonic policy improvement guarantees.
Limitations: Computationally expensive and complex to implement.
Use Cases: Effective in environments where stability and reliability of policy improvement are critical.
Comparison Factors

Sample Efficiency: How many samples (state-action-reward tuples) the algorithm needs to learn an effective policy.
Stability and Reliability: The consistency of the learning process and the algorithmâ€™s robustness to hyperparameter settings.
Scalability: How well the algorithm performs as the complexity of the environment increases.
Computational Requirements: The computational resources needed for training, including memory and processing power.
Suitability for Task Types: Whether the algorithm is better suited for discrete or continuous action spaces, and the complexity of the task.


Pytorch vs TensorFlow

https://builtin.com/data-science/pytorch-vs-tensorflow