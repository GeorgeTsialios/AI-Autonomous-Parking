~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					RANDOM CAR SPAWN, FIXED SPOT SPAWN (3 ή 8), INSTANT PARKING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 1 <- PPO 47)

Δοκιμάζω τον TD3 στο μοντέλο 47 του PPO. Μετά από μόλις 1Μ steps, η γραφική των rewards συγκλίνει σε αρνητική τιμή ενώ το success rate είναι μικρό.

Training 2 <- SAC 4)

Δοκιμάζω τον TD3 στο μοντέλο 4 του SAC. Μετά από 4.5Μ steps, η γραφική των rewards συγκλίνει στην τιμή 5.600, ενώ το success rate του πράκτορα τείνει στο 100%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι παρκάρει σχεδόν πάντα και προτιμάει να παρκάρει προς τα μπροστά παρά με την όπισθεν. Αυτό είναι λογικό, καθώς επιβραβεύεται με +1000, όταν παρκάρει με θετική ταχύτητα.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					RANDOM CAR SPAWN, RANDOM SPOT SPAWN, INSTANT PARKING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 4 <- 2)

(4Α) Εκπαιδεύω το μοντέλο των 4.5Μ steps της εκπαίδευσης 2 σε περιβάλλον με τελείως random spot spawn. Όμως, μετά από 1.5M steps βλέπω ότι η γραφική των rewards συγκλίνει σε αρνητική τιμή, ενώ η γραφική του success rate συγκλίνει στο 20%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι παρκάρει μόνο όταν η ελεύθερη θέση είναι η 3 ή η 8 (εξου και το 20%). Στις υπόλοιπες περιπτώσεις, ο πράκτορας απλά στέκεται ακίνητος (4Β) Αυξάνω το punishment for standing still και Εκπαιδεύω το μοντέλο των 4.5Μ steps της εκπαίδευσης 2. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πάλι μένει συχνά ακίνητος, άρα η αύξηση του punishment δεν βοήθησε. Μάλιστα, παρατηρώ ότι παρκάρει και σε άλλες θέσεις, όταν είναι κοντά σε αυτές. Άρα, επαληθεύεται ότι δεν παίζει τόσο ρόλο το fixed spot spawn, αφού το αμάξι δέχεται ως είσοδο τα offset x και y και όχι τις συντεταγμένες της θέσης. Όμως, το θέμα είναι ότι όταν το αμάξι κάνει spawn μακριά από τη θέση, βλέπει μεγάλες τιμές για τα offset x, y, που δεν έχει συναντήσει στην προηγούμενη εκπαίδευση αφού η θέση ήταν κεντρικά στον χάρτη, με αποτέλεσμα να κολλάει.

Training 5 <-2)

Εκπαιδεύω με τη reward function της εκπαίδευσης 2 σε περιβάλλον με τελείως random spot spawn. Μετά από μόλις 3Μ steps, το success rate του πράκτορα συγκλίνει κοντά στο 1. Εξετάζοντας τον πράκτορα, παρατηρώ ότι παρκάρει σχεδόν πάντα σε όλες τις θέσεις και προς τα εμπρός και με την όπισθεν. (5Β) Αφαιρώ το reward +1000 για όταν παρκάρει ο πράκτορας με θετική ταχύτητα (ώστε να εξισορροπηθεί η συχνότητα παρκαρίσματος προς τα εμπρός και με την όπισθεν) και μειώνω το reward for terminated (ώστε να μάθει ο πράκτορας να σέβεται περισσότερο τα collisions). Μετά από 4Μ steps, το success rate συγκλίνει στο 60%. Εξετάζοντας τον πράκτορα, παρκάρει συχνά χωρίς να τρακάρει καθόλου! Ωστόσο, αρκετές φορές υιοθετεί την πολιτική του να κάνει τον γύρο του χάρτη. (5C) Αυξάνω λίγο το reward for terminated, ώστε να αυξηθεί το success rate. Μετά από 4Μ steps, το success rate συγκλίνει κοντά στο 1. Όμως εξετάζοντας τον πράκτορα, παρατηρώ ότι η πολιτική του μοιάζει με αυτήν του 5Α, δηλ. συγκρούεται κάποιες φορές με αντικείμενα προκειμένου να παρκάρει. (5D <- 5) Αφαιρώ το reward +1000 για όταν παρκάρει ο πράκτορας με θετική ταχύτητα (ώστε να εξισορροπηθεί η συχνότητα παρκαρίσματος προς τα εμπρός και με την όπισθεν) και αυξάνω το punishment for collisions (-1000, ώστε να μάθει ο πράκτορας να σέβεται περισσότερο τα collisions). Ωστόσο, παρατηρώ ότι ο πράκτορας μένει συχνά ακίνητος ή κάνει τον γύρο της πίστας, δηλ. αποφεύγει πάντα τις συγκρούσεις, αλλά δεν προσπαθεί να παρκάρει. Το ίδιο συνέβη και όταν δοκίμασα punishment -500, -250, -175, -125. (5Ε) Μειώνω το punishment for collisions σε -50.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
					RANDOM CAR SPAWN (ΟΠΟΙΟΔΗΠΟΤΕ RECTANGLE), RANDOM SPOT SPAWN, INSTANT PARKING
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 6 <-2)

Εκπαιδεύω με τη reward function της εκπαίδευσης 2 σε περιβάλλον με τελείως random car spawn (και στα υπόλοιπα rectangles). Παρατηρώ ότι η γραφική του success rate αρχικά αυξάνεται και φτάνει την τιμή 60%. Εξετάζοντας τον πράκτορα, βλέπω ότι παρκάρει ακόμα και σε περιπτώσεις όπου η θέση είναι αντίθετα του car spawn, όμως η πολιτική του πράκτορα μοιάζει να μην είναι ακόμα σταθερή. Ωστόσο στη συνέχεια, το success rate μειώνεται και συγκλίνει στην τιμή 10%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κινείται συνεχώς μπρος-πίσω, ώστε να αποφύγει κομμάτι του punishment for moving too slow.

Training 7 <-2)

Αλλάζω το περιβάλλον ώστε να επιλέγονται όλα τα spawn rects, αλλά όταν επιλεγεί το πάνω, η ελεύθερη θέση να είναι μία από τις πάνω και όταν επιλεγεί το κάτω, η ελεύθερη θέση να είναι μία από τις κάτω. Μετά από 3Μ steps, η γραφική του success rate συγκλίνει στην τιμή 65%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι καταφέρνει συχνά να παρκάρει, όμως κάποιες φορές πηγαίνει μπρος-πίσω ή συγκρούεται διαδοχικά με τοίχους.

Training 8 <-2)

Αλλάζω τα spawn rects, χωρίζοντας το κάθε πλαϊνό spawn rect σε 2, ένα πάνω κι ένα κάτω. Όταν επιλεγεί ένα πάνω, η ελεύθερη θέση θα είναι μία από τις πάνω κοκ. Οι γραφικές των rewards και του success rate έχουν αρχικά την αναμενόμενη θετική κλίση, όμως στη συνέχεια οι τιμές τους μειώνονται και το success rate συγκλίνει στην τιμή 20%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι επαναλαμβάνει διαδοχικά ζευγάρια κινήσεων μπρος-πίσω κι έτσι δεν αλλάζει τη θέση του.

Training 9)

Εφαρμόζω FrameSkip = 4, ώστε να μην μπορεί ο πράκτορας να μένει ακίνητος πηγαίνοντας μπρος-πίσω σε κάθε frame. Μετά από μόλις 1Μ steps, η γραφική του success rate συγκλίνει στην τιμή 1. Εξετάζοντας τον πράκτορα, παρατηρώ ότι παρκάρει σχεδόν κάθε φορά.

Training 10 <-6)

Εφαρμόζω FrameSkip = 4 σε περιβάλλον με τελείως random car spawn. Μετά από 2Μ steps, η γραφική του success rate συγκλίνει στην τιμή 1. Εξετάζοντας τον πράκτορα, παρατηρώ ότι παρκάρει σχεδόν κάθε φορά. Ωστόσο, επιλέγει να διανύει μεγάλες αποστάσεις με την όπισθεν, ενώ συγκρούεται συχνά με άλλα αντικείμενα.

Training 11 <-10)

Μειώνω το reward for terminated σε -500, ώστε να μάθει ο πράκτορας να σέβεται τις συγκρούσεις. Ωστόσο, μετά από 1Μ steps, η γραφική των rewards του πράκτορα συγκλίνει σε αρνητική τιμή, ενώ το success rate είναι κοντά στο 0. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει υιοθετήσει την πολιτική του να κάνει τον γύρο του χάρτη.

Training 12 <-10)

Curriculum learning. Εκπαιδεύω το μοντέλο των 3.5Μ steps της εκπαίδευσης 10, αυξάνοντας το punishment for collision σε -500. Εκπαιδεύουμε για ακόμα 4Μ steps. Η γραφική του success rate γρηγορά συγκλίνει στο 1, ενώ αυτή του reward συγκλίνει και πάλι στο 5000, όμως εμφανίζει αρκετές αυξομειώσεις, οι οποίες οφείλονται στο μεγάλο punishment για τις συγκρούσεις του πράκτορα. Εξετάζοντας τον πράκτορα, παρατηρώ ότι και πάλι παρκάρει κάθε φορά και μάλιστα συγκρούεται σπάνια, χωρίς όμως να έχει εξαλείψει πλήρως τις συγκρούσεις. (12B) Εκπαιδεύω με την ίδια reward function, αλλά από την αρχή. Μετά από 6Μ steps, το success rate του πράκτορα συγκλίνει στην τιμή 1. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κινείται πιο αργά και συγκρούεται πιο σπάνια σε σχέση με πριν, αλλά παρκάρει μόνο προς τα εμπρός. (12C) Αφαιρώ το reward for parking moving forward, ώστε να εξισορροπηθεί η συχνότητα παρκαρίσματος προς τα εμπρός και με την όπισθεν. Μετά από 6Μ steps, το success rate του πράκτορα συγκλίνει στην τιμή 1. Εξετάζοντας τον πράκτορα, παρατηρώ ότι τώρα παρκάρει μόνο με την όπισθεν.

Training 13)

Εκπαιδεύω με την πολύ απλουστευμένη reward function: big reward for parking, small punishment for colliding, small reward for moving. Ωστόσο, μετά από 2Μ steps, η γραφική του success rate συγκλίνει στην τιμή 0. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει υιοθετήσει την πολιτική του να στέκεται ακίνητος.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
			   RANDOM Car Spawn (ΟΠΟΙΟΔΗΠΟΤΕ RECTANGLE), RANDOM Spot Spawn, NORMAL Parking
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 14 <-12)

Πλέον παίζει ρόλο και το κομμάτι 'elif inside spot:' της reward function. (14Α) Μετά από 3Μ steps, παρατηρώ ότι η γραφική του average reward συγκλίνει σε αρνητική τιμή, ενώ η γραφική του success rate συγκλίνει στο 0. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει υιοθετήσει την πολιτική του να κάνει τον γύρο του χάρτη. (14B) Αυξάνω το reward for being inside the spot (από 2 σε 50), ώστε να ωθήσω τον πράκτορα να προσπαθήσει να παρκάρει, παρά το μεγάλο punishment για τα collisions. Ωστόσο, μετά από 2Μ steps, οι γραφικές είναι πανομοιότυπες με πριν και εξετάζοντας τον πράκτορα, παρατηρώ την ίδια συμπεριφορά. (14C) Μειώνω το punishment for collision από -500 σε -10, ώστε να ωθήσω τον πράκτορα να παρκάρει, αντί να κάνει τον γύρο του χάρτη. Ωστόσο, τα αποτελέσματα είναι ακριβώς ίδια με πριν. Μάλιστα, σταματάω την εκπαίδευση στα μόλις 1Μ steps, καθώς η γραφική των rewards δείχνει ελάχιστες αυξομειώσεις, που σημαίνει ότι η συμπεριφορά του πράκτορα είναι σταθερή. (14D) Προχωράω σε κινήσεις και απλοποιήσεις ώστε να ανακαλύψει ο πράκτορας στην αρχή της εκπαίδευσης το παρκάρισμα, προτού υιοθετήσει πλήρως μία συμπεριφορά. Αυξάνω το reward for being inside the spot, αφαιρώ τελείως το punishment for collision και αλλάζω σε 10 τα frames που απαιτείται το αμάξι να έχει μικρή ταχύτητα για να θεωρηθεί επιτυχές παρκάρισμα. Ωστόσο, μετά από 2Μ steps, τα αποτελέσματα είναι ίδια με πριν.  

Training 15) 

(15Α <- 14B) Δοκιμάζω Curriculum Learning με το μοντέλο των 4Μ steps της εκπαίδευσης 12 στη reward function του 14Β. Μετά από 1Μ steps, παρατηρώ πως έχουμε μία περίπτωση reward hacking. Συγκεκριμένα, η γραφική των rewards έχει αυξηθεί πολύ πέραν του 5000 (reward for terminated), ενώ το success rate βρίσκεται στο 25%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει αναπτύξει την πολιτική να μπαίνει στη θέση πάρκινγκ και να κινείται εμπρός-πίσω, ώστε να συγκεντρώνει συνεχώς τα rewards for being inside the spot. Προκύπτει πως η πολιτική αυτή συγκεντρώνει συνολικά μεγαλύτερο total reward από το να παρκάρει στη θέση. 
(15Β) Μειώνω το reward for being inside the spot και αυξάνω το reward for terminated για να σταματήσει το reward hacking. Μάλιστα, παρατήρησα πως πριν, ο πράκτορας έμενε ακίνητος σε κάποια επεισόδια. Για αυτό αυξάνω το punishment for standing still και μειώνω το punishment for collisions, ώστε να ωθήσω τον πράκτορα να κινείται. Ξανά, εφαρμόζω Curriculum Learning με το μοντέλο των 4Μ steps της εκπαίδευσης 12. Μετά από μόλις 500Κ steps, το success rate συγκλίνει στο 100%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει μάθει να παρκάρει επιτυχώς, όμως κάθε φορά συγκρούεται πρώτα με τον κήπο του χάρτη. 
(15C) Αλλάζω τους κανόνες του παιχνιδιού, ώστε να πρέπει να μείνει εντελώς ακίνητος ο πράκτορας για 20 frames, προκειμένου να θεωρηθεί επιτυχές παρκινγκ. Ξανά, εφαρμόζω Curriculum Learning με το μοντέλο των 4Μ steps της εκπαίδευσης 12. Μετά από 1Μ steps, το success rate τείνει στο 100%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι επιτυχώς κάθε φορά, πραγματοποιώντας μία σύγκρουση με τον κήπο του χάρτη, όπως πριν. 
(15D) Αυξάνω τα απαιτούμενα frames σε 40, ενώ αυξάνω το punishment for collision από -10 σε -500. Ξανά, εφαρμόζω Curriculum Learning με το μοντέλο των 4Μ steps της εκπαίδευσης 12. Μετά από 700K steps, το success rate συγκλίνει στο 60%. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κάποιες φορές παρκάρει επιτυχώς χωρίς καμία σύγκρουση, αλλά άλλες φορές φοβάται να μπει στη θέση. 
(15Ε) Εφαρμόζω μόνο την αλλαγή των απαιτούμενων frames σε 40. Ξανά, εφαρμόζω Curriculum Learning με το μοντέλο των 4Μ steps της εκπαίδευσης 12. Ήδη μετά από 400Κ steps, το success rate τείνει στο 1. Εξετάζοντας τον πράκτορα, παρατηρώ ότι συγκρούεται πρώτα με τον κήπο και μετά παρκάρει. 
(15F) Αυξάνω το punishment for collision από -10 σε -1000 και εφαρμόζω Curriculum Learning με το μοντέλο των 1.5Μ steps της εκπαίδευσης 15Ε. Μετά από 500Κ steps, το success rate τείνει στο 1, ενώ η γραφική των rewards συγκλίνει στην τιμή 10000, έχοντας όμως πολλές βυθίσεις. Οι βυθίσεις αυτές οφείλονται στη μεγάλη τιμή του punishment for collision κι έτσι καταλαβαίνουμε πως οι συγκρούσεις δεν έχουν εξαλειφθεί πλήρως. Πράγματι, εξετάζοντας τον πράκτορα, παρατηρούμε πως όταν παρκάρει με την όπισθεν, ο πράκτορας μειώνει την ταχύτητα του και παρκάρει χωρίς να συγκρούεται, ωστόσο όταν παρκάρει προς τα εμπρός η πολιτική του είναι ίδια με πριν, δηλ. συγκρούεται πρώτα με τον κήπο. 
(15G) Αυξάνω το punishment for collision από -1000 σε -3000 και εφαρμόζω Curriculum Learning με το μοντέλο των 1Μ steps της εκπαίδευσης 15F. Ήδη μετά από 1Μ steps, το success rate συγκλίνει στο 100%. Η γραφική των rewards συγκλίνει στην τιμή 10000, έχοντας όμως ξανά πολλές βυθίσεις, επομένως και πάλι οι συγκρούσεις δεν έχουν εξαλειφθεί πλήρως. Ωστόσο, εξετάζοντας τον πράκτορα, παρατηρώ πως πλέον έχει μάθει να παρκάρει και προς τα εμπρός χωρίς να συγκρούεται και πλέον οι συγκρούσεις είναι σπάνιες.
(15H) Αυξάνω το punishment for collision από -3000 σε -6000 και εφαρμόζω Curriculum Learning με το μοντέλο των 2Μ steps της εκπαίδευσης 15G. Παρατηρώ πως πλέον ο πράκτορας δεν συγκρούεται ποτέ με άλλα αντικείμενα. Ωστόσο, το success rate του είναι μικρότερο σε σχέση με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι σε ορισμένα επεισόδια, προκειμένου να μην ρισκάρει να συγκρουστεί, ο πράκτορας επιλέγει να μείνει ακίνητος.