Training 1)

Τα 1000 επεισόδια (600.000 steps) πήραν 25min στη CPU. Ωστόσο, παρατήρησα ότι ο πράκτορας των 600.000 steps κουνιόταν αργά. Αυτό μάλλον οφείλεται στο ότι ξέχασα το punishment για όταν κινείται αργά χωρίς να έχει παρκάρει για όταν η ταχύτητα του == 0 -το οποίο συμβαίνει σπάνια. Τώρα το αλλάζω, ώστε αυτό το punishment να εφαρμόζεται όταν η ταχύτητα είναι < 10.

Training 2)

Εκπαιδεύσαμε για 3.500.000 steps (5.800 επεισόδια) και χρειάστηκαν 2.5 ώρες. Ωστόσο, βλέπουμε πως η γραφική των average rewards δεν έχει βελτιωθεί καθόλου.

Training 3)

Έκανα κανονικοποίηση της εισόδου και των rewards. Στα 1000 επεισόδια (600.000 steps), η γραφική των average rewards είναι παρόμοια με αυτήν του Training 2. Επίσης, παρατηρώ ότι η γραφική στο τέλος έχει σταθεροποιηθεί στην τιμή -5.400. Αξιολογώντας τον πράκτορα των 600.000 steps, είδα ότι απλά μένει ακίνητος σε όλο το επεισόδιο, κι έτσι προκύπτει reward: -9 * 600 = -5.400.

Training 4)

Εφάρμοσα MaxAndSkip = 4, δηλ. ο πράκτορας παίρνει απόφαση κάθε 4 frames. Παρατηρώ ότι πάλι το reward σταθεροποιήθηκε στο -5.400. Ωστόσο, είδα ότι η εκπαίδευση διήρκησε περισσότερη ώρα (συγκεκριμένα τα 600.000 steps χρειάστηκαν 34'). Έψαξα τι ακριβώς κάνει η κλάση MaxAndSkipEnv. Αφενός κάνει αυτό που θέλω, δηλ. ο πράκτορας επαναλαμβάνει την ίδια action για 4 frames, το οποίο είναι επιθυμητό για να μην διαλέγει πάρα πολύ συχνά διαφορετικά actions (παίρνει υπερβολικά γρήγορα αποφάσεις), να έχει πιο ομαλή οδήγηση. Έτσι, επιταχύνεται κι η εκπαίδευση. Ωστόσο, η κλάση αυτή κάνει επίσης Max-Pooling Over Frames, δηλ. επιστρέφει τη μέγιστη τιμή του κάθε pixel σε αυτά τα skipped frames. Αυτό είναι κάτι επιθυμητό όταν έχουμε περιβάλλον Atari, αλλά όχι στην περίπτωση μου, όπου τα observations δεν είναι frames αλλά vectors. Για αυτό κατέβασα τον κώδικα της κλάσης και τον άλλαξα, ώστε να αφήσω μόνο την 1η λειτουργία (άρα την ονόμασα SkipEnv class).

Training 5)

Θα εκπαιδεύσω τον πράκτορα με την custom SkipEnv class μου. Παρατηρώ ότι η γραφική των average rewards με την SkipEnv class μου είναι παρόμοια με αυτήν με την MaxAndSkipEnv και συγκλίνει στην τιμή -5.400. Αυτό είναι λογικό καθώς πρέπει να αλλάξουμε την reward function για να αλλάξει αυτό. Ωστόσο, δεν παρατηρήθηκε speed up μεταξύ της SkipEnv class και της MaxAndSkipEnv, δηλ. τα fps τους είναι ίδια κι έτσι χρειάστηκαν πάλι 34' για τα 600.000 steps. Παρόλα αυτά, και οι δύο κλάσεις συγκλίνουν πολύ γρηγορότερα από την περίπτωση χωρίς frame skip (PPO_3). Για αυτό, θα τις προτιμήσουμε στη συνέχεια.

----------------------------------------------------------------------------------------------------------------------------------------
Προσπάθησα να επιταχύνω τη διαδικασία της εκπαίδευσης, ενισχύωντας το hardware πάνω στο οποίο εκπαιδεύονται τα μοντέλα. Συγκεκριμένα, η βιβλιοθήκη stableBaselines3 χρησιμοποιεί Vectorized Environments: Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Vectorized environments in reinforcement learning offer several advantages:

Parallelism: By running multiple instances of an environment in parallel, vectorized environments can significantly speed up the data collection process. This is particularly beneficial in reinforcement learning, where algorithms often require large amounts of data to learn effectively.

Efficiency: Vectorized environments can make better use of hardware resources, such as multi-core CPUs or GPUs, by distributing the workload across multiple cores or processing units. This leads to more efficient use of the available computational power.

Υπάρχουν 2 ειδών vectorized environments στη βιβλιοθήκη stable-baselines3:

-DummyVecEnv is a simple implementation of VecEnv (VecEnv is an abstract class that serves as the base for all vectorized environments in Gym) that runs each environment in a sequential manner within a single process. Despite its name, it's useful for testing and development because it simplifies debugging by avoiding the complexities of multiprocessing. However, because it runs environments sequentially, it does not offer the same performance benefits as parallelized implementations when it comes to training time. It is preferrable in the case of a simple environment such as Cartpole-v1, because then the overhead of multiprocess outweighs the environment computation time.
-SubprocVecEnv is an implementation of VecEnv that runs each environment in its own subprocess. This allows for true parallelism, taking advantage of multiple CPU cores, which can significantly speed up the training process. However, the use of multiple processes can make debugging more challenging and may introduce overhead due to inter-process communication.

Προσπάθησα να χρησιμοποιήσω SubprocVecEnv αντί του default DummyVecEnv, για να αξιοποιήσω τους 4 CPU cores του υπολογιστή μου, ωστόσο προέκυψε το εξής πρόβλημα: SubprocVecEnv creates separate processes for each environment instance to speed up training by running them in parallel. Each process needs to serialize (pickle) the environment to send it to the child processes. However, if your environment includes non-serializable objects like pygame.surface.Surface, this process fails. Για να λυθεί αυτό το πρόβλημα θα έπρεπε να αλλάξω σημαντικά τον κώδικα για το ίδιο το παιχνίδι και για αυτό δεν προχώρησα σε αυτήν την κατεύθυνση.


Προσπάθησα να εκπαιδεύσω το μοντέλο χρησιμοποιώντας τη GPU του υπολογιστή μου και όχι τη CPU. Γενικά, η εκπαίδευση αλγόριθμών μηχανικής μάθησης επιταχύνεται όταν χρησιμοποποιείται η GPU, καθώς προσφέρει significant improvements in computational efficiency, parallel processing capabilities, and overall training speed. Πιο συγκεκριμένα, πολλοί RL algorithms, especially those involving deep reinforcement learning (e.g., Deep Q-Networks (DQN), Proximal Policy Optimization (PPO)), rely heavily on deep neural networks. Training these networks involves a large number of matrix multiplications and other operations that benefit from GPU acceleration.
Για να χρησιμοποιήσουμε τη GPU κατά την εκπαίδευση, έπρεπε πρώτα να εγκαταστήσουμε το CUDA Toolkit της NVidia.Το CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and application programming interface (API) model created by NVIDIA. It enables developers to use NVIDIA GPUs for general purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units). Στη συνέχεια, έπρεπε να εγκαταστήσουμε τη cuDNN library της NVidia, which provides optimized implementations for deep learning operations. Τέλος, έπρεπε να κατεβάσουμε άλλη έκδοση της PyTorch (σε αυτήν τη βιβλιοθήκη είναι γραμμένη η stable-baselines3), η οποία να υποστηρίζει GPU Support.
----------------------------------------------------------------------------------------------------------------------------------------

Training 6)

Εκπαίδευσα χρησιμοποιώντας τη GPU μου για 1000 επεισόδια, ώστε να δω το speed up στον χρόνο εκπαίδευσης σε σχέση με πριν. Προφανώς η γραφική είναι σχεδόν πανομοιότυπη με πριν, αφού δεν άλλαξα τίποτα στο μοντέλο. Προς μεγάλη μου έκπληξη, προέκυψε πως η εκπαίδευση στη GPU ήταν πιο αργή (πήρε 49') από την εκπαίδευση στη CPU (πήρε 34'). Αυτό ίσως οφείλεται στο ότι το λάπτοπ μου διαθέτει μία παρωχημένη GPU (την NVIDIA GeForce MX110). Ευτυχώς, αλλάζοντας την παράμετρο device της κλάσης PPO ορίζουμε σε ποια συσκευή επιθυμούμε να πραγματοποιηθεί η εκπαίδευση.

Training 7)

Μέχρι στιγμής, το average reward του πράκτορα συγκλίνει στην τιμή -5.400. Αυτό σημαίνει ότι ο πράκτορας επιλέγει να μένει ακίνητος από την αρχή μέχρι το τέλος του επεισοδίου. Έτσι, παίρνει το punishment για το ότι δεν έχει παρκάρει (-7) και το punishment για το ότι έχει μικρή ταχύτητα (-2). Όμως, έτσι αποφεύγει το τεράστιο συγκριτικά punishment για τα collisions (-1000). Άρα, ο πράκτορας φοβάται τόσο μην συγκρουστεί με κάτι, που επιλέγει να μην παίξει καθόλου. Όμως έτσι, ο πράκτορας έχει κολλήσει σε αυτό το τοπικό μέγιστο (local optimum). Για αυτό ΜΕΙΩΣΑ το punishment της σύγκρουσης σε 100. Βλέπω ότι με 600.000 steps η γραφική συγκλίνει και πάλι στο -5400. Άρα, σταματάω την εκπαίδευση.

----------------------------------------------------------------------------------------------------------------------------------------
Επιλέγω smoothing = 0.99 για τις καμπύλες των average rewards. Έτσι, βλέπω την πιο εξομαλυμένη μορφή των καμπυλών η οποία θεωρώ μας δίνει την πιο χρήσιμη πληροφορία σχετικά με την τάση της γραφικής, χωρίς να αποπροσανατολιζόμαστε από τις συχνές βυθίσεις και μέγιστα. Άλλωστε, η κανονική μορφή της γραφικής (χωρίς smoothing) φαίνεται με αχνό χρώμα.
----------------------------------------------------------------------------------------------------------------------------------------

Training 8)

Αφαίρεσα το punishment για όταν δεν έχει παρκάρει. Θεωρώ ότι ίσως μπερδεύει τον πράκτορα και δεν μπορεί να εξηγήσει που οφείλεται. Το average reward είναι σίγουρα πολύ καλύτερο από πριν (όπως είναι λογικό), όμως δεν βλέπω ότι η γραφική ξεκινάει να συγκλίνει από νωρίς στην τιμή -1000, κι έτσι σταματάω την εκπαίδευση.

Training 9)

Άλλαξα τον συντελεστή εντροπίας ent_coef από 0 σε 0.01. Έτσι, εφαρμόζουμε Entropy regularization η οποία ενθαρρύνει τον πράκτορα να εξερευνήσει το περιβάλλον, ωθώντας σε πιο ίση κατανομή των actions του πράκτορα. Ωστόσο, πρέπει να μην το παρακάνουμε με το Entropy regularization, γιατί τότε οι actions του πράκτορα θα είναι random. Η γραφική παράσταση αυξάνεται και συγκλίνει κοντά στην τιμή 0. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει μάθει να μην μένει ακίνητος, ωστόσο συγκρούεται συχνά με άλλα αντικείμενα και η κίνηση του δεν είναι πάντα προς τον στόχο, αλλά μοιάζει τυχαία.

----------------------------------------------------------------------------------------------------------------------------------------
Γίνεται να συνεχίσεις την εκπαίδευση σε ένα ήδη υπάρχον μοντέλο και η γραφική στο Tensorboard να συνεχίσει από το σημείο που σταμάτησε, αλλά ΜΟΝΟ αν συνεχίσεις την πιο πρόσφατη εκπαίδευση. Αυτό γιατί, το νέο log αρχείο θα πάει να το βάλει στον πιο πρόσφατο φάκελο. Έτσι, αν έχεις παρεμβάλει άλλη εκπαίδευση μετά από αυτήν που θες να συνεχίσεις, θα πρέπει πρώτα να μετακινήσεις τον φάκελο της παρεμβαλόμενης αλλού. Μπορείς να τον επαναφέρεις έπειτα.
----------------------------------------------------------------------------------------------------------------------------------------

Training 10)

Άλλαξα το reward/punishment για το self.difference (αν βελτιώνει τη θέση του ή όχι) σε punishment ανάλογα με την απόσταση του από τη θέση. Έτσι, θέλω να μάθει να προτιμάει να βρίσκεται όσο το δυνατόν πιο κοντά στη θέση. Βλέπουμε την αναμενόμενη αυξητική τάση στην αρχή της γραφικής, ωστόσο μετά συγκλίνει, αρά δεν έχει νόημα να εκπαιδεύσουμε για παραπάνω επεισόδια. Επίσης, παρατηρώ ότι παρά το smoothing = 0.6 στη γραφική, αυτή κάνει απότομες βυθίσεις και μέγιστα. Αυτό μπορεί να οφείλεται στο ότι το learning rate του αλγορίθμου είναι πολύ μεγάλο κι έτσι ο πράκτορας είναι υπερευαίσθητος στα rewards που δέχεται και αντιδρά υπερβολικά με βάση αυτά.
Εξετάζοντας τον πράκτορα, παρατήρησα την εξής ενδιαφέρουσα συμπεριφορά. Ο πράκτορας έχει κολλήσει σε ένα local optimum, όπου η πολιτική του είναι απλά να κάνει γύρω γύρω την πίστα. Έτσι αποφεύγει το τεράστιο punishment (-100) του collision και το punishment για όταν μένει ακίνητος (-2). Παίρνει μόνο τα punishments λόγω της απόστασης του από τη θέση, τα οποία όμως είναι μικρά.

Training 11)

Δοκιμάζω να μειώσω το learning rate του αλγορίθμου. Η default τιμή του από την stable-baselines3 για τον αλγόριθμο PPO είναι η 0.0003 κι εγώ το υποδεκαπλασίασα, δηλ. το πήγα στην τιμή 0.00003. Η καμπύλη πλέον είναι πράγματι πιο ομαλή και ο πράκτορας μαθαίνει πιο αργά, αλλά σταθερά.

Training 12)

Επανέφερα το default learning rate. Σκέφτηκα για αρχή, να βγάλω τα punishments για collision και moving too slow. Πλέον απλά τιμωρώ τον πράκτορα με βάση την απόσταση του από τη θέση parking. Άρα, θέλω να τον δω να πηγαίνει στη θέση αυτή κάθε φορά, χωρίς να τον τρομάζουν οι συγκρούσεις με άλλα αντικείμενα. Μετά από 400.000 steps, παρατηρώ ότι η γραφική δεν έχει την αναμενόμενη αυξητική τάση και για αυτό σταματάω την εκπαίδευση. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κινείται πολύ αργά.

Training 13)

Επαναφέρω το punishment moving too slow. Η γραφική έχει μία πολύ μικρή αυξητική τάση. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πλέον δεν κινείται αργά, αλλά δεν τον βλέπω να κινείται προς τη θέση parking.

Training 14)

Αυξάνω το punishment της απόστασης απ τη θέση, ώστε να μάθει ο πράκτορας ότι είναι σημαντικό να κινείται προς τη θέση parking. Δυστυχώς, παρατηρώ ότι η γραφική των rewards δεν αυξάνει καθόλου, αλλά συγκλίνει.

Training 15)

Ανακάλυψα ότι η MLPpolicy που χρησιμοποιεί ο PPO πράκτορας μου, χρησιμοποιεί την tanh activation function, η οποία δουλεύει καλύτερα με εισόδους κανονικοποιημένες στο διάστημα [-1,1]. Για αυτό, κανονικοποίησα όλες τις εισόδους στο διάστημα αυτό. Η γραφική έχει πολύ μικρή θετική κλίση και με αυτόν τον ρυθμό δεν θα φτάσει ποτέ σε θετικά αποτελέσματα.

Training 16)

Δοκιμάζω χωρίς το Frame Skip = 4. Η γραφική δεν δείχνει αυξητική τάση. Εξετάζοντας τον πράκτορα, βλέπω ότι μένει πολύ ακίνητος.

Training 17)

Αφαιρώ και τον ent_coef, μήπως αυτός ωθεί τον πράκτορα σε τυχαίες actions, με αποτέλεσμα να μένει ακίνητος. Η γραφική συγκλίνει στην ίδια τιμή με αυτήν της εκπαίδευσης 15, αλλά σε λιγότερο χρόνο. Άρα μάλλον δεν χρειαζόμαστε το Frame Skip και τον ent_coef.

-----------------------------------------------------------------------------------------------------------------------------
2. Consider the Complexity of Your Task
Begin with a simple NN architecture and gradually increase the network's size if necessary. The complexity of your task should guide the network size.

Simple tasks: 1-2 hidden layers with 64 neurons each.
Moderately complex tasks: 2-3 hidden layers with 128-256 neurons each.
Highly complex tasks: 3-4 hidden layers with 256-512 neurons each.

4. Monitor Training and Adjust
Track the performance of your agent during training using TensorBoard or other monitoring tools. If the agent is underperforming, consider increasing the number of neurons or layers.

5. Avoid Overfitting
A very large network can overfit to the training data. Regularly validate the agent's performance on different initial conditions (starting positions and parking spot locations) to ensure it generalizes well.
-----------------------------------------------------------------------------------------------------------------------------

Training 18)

Αλλάζω την τοπολογία του ΝΝ, αντιγράφοντας αυτήν του Samuel Arzt. Πλέον το ΝΝ αποτελείται από 3 hidden layers και το καθένα περιέχει 128 νευρώνες. Η γραφική των rewards είχε αρνητική κλίση lol. Εξετάζοντας τον πράκτορα, παρατήρησα ότι συγκρουόταν διαδοχικά με αντικείμενα. 

Training 19)

Επαναφέρω το punishment for collision. Η γραφική έχει απότομη θετική κλίση στην αρχή, ωστόσο αρχίζει να συγκλίνει σε μόλις 50k επεισόδια. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κάθεται πολύ ακίνητος.

Training 20)

Αλλάζω τα βάρη, ώστε να δώσω έμφαση στο να μην μένει ακίνητος ο πράκτορας. Η γραφική του πράκτορα συγκλίνει πολύ γρήγορα.

Training 21)

Αλλάζω κάποιες hyperparameters, αντιγράφοντας τον Samuel Arzt. Η γραφική του πράκτορα συγκλίνει πολύ γρήγορα.

Training 22)

Βάζω πάλι τον ent_coef=0.01 για να mix things up. Η γραφική παράσταση έχει μία μικρή θετική κλίση και ο πράκτορας βελτιώνει το average reward του αργά αλλά σταθερά. Με 4 million steps εκπαίδευσης, βλέπω ότι ο πράκτορας ακολουθεί τη στρατηγική του να τρέχει γύρω γύρω την πίστα. Μετά από 7M steps (5 hours 30 min) η γραφική συγκλίνει και δεν έχει νόημα να συνεχίσουμε την εκπαίδευση.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Αλλάζω στρατηγική, θα ξεκινήσω απ το πιο απλό πρόβλημα και αν ο πράκτορας το μάθει καλά, τότε θα αυξήσω τη δυσκολία.
							FIXED SPOT SPAWN, FIXED CAR SPAWN
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 23) 

Η γραφική έχει την αναμενόμενη αυξητική τάση αρχικά, αλλά συγκλίνει γρήγορα σε αρνητική τιμή (-800).

Training 24)

ent_coef = 0.01. Η γραφική είναι καλύτερη από πριν και συγκλίνει στην τιμή -300 (αχνή γραμμή). Παρατηρώ ότι ο πράκτορας έμαθε να εκτελεί μία συγκεκριμένη χορογραφία γύρω από τη θέση.

Training 25)**

ent_coef = 0.001. Η γραφική είναι ίδια με πριν. Έχει ενδιαφέρον η χορογραφία που έμαθε ο πράκτορας, η οποία είναι να κάνει κύκλους περνώντας πάνω απ τη θέση.

Training 26)

Προσπαθώ να ενθαρρύνω τον πράκτορα να καταλάβει ότι όταν μπει στη θέση, πρέπει να κάτσει ακίνητος. Για αυτό εφαρμόζω κι άλλο reward shaping, όταν μπαίνει στη θέση το reward που παίρνει είναι αντιστρόφως ανάλογο της ταχύτητας του. Η γραφική έχει ακριβώς την ίδια μορφή με πριν.

Training 27)**

Δοκιμάζω 128 νευρώνες αντί για 64. Η γραφική έχει ακριβώς την ίδια μορφή με πριν.

Training 28)

Άλλαξα το punishment με βάση το distance σε punishment με βάση το offset_x και το offset_y. Μετά από 2M steps, η γραφική των rewards συγκλίνει σε πολύ υψηλές τιμές (+4000). Εξετάζοντας τον πράκτορα σε 100 επεισόδια, καταφέρνει να παρκάρει σε 76 από αυτά. Αυτό οφείλεται στο ότι η μέθοδος model.predict() που χρησιμοποιείται δεν επιστρέφει ντετερμινιστικές actions, αλλά την πιθανότητα να επιλεγεί το κάθε action και με βάση αυτήν την πιθανότητα επιλέγει κάθε φορά action. Μετά από 4Μ steps, η γραφική έχει αυξηθεί κι άλλο και συγκλίνει στην τιμή 4500. Eξετάζοντας τον πράκτορα σε 100 επεισόδια, καταφέρνει να παρκάρει σε 92 από αυτά. Μετά από 4.85M steps, η γραφική έχει αυξηθεί κι άλλο και συγκλίνει στην τιμή 5000, που είναι η επιβράβευση για το παρκάρισμα. Πράγματι, εξετάζοντας τον πράκτορα σε 100 επεισόδια, καταφέρνει να παρκάρει σε 96 από αυτά. Μάλιστα, αν στην model.predict() θέσουμε το όρισμα deterministic = True, το οποίο ορίζει να επιλέγεται πάντα το action με τη μεγαλύτερη πιθανότητα, τότε έχουμε success rate 100%. 

----------------------------------------------------------------------------------------------------------------------------------------
Continuous vs Discrete Action spaces

Proximal Policy Optimization (PPO) is a versatile reinforcement learning algorithm that works well with both discrete and continuous action spaces.
In Discrete Action spaces the action selection is simpler and thus is often more stable and require less fine-tuning. It is often used in games (e.g. Atari games) where the actions are discrete such as "move left", "move right".
Continuous Action spaces allow for more precise control, which is essential for tasks like Robotics (controlling the joint angles of a robotic arm) or Control (balancing a pole).
In general, if your task involves a limited set of actions, using a discrete action space is often easier and more stable. However, If precise control is crucial inyour task, a continuous action space is preferrable, but it may require more careful tuning of hyperparameters.

Reward Normalization

It can be beneficial, but it's not strictly necessary to normalize them to a specific range eg [0,1]. Instead, what matters is that the reward signals are structured in a way that helps the learning process. Namely, large variations in reward values can make training unstable. That is because, rewards or punishments that are severely larger than the rest might dominate the network's updates.
----------------------------------------------------------------------------------------------------------------------------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
							FIXED SPOT SPAWN, RANDOM CAR SPAWN
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 29)

Χρησιμοποίησα την ίδια reward function και αρχιτεκτονική δικτύου με το μοντέλο που έλυσε το απλούστερο πρόβλημα. Ωστόσο μετά από 2.5M steps, βλέπω πως η γραφική συγκλίνει σε αρνητική τιμή. Εξετάζοντας τον πράκτορα, παρατηρώ ότι υιοθέτησε τη στρατηγική του να κάνει γύρω γύρω την πίστα.

Training 30)

Χρησιμοποιώ μεγαλύτερο ΝΝ: 3 hidden layers με 128 neurons το καθένα. Μετά από 600.000 steps και εξετάζοντας τον πράκτορα, βλέπω ότι δημιουργείται η ίδια πολιτική.

Training 31)

Default NN και μεγαλύτερο punishment για τα offset. Μετά από 5M steps, η γραφική συγκλίνει σε αρνητική τιμή (-1700). Εξετάζοντας τον πράκτορα, παρατηρώ ότι δεν εμφανίζει πλέον τη στρατηγική του γύρω γύρω. Αντίθετα, δείχνει συχνά μία τάση να κινηθεί προς τη θέση, το οποίο είναι ενθαρρυντικό. Μοιάζει να μην τον ενδιαφέρουν τόσο πλέον τα collisions, αλλά δεν με ανησυχεί αυτό. Ας μάθει πρώτα να παρκάρει, που είναι ο κεντρικός του στόχος, και μετά μπορεί να βελτιώσει κι άλλο την πολιτική του, προσπαθώντας να παρκάρει χωρίς να συγκρουστεί. Επίσης, δεν μοιάζει να μένει ακίνητος, το οποίο είναι ενθαρρυντικό γιατί εξερευνεί το περιβάλλον.

Training 32)

Μεγαλύτερο ΝΝ και άλλαξα το punishment for moving too slow. Συγκεκριμένα, όταν το αμάξι είναι μακριά από τη θέση παρκινγκ, το τιμωρώ όταν πηγαίνει με ταχύτητα <25% (της max ταχύτητας του). Όταν το αμάξι είναι κοντά στη θέση παρκινγκ (αλλά όχι μέσα της) το τιμωρώ όταν πηγαίνει με ταχύτητα <10% (της max ταχύτητας του). Δηλ. όταν πλησιάζει στη θέση, τον αφήνω να πηγαίνει πιο αργά. Αυτό είναι λογικό, για να κάνει μανούβρες ακριβείας με μικρή ταχύτητα. Μου ήρθε αυτή η ιδέα καθώς πριν τον έβλεπα να πλησιάζει στη θέση παρκινγκ αλλά να κινείται πολύ γρήγορα και να τρακάρει. Ωστόσο, μετά από 2.3Μ steps, η γραφική συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 33)

Default NN και πρόσθεσα reward for being in the right angle, όταν το αμάξι βρίσκεται κοντά στη θέση παρκινγκ (αλλά όχι μέσα της). Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν. Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 34)

Άλλαξα το punishment για collision σε punishment για πολύ μικρή τιμή radar. Έτσι επιδιώκω να καταλάβει καλύτερα ο πράκτορας το γιατί τιμωρείται, ενώ αυξάνεται συνολικά το punishment που δέχεται ο πράκτορας για τέτοιες περιπτώσεις. Αυτό είναι θεμιτό, γιατί παρατήρησα πριν ότι ο πράκτορας κόλλαγε ορισμένες φορές, καθώς επιχειρούσε να μετακινηθεί προς τη θέση, αλλά είχε μπροστά του εμπόδιο. Έτσι, κατέληγε να συγκρούεται διαδοχικά με το εμπόδιο. Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 35)

Αύξησα το reward for being inside the spot. Έτσι, επιδιώκω όταν περάσει από τη θέση, να πάρει μεγάλο reward και να μάθει να μένει εκεί. Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 36 <- 33)

Μείωσα το learning rate σε 0.00003. Αυτό γιατί μικρές τιμές της υπερπαραμέτρου αυτής, επιβραδύνουν μεν τη σύγκλιση του αλγορίθμου, εξασφαλίζουν δε καλύτερη ακρίβεια του μοντέλου. Αντίθετα, μεγάλες τιμές αυτής της υπερπαραμέτρου προσφέρουν ταχύτερη σύγκλιση με αντίκτυπο όμως στην ακρίβεια. Άρα, εξαιτίας του μεγάλου βήματος, το μοντέλο μπορεί να μην συγκλίνει στη βέλτιστη λύση. Ωστόσο, μετά από 1Μ steps εκπαίδευσης, η γραφική έχει βελτιωθεί ελάχιστα. Εξετάζοντας τον πράκτορα, παρατηρώ ότι μάλλον το learning rate είναι τώρα πολύ μικρό, γιατί δεν βλέπω να έχει μάθει κάποια χρήσιμη συμπεριφορά. 

Training 37 <- 33)

Μεγαλύτερο ΝΝ. Μετά από 1Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με το training 33.

Training 38 <- 33)

Μικρότερο ΝΝ (2 layers of 32 neurons). Μετά από 1Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με το training 33.

Training 39 <- 33)

Μικρότερο ΝΝ (1 layer of 16 neurons). Μετά από ψ

Training 40 <- 33)

Έβαλα το inside_spot στο state space. Όταν το αμάξι βρίσκεται μέσα στη θέση, τότε παίρνει την τιμή 1, αλλίως έχει την τιμή -1. Το σκεπτικό μου ήταν να δώσω πιο εύκολα στον πράκτορα να καταλάβει πότε επιτυγχάνεται το μεγάλο reward. Ωστόσο, μετά από 4Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με τα προηγούμενα trainings.

Training 41 <- 33)

Έβαλα το distance στο state space (διατηρώντας τα offset_x, offset_y), στο διάστημα [1,1] λόγω της tanh. Όμως, όταν το αμάξι μπαίνει στη θέση, το distance γίνεται αυτόματα -1.  Το σκεπτικό μου ήταν να δώσω πιο εύκολα στον πράκτορα να καταλάβει πότε επιτυγχάνεται το μεγάλο reward. Ωστόσο, μετά από 2Μ steps η γραφική των rewards έχει αυξηθεί ελάχιστα. 

Training 42 <- 33)

Δοκιμάζω Frame skip = 4. Έτσι, επειδή θα αλλάζει action κάθε 4 frames, ελπίζω ο πράκτορας να είναι πιο σταθερός στις κινήσεις του και να μάθει να μένει μέσα στη θέση. Ωστόσο, μετά από 1.7Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με το training 33.

Training 43 <- 33)

Τον εκπαιδεύω να παρκάρει μόνο στο κάτω μισό του χάρτη. Η γραφική είναι σχεδόν πανομοιότυπη με την 33. Στη συνέχεια, τον εκπαιδεύω να παρκάρει σε οποιαδήποτε θέση (Training 43B). Ξανά, η γραφική συγκλίνει παράλληλα με την γραφική 33, ενώ εξετάζοντας τον πράκτορα, παρατηρώ ότι η συμπεριφορά του είναι παρόμοια με την 33. Άρα, βγάζουμε το συμπέρασμα ότι δεν υπάρχουν μεγάλες διαφορές στη δυσκολία του περιβάλλοντος μεταξύ fixed spot spawn και random spot spawn. Αυτό είναι λογικό, καθώς ο πράκτορας δέχεται το offset στα observations του, άρα δέχεται την πληροφορία της σχετικής απόστασης με τη θέση και όχι την ακριβή τοποθεσία της.

Training 44 <- 33) **

Fixed Car spawn, random spot spawn. 

Training 45 <- 33)

Απλοποιώ την reward function έχοντας ενιαία punishments ανεξάρτητα της απόστασης, ενώ προσθέτω punishment for being in the wrong angle.

Training 46 <- 33)

offsets - inside spot




----------------------------------------------------------------------------------------------------------------------------------------
ReLU

The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance. https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/

+ ReLU tends to converge faster than tanh
+ Simplicity: ReLU is computationally simple and efficient.
+ Gradient Propagation: ReLU does not saturate for very high positive values, which helps mitigate the vanishing gradient problem that tanh has.
- Dying ReLU Problem: Neurons can sometimes get stuck during training if they enter a region where the input is always negative, resulting in zero gradients.

Tanh

- A general problem with the tanh function is that it saturates. This means that large values snap to 1.0 and small values snap to -1. Further, the function is only really sensitive to changes around its mid-point of it input (0.5). Once saturated, it becomes challenging for the learning algorithm to continue to adapt the weights to improve the performance of the model. This is called the vanishing gradient problem and prevents deep (multi-layered) networks from learning effectively.
----------------------------------------------------------------------------------------------------------------------------------------