Training 1)

Τα 1000 επεισόδια (600.000 steps) πήραν 25min στη CPU. Ωστόσο, παρατήρησα ότι ο πράκτορας των 600.000 steps κουνιόταν αργά. Αυτό μάλλον οφείλεται στο ότι ξέχασα το punishment για όταν κινείται αργά χωρίς να έχει παρκάρει για όταν η ταχύτητα του == 0 -το οποίο συμβαίνει σπάνια. Τώρα το αλλάζω, ώστε αυτό το punishment να εφαρμόζεται όταν η ταχύτητα είναι < 10.

Training 2)

Εκπαιδεύσαμε για 3.500.000 steps (5.800 επεισόδια) και χρειάστηκαν 2.5 ώρες. Ωστόσο, βλέπουμε πως η γραφική των average rewards δεν έχει βελτιωθεί καθόλου.

Training 3)

Έκανα κανονικοποίηση της εισόδου και των rewards. Στα 1000 επεισόδια (600.000 steps), η γραφική των average rewards είναι παρόμοια με αυτήν του Training 2. Επίσης, παρατηρώ ότι η γραφική στο τέλος έχει σταθεροποιηθεί στην τιμή -5.400. Αξιολογώντας τον πράκτορα των 600.000 steps, είδα ότι απλά μένει ακίνητος σε όλο το επεισόδιο, κι έτσι προκύπτει reward: -9 * 600 = -5.400.

Training 4)

Εφάρμοσα MaxAndSkip = 4, δηλ. ο πράκτορας παίρνει απόφαση κάθε 4 frames. Παρατηρώ ότι πάλι το reward σταθεροποιήθηκε στο -5.400. Ωστόσο, είδα ότι η εκπαίδευση διήρκησε περισσότερη ώρα (συγκεκριμένα τα 600.000 steps χρειάστηκαν 34'). Έψαξα τι ακριβώς κάνει η κλάση MaxAndSkipEnv. Αφενός κάνει αυτό που θέλω, δηλ. ο πράκτορας επαναλαμβάνει την ίδια action για 4 frames, το οποίο είναι επιθυμητό για να μην διαλέγει πάρα πολύ συχνά διαφορετικά actions (παίρνει υπερβολικά γρήγορα αποφάσεις), να έχει πιο ομαλή οδήγηση. Έτσι, επιταχύνεται κι η εκπαίδευση. Ωστόσο, η κλάση αυτή κάνει επίσης Max-Pooling Over Frames, δηλ. επιστρέφει τη μέγιστη τιμή του κάθε pixel σε αυτά τα skipped frames. Αυτό είναι κάτι επιθυμητό όταν έχουμε περιβάλλον Atari, αλλά όχι στην περίπτωση μου, όπου τα observations δεν είναι frames αλλά vectors. Για αυτό κατέβασα τον κώδικα της κλάσης και τον άλλαξα, ώστε να αφήσω μόνο την 1η λειτουργία (άρα την ονόμασα SkipEnv class).

Training 5)

Θα εκπαιδεύσω τον πράκτορα με την custom SkipEnv class μου. Παρατηρώ ότι η γραφική των average rewards με την SkipEnv class μου είναι παρόμοια με αυτήν με την MaxAndSkipEnv και συγκλίνει στην τιμή -5.400. Αυτό είναι λογικό καθώς πρέπει να αλλάξουμε την reward function για να αλλάξει αυτό. Ωστόσο, δεν παρατηρήθηκε speed up μεταξύ της SkipEnv class και της MaxAndSkipEnv, δηλ. τα fps τους είναι ίδια κι έτσι χρειάστηκαν πάλι 34' για τα 600.000 steps. Παρόλα αυτά, και οι δύο κλάσεις συγκλίνουν πολύ γρηγορότερα από την περίπτωση χωρίς frame skip (PPO_3). Για αυτό, θα τις προτιμήσουμε στη συνέχεια.

----------------------------------------------------------------------------------------------------------------------------------------
Προσπάθησα να επιταχύνω τη διαδικασία της εκπαίδευσης, ενισχύωντας το hardware πάνω στο οποίο εκπαιδεύονται τα μοντέλα. Συγκεκριμένα, η βιβλιοθήκη stableBaselines3 χρησιμοποιεί Vectorized Environments: Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Vectorized environments in reinforcement learning offer several advantages:

Parallelism: By running multiple instances of an environment in parallel, vectorized environments can significantly speed up the data collection process. This is particularly beneficial in reinforcement learning, where algorithms often require large amounts of data to learn effectively.

Efficiency: Vectorized environments can make better use of hardware resources, such as multi-core CPUs or GPUs, by distributing the workload across multiple cores or processing units. This leads to more efficient use of the available computational power.

Υπάρχουν 2 ειδών vectorized environments στη βιβλιοθήκη stable-baselines3:

-DummyVecEnv is a simple implementation of VecEnv (VecEnv is an abstract class that serves as the base for all vectorized environments in Gym) that runs each environment in a sequential manner within a single process. Despite its name, it's useful for testing and development because it simplifies debugging by avoiding the complexities of multiprocessing. However, because it runs environments sequentially, it does not offer the same performance benefits as parallelized implementations when it comes to training time. It is preferrable in the case of a simple environment such as Cartpole-v1, because then the overhead of multiprocess outweighs the environment computation time.
-SubprocVecEnv is an implementation of VecEnv that runs each environment in its own subprocess. This allows for true parallelism, taking advantage of multiple CPU cores, which can significantly speed up the training process. However, the use of multiple processes can make debugging more challenging and may introduce overhead due to inter-process communication.

Προσπάθησα να χρησιμοποιήσω SubprocVecEnv αντί του default DummyVecEnv, για να αξιοποιήσω τους 4 CPU cores του υπολογιστή μου, ωστόσο προέκυψε το εξής πρόβλημα: SubprocVecEnv creates separate processes for each environment instance to speed up training by running them in parallel. Each process needs to serialize (pickle) the environment to send it to the child processes. However, if your environment includes non-serializable objects like pygame.surface.Surface, this process fails. Για να λυθεί αυτό το πρόβλημα θα έπρεπε να αλλάξω σημαντικά τον κώδικα για το ίδιο το παιχνίδι και για αυτό δεν προχώρησα σε αυτήν την κατεύθυνση.


Προσπάθησα να εκπαιδεύσω το μοντέλο χρησιμοποιώντας τη GPU του υπολογιστή μου και όχι τη CPU. Γενικά, η εκπαίδευση αλγόριθμών μηχανικής μάθησης επιταχύνεται όταν χρησιμοποποιείται η GPU, καθώς προσφέρει significant improvements in computational efficiency, parallel processing capabilities, and overall training speed. Πιο συγκεκριμένα, πολλοί RL algorithms, especially those involving deep reinforcement learning (e.g., Deep Q-Networks (DQN), Proximal Policy Optimization (PPO)), rely heavily on deep neural networks. Training these networks involves a large number of matrix multiplications and other operations that benefit from GPU acceleration.
Για να χρησιμοποιήσουμε τη GPU κατά την εκπαίδευση, έπρεπε πρώτα να εγκαταστήσουμε το CUDA Toolkit της NVidia.Το CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and application programming interface (API) model created by NVIDIA. It enables developers to use NVIDIA GPUs for general purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units). Στη συνέχεια, έπρεπε να εγκαταστήσουμε τη cuDNN library της NVidia, which provides optimized implementations for deep learning operations. Τέλος, έπρεπε να κατεβάσουμε άλλη έκδοση της PyTorch (σε αυτήν τη βιβλιοθήκη είναι γραμμένη η stable-baselines3), η οποία να υποστηρίζει GPU Support.
----------------------------------------------------------------------------------------------------------------------------------------

Training 6)

Εκπαίδευσα χρησιμοποιώντας τη GPU μου για 1000 επεισόδια, ώστε να δω το speed up στον χρόνο εκπαίδευσης σε σχέση με πριν. Προφανώς η γραφική είναι σχεδόν πανομοιότυπη με πριν, αφού δεν άλλαξα τίποτα στο μοντέλο. Προς μεγάλη μου έκπληξη, προέκυψε πως η εκπαίδευση στη GPU ήταν πιο αργή (πήρε 49') από την εκπαίδευση στη CPU (πήρε 34'). Αυτό ίσως οφείλεται στο ότι το λάπτοπ μου διαθέτει μία παρωχημένη GPU (την NVIDIA GeForce MX110). Ευτυχώς, αλλάζοντας την παράμετρο device της κλάσης PPO ορίζουμε σε ποια συσκευή επιθυμούμε να πραγματοποιηθεί η εκπαίδευση.

Training 7)

Μέχρι στιγμής, το average reward του πράκτορα συγκλίνει στην τιμή -5.400. Αυτό σημαίνει ότι ο πράκτορας επιλέγει να μένει ακίνητος από την αρχή μέχρι το τέλος του επεισοδίου. Έτσι, παίρνει το punishment για το ότι δεν έχει παρκάρει (-7) και το punishment για το ότι έχει μικρή ταχύτητα (-2). Όμως, έτσι αποφεύγει το τεράστιο συγκριτικά punishment για τα collisions (-1000). Άρα, ο πράκτορας φοβάται τόσο μην συγκρουστεί με κάτι, που επιλέγει να μην παίξει καθόλου. Όμως έτσι, ο πράκτορας έχει κολλήσει σε αυτό το τοπικό μέγιστο (local optimum). Για αυτό ΜΕΙΩΣΑ το punishment της σύγκρουσης σε 100. Βλέπω ότι με 600.000 steps η γραφική συγκλίνει και πάλι στο -5400. Άρα, σταματάω την εκπαίδευση.

----------------------------------------------------------------------------------------------------------------------------------------
Επιλέγω smoothing = 0.99 για τις καμπύλες των average rewards. Έτσι, βλέπω την πιο εξομαλυμένη μορφή των καμπυλών η οποία θεωρώ μας δίνει την πιο χρήσιμη πληροφορία σχετικά με την τάση της γραφικής, χωρίς να αποπροσανατολιζόμαστε από τις συχνές βυθίσεις και μέγιστα. Άλλωστε, η κανονική μορφή της γραφικής (χωρίς smoothing) φαίνεται με αχνό χρώμα.
----------------------------------------------------------------------------------------------------------------------------------------

Training 8)

Αφαίρεσα το punishment για όταν δεν έχει παρκάρει. Θεωρώ ότι ίσως μπερδεύει τον πράκτορα και δεν μπορεί να εξηγήσει που οφείλεται. Το average reward είναι σίγουρα πολύ καλύτερο από πριν (όπως είναι λογικό), όμως δεν βλέπω ότι η γραφική ξεκινάει να συγκλίνει από νωρίς στην τιμή -1000, κι έτσι σταματάω την εκπαίδευση.

Training 9)

Άλλαξα τον συντελεστή εντροπίας ent_coef από 0 σε 0.01. Έτσι, εφαρμόζουμε Entropy regularization η οποία ενθαρρύνει τον πράκτορα να εξερευνήσει το περιβάλλον, ωθώντας σε πιο ίση κατανομή των actions του πράκτορα. Ωστόσο, πρέπει να μην το παρακάνουμε με το Entropy regularization, γιατί τότε οι actions του πράκτορα θα είναι random. Η γραφική παράσταση αυξάνεται και συγκλίνει κοντά στην τιμή 0. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει μάθει να μην μένει ακίνητος, ωστόσο συγκρούεται συχνά με άλλα αντικείμενα και η κίνηση του δεν είναι πάντα προς τον στόχο, αλλά μοιάζει τυχαία.

----------------------------------------------------------------------------------------------------------------------------------------
Γίνεται να συνεχίσεις την εκπαίδευση σε ένα ήδη υπάρχον μοντέλο και η γραφική στο Tensorboard να συνεχίσει από το σημείο που σταμάτησε, αλλά ΜΟΝΟ αν συνεχίσεις την πιο πρόσφατη εκπαίδευση. Αυτό γιατί, το νέο log αρχείο θα πάει να το βάλει στον πιο πρόσφατο φάκελο. Έτσι, αν έχεις παρεμβάλει άλλη εκπαίδευση μετά από αυτήν που θες να συνεχίσεις, θα πρέπει πρώτα να μετακινήσεις τον φάκελο της παρεμβαλόμενης αλλού. Μπορείς να τον επαναφέρεις έπειτα.
----------------------------------------------------------------------------------------------------------------------------------------

Training 10)

Άλλαξα το reward/punishment για το self.difference (αν βελτιώνει τη θέση του ή όχι) σε punishment ανάλογα με την απόσταση του από τη θέση. Έτσι, θέλω να μάθει να προτιμάει να βρίσκεται όσο το δυνατόν πιο κοντά στη θέση. Βλέπουμε την αναμενόμενη αυξητική τάση στην αρχή της γραφικής, ωστόσο μετά συγκλίνει, αρά δεν έχει νόημα να εκπαιδεύσουμε για παραπάνω επεισόδια. Επίσης, παρατηρώ ότι παρά το smoothing = 0.6 στη γραφική, αυτή κάνει απότομες βυθίσεις και μέγιστα. Αυτό μπορεί να οφείλεται στο ότι το learning rate του αλγορίθμου είναι πολύ μεγάλο κι έτσι ο πράκτορας είναι υπερευαίσθητος στα rewards που δέχεται και αντιδρά υπερβολικά με βάση αυτά.
Εξετάζοντας τον πράκτορα, παρατήρησα την εξής ενδιαφέρουσα συμπεριφορά. Ο πράκτορας έχει κολλήσει σε ένα local optimum, όπου η πολιτική του είναι απλά να κάνει γύρω γύρω την πίστα. Έτσι αποφεύγει το τεράστιο punishment (-100) του collision και το punishment για όταν μένει ακίνητος (-2). Παίρνει μόνο τα punishments λόγω της απόστασης του από τη θέση, τα οποία όμως είναι μικρά.

Training 11)

Δοκιμάζω να μειώσω το learning rate του αλγορίθμου. Η default τιμή του από την stable-baselines3 για τον αλγόριθμο PPO είναι η 0.0003 κι εγώ το υποδεκαπλασίασα, δηλ. το πήγα στην τιμή 0.00003. Η καμπύλη πλέον είναι πράγματι πιο ομαλή και ο πράκτορας μαθαίνει πιο αργά, αλλά σταθερά.

Training 12)

Επανέφερα το default learning rate. Σκέφτηκα για αρχή, να βγάλω τα punishments για collision και moving too slow. Πλέον απλά τιμωρώ τον πράκτορα με βάση την απόσταση του από τη θέση parking. Άρα, θέλω να τον δω να πηγαίνει στη θέση αυτή κάθε φορά, χωρίς να τον τρομάζουν οι συγκρούσεις με άλλα αντικείμενα. Μετά από 400.000 steps, παρατηρώ ότι η γραφική δεν έχει την αναμενόμενη αυξητική τάση και για αυτό σταματάω την εκπαίδευση. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κινείται πολύ αργά.

Training 13)

Επαναφέρω το punishment moving too slow. Η γραφική έχει μία πολύ μικρή αυξητική τάση. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πλέον δεν κινείται αργά, αλλά δεν τον βλέπω να κινείται προς τη θέση parking.

Training 14)

Αυξάνω το punishment της απόστασης απ τη θέση, ώστε να μάθει ο πράκτορας ότι είναι σημαντικό να κινείται προς τη θέση parking. Δυστυχώς, παρατηρώ ότι η γραφική των rewards δεν αυξάνει καθόλου, αλλά συγκλίνει.

Training 15)

Ανακάλυψα ότι η MLPpolicy που χρησιμοποιεί ο PPO πράκτορας μου, χρησιμοποιεί την tanh activation function, η οποία δουλεύει καλύτερα με εισόδους κανονικοποιημένες στο διάστημα [-1,1]. Για αυτό, κανονικοποίησα όλες τις εισόδους στο διάστημα αυτό. Η γραφική έχει πολύ μικρή θετική κλίση και με αυτόν τον ρυθμό δεν θα φτάσει ποτέ σε θετικά αποτελέσματα.

Training 16)

Δοκιμάζω χωρίς το Frame Skip = 4. Η γραφική δεν δείχνει αυξητική τάση. Εξετάζοντας τον πράκτορα, βλέπω ότι μένει πολύ ακίνητος.

Training 17)

Αφαιρώ και τον ent_coef, μήπως αυτός ωθεί τον πράκτορα σε τυχαίες actions, με αποτέλεσμα να μένει ακίνητος. Η γραφική συγκλίνει στην ίδια τιμή με αυτήν της εκπαίδευσης 15, αλλά σε λιγότερο χρόνο. Άρα μάλλον δεν χρειαζόμαστε το Frame Skip και τον ent_coef.

-----------------------------------------------------------------------------------------------------------------------------
2. Consider the Complexity of Your Task
Begin with a simple NN architecture and gradually increase the network's size if necessary. The complexity of your task should guide the network size.

Simple tasks: 1-2 hidden layers with 64 neurons each.
Moderately complex tasks: 2-3 hidden layers with 128-256 neurons each.
Highly complex tasks: 3-4 hidden layers with 256-512 neurons each.

4. Monitor Training and Adjust
Track the performance of your agent during training using TensorBoard or other monitoring tools. If the agent is underperforming, consider increasing the number of neurons or layers.

5. Avoid Overfitting
A very large network can overfit to the training data. Regularly validate the agent's performance on different initial conditions (starting positions and parking spot locations) to ensure it generalizes well.
-----------------------------------------------------------------------------------------------------------------------------

Training 18)

Αλλάζω την τοπολογία του ΝΝ, αντιγράφοντας αυτήν του Samuel Arzt. Πλέον το ΝΝ αποτελείται από 3 hidden layers και το καθένα περιέχει 128 νευρώνες. Η γραφική των rewards είχε αρνητική κλίση lol. Εξετάζοντας τον πράκτορα, παρατήρησα ότι συγκρουόταν διαδοχικά με αντικείμενα. 

Training 19)

Επαναφέρω το punishment for collision. Η γραφική έχει απότομη θετική κλίση στην αρχή, ωστόσο αρχίζει να συγκλίνει σε μόλις 50k επεισόδια. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κάθεται πολύ ακίνητος.

Training 20)

Αλλάζω τα βάρη, ώστε να δώσω έμφαση στο να μην μένει ακίνητος ο πράκτορας. Η γραφική του πράκτορα συγκλίνει πολύ γρήγορα.

Training 21)

Αλλάζω κάποιες hyperparameters, αντιγράφοντας τον Samuel Arzt. Η γραφική του πράκτορα συγκλίνει πολύ γρήγορα.

Training 22)

Βάζω πάλι τον ent_coef=0.01 για να mix things up. Η γραφική παράσταση έχει μία μικρή θετική κλίση και ο πράκτορας βελτιώνει το average reward του αργά αλλά σταθερά. Με 4 million steps εκπαίδευσης, βλέπω ότι ο πράκτορας ακολουθεί τη στρατηγική του να τρέχει γύρω γύρω την πίστα. Μετά από 7M steps (5 hours 30 min) η γραφική συγκλίνει και δεν έχει νόημα να συνεχίσουμε την εκπαίδευση.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Αλλάζω στρατηγική, θα ξεκινήσω απ το πιο απλό πρόβλημα και αν ο πράκτορας το μάθει καλά, τότε θα αυξήσω τη δυσκολία.
							FIXED SPOT SPAWN, FIXED CAR SPAWN
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 23) 

Η γραφική έχει την αναμενόμενη αυξητική τάση αρχικά, αλλά συγκλίνει γρήγορα σε αρνητική τιμή (-800).

Training 24)

ent_coef = 0.01. Η γραφική είναι καλύτερη από πριν και συγκλίνει στην τιμή -300 (αχνή γραμμή). Παρατηρώ ότι ο πράκτορας έμαθε να εκτελεί μία συγκεκριμένη χορογραφία γύρω από τη θέση.

Training 25)**

ent_coef = 0.001. Η γραφική είναι ίδια με πριν. Έχει ενδιαφέρον η χορογραφία που έμαθε ο πράκτορας, η οποία είναι να κάνει κύκλους περνώντας πάνω απ τη θέση.

Training 26)

Προσπαθώ να ενθαρρύνω τον πράκτορα να καταλάβει ότι όταν μπει στη θέση, πρέπει να κάτσει ακίνητος. Για αυτό εφαρμόζω κι άλλο reward shaping, όταν μπαίνει στη θέση το reward που παίρνει είναι αντιστρόφως ανάλογο της ταχύτητας του. Η γραφική έχει ακριβώς την ίδια μορφή με πριν.

Training 27)**

Δοκιμάζω 128 νευρώνες αντί για 64. Η γραφική έχει ακριβώς την ίδια μορφή με πριν.

Training 28)

Άλλαξα το punishment με βάση το distance σε punishment με βάση το offset_x και το offset_y. Μετά από 2M steps, η γραφική των rewards συγκλίνει σε πολύ υψηλές τιμές (+4000). Εξετάζοντας τον πράκτορα σε 100 επεισόδια, καταφέρνει να παρκάρει σε 76 από αυτά. Αυτό οφείλεται στο ότι η μέθοδος model.predict() που χρησιμοποιείται δεν επιστρέφει ντετερμινιστικές actions, αλλά την πιθανότητα να επιλεγεί το κάθε action και με βάση αυτήν την πιθανότητα επιλέγει κάθε φορά action. Μετά από 4Μ steps, η γραφική έχει αυξηθεί κι άλλο και συγκλίνει στην τιμή 4500. Eξετάζοντας τον πράκτορα σε 100 επεισόδια, καταφέρνει να παρκάρει σε 92 από αυτά. Μετά από 4.85M steps, η γραφική έχει αυξηθεί κι άλλο και συγκλίνει στην τιμή 5000, που είναι η επιβράβευση για το παρκάρισμα. Πράγματι, εξετάζοντας τον πράκτορα σε 100 επεισόδια, καταφέρνει να παρκάρει σε 96 από αυτά. Μάλιστα, αν στην model.predict() θέσουμε το όρισμα deterministic = True, το οποίο ορίζει να επιλέγεται πάντα το action με τη μεγαλύτερη πιθανότητα, τότε έχουμε success rate 100%. 

----------------------------------------------------------------------------------------------------------------------------------------
Continuous vs Discrete Action spaces

Proximal Policy Optimization (PPO) is a versatile reinforcement learning algorithm that works well with both discrete and continuous action spaces.
In Discrete Action spaces the action selection is simpler and thus is often more stable and require less fine-tuning. It is often used in games (e.g. Atari games) where the actions are discrete such as "move left", "move right".
Continuous Action spaces allow for more precise control, which is essential for tasks like Robotics (controlling the joint angles of a robotic arm) or Control (balancing a pole).
In general, if your task involves a limited set of actions, using a discrete action space is often easier and more stable. However, If precise control is crucial inyour task, a continuous action space is preferrable, but it may require more careful tuning of hyperparameters.

Reward Normalization

It can be beneficial, but it's not strictly necessary to normalize them to a specific range eg [0,1]. Instead, what matters is that the reward signals are structured in a way that helps the learning process. Namely, large variations in reward values can make training unstable. That is because, rewards or punishments that are severely larger than the rest might dominate the network's updates.
----------------------------------------------------------------------------------------------------------------------------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
							FIXED SPOT SPAWN, RANDOM CAR SPAWN
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Training 29)

Χρησιμοποίησα την ίδια reward function και αρχιτεκτονική δικτύου με το μοντέλο που έλυσε το απλούστερο πρόβλημα. Ωστόσο μετά από 2.5M steps, βλέπω πως η γραφική συγκλίνει σε αρνητική τιμή. Εξετάζοντας τον πράκτορα, παρατηρώ ότι υιοθέτησε τη στρατηγική του να κάνει γύρω γύρω την πίστα.

Training 30)

Χρησιμοποιώ μεγαλύτερο ΝΝ: 3 hidden layers με 128 neurons το καθένα. Μετά από 600.000 steps και εξετάζοντας τον πράκτορα, βλέπω ότι δημιουργείται η ίδια πολιτική.

Training 31)

Default NN και μεγαλύτερο punishment για τα offset. Μετά από 5M steps, η γραφική συγκλίνει σε αρνητική τιμή (-1700). Εξετάζοντας τον πράκτορα, παρατηρώ ότι δεν εμφανίζει πλέον τη στρατηγική του γύρω γύρω. Αντίθετα, δείχνει συχνά μία τάση να κινηθεί προς τη θέση, το οποίο είναι ενθαρρυντικό. Μοιάζει να μην τον ενδιαφέρουν τόσο πλέον τα collisions, αλλά δεν με ανησυχεί αυτό. Ας μάθει πρώτα να παρκάρει, που είναι ο κεντρικός του στόχος, και μετά μπορεί να βελτιώσει κι άλλο την πολιτική του, προσπαθώντας να παρκάρει χωρίς να συγκρουστεί. Επίσης, δεν μοιάζει να μένει ακίνητος, το οποίο είναι ενθαρρυντικό γιατί εξερευνεί το περιβάλλον.

Training 32)

Μεγαλύτερο ΝΝ και άλλαξα το punishment for moving too slow. Συγκεκριμένα, όταν το αμάξι είναι μακριά από τη θέση παρκινγκ, το τιμωρώ όταν πηγαίνει με ταχύτητα <25% (της max ταχύτητας του). Όταν το αμάξι είναι κοντά στη θέση παρκινγκ (αλλά όχι μέσα της) το τιμωρώ όταν πηγαίνει με ταχύτητα <10% (της max ταχύτητας του). Δηλ. όταν πλησιάζει στη θέση, τον αφήνω να πηγαίνει πιο αργά. Αυτό είναι λογικό, για να κάνει μανούβρες ακριβείας με μικρή ταχύτητα. Μου ήρθε αυτή η ιδέα καθώς πριν τον έβλεπα να πλησιάζει στη θέση παρκινγκ αλλά να κινείται πολύ γρήγορα και να τρακάρει. Ωστόσο, μετά από 2.3Μ steps, η γραφική συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 33)

Default NN και πρόσθεσα reward for being in the right angle, όταν το αμάξι βρίσκεται κοντά στη θέση παρκινγκ (αλλά όχι μέσα της). Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν. Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 34)

Άλλαξα το punishment για collision σε punishment για πολύ μικρή τιμή radar. Έτσι επιδιώκω να καταλάβει καλύτερα ο πράκτορας το γιατί τιμωρείται, ενώ αυξάνεται συνολικά το punishment που δέχεται ο πράκτορας για τέτοιες περιπτώσεις. Αυτό είναι θεμιτό, γιατί παρατήρησα πριν ότι ο πράκτορας κόλλαγε ορισμένες φορές, καθώς επιχειρούσε να μετακινηθεί προς τη θέση, αλλά είχε μπροστά του εμπόδιο. Έτσι, κατέληγε να συγκρούεται διαδοχικά με το εμπόδιο. Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 35)

Αύξησα το reward for being inside the spot. Έτσι, επιδιώκω όταν περάσει από τη θέση, να πάρει μεγάλο reward και να μάθει να μένει εκεί. Μετά από 2.5Μ steps, η γραφική παράσταση συγκλίνει στην ίδια τιμή με πριν. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πράγματι έχει παρόμοια συμπεριφορά με πριν.

Training 36 <- 33)

Μείωσα το learning rate σε 0.00003. Αυτό γιατί μικρές τιμές της υπερπαραμέτρου αυτής, επιβραδύνουν μεν τη σύγκλιση του αλγορίθμου, εξασφαλίζουν δε καλύτερη ακρίβεια του μοντέλου. Αντίθετα, μεγάλες τιμές αυτής της υπερπαραμέτρου προσφέρουν ταχύτερη σύγκλιση με αντίκτυπο όμως στην ακρίβεια. Άρα, εξαιτίας του μεγάλου βήματος, το μοντέλο μπορεί να μην συγκλίνει στη βέλτιστη λύση. Ωστόσο, μετά από 1Μ steps εκπαίδευσης, η γραφική έχει βελτιωθεί ελάχιστα. Εξετάζοντας τον πράκτορα, παρατηρώ ότι μάλλον το learning rate είναι τώρα πολύ μικρό, γιατί δεν βλέπω να έχει μάθει κάποια χρήσιμη συμπεριφορά. 

Training 37 <- 33)

Μεγαλύτερο ΝΝ. Μετά από 1Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με το training 33.

Training 38 <- 33)

Μικρότερο ΝΝ (2 layers of 32 neurons). Μετά από 1Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με το training 33.

Training 39 <- 33)

Μικρότερο ΝΝ (1 layer of 16 neurons). Μετά από 3Μ steps, η γραφική συγκλίνει σε μικρότερη τιμή απ την 33.

Training 40 <- 33)

Έβαλα το inside_spot στο state space. Όταν το αμάξι βρίσκεται μέσα στη θέση, τότε παίρνει την τιμή 1, αλλίως έχει την τιμή -1. Το σκεπτικό μου ήταν να δώσω πιο εύκολα στον πράκτορα να καταλάβει πότε επιτυγχάνεται το μεγάλο reward. Ωστόσο, μετά από 4Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με τα προηγούμενα trainings.

Training 41 <- 33)

Έβαλα το distance στο state space (διατηρώντας τα offset_x, offset_y), στο διάστημα [1,1] λόγω της tanh. Όμως, όταν το αμάξι μπαίνει στη θέση, το distance γίνεται αυτόματα -1.  Το σκεπτικό μου ήταν να δώσω πιο εύκολα στον πράκτορα να καταλάβει πότε επιτυγχάνεται το μεγάλο reward. Ωστόσο, μετά από 2Μ steps η γραφική των rewards έχει αυξηθεί ελάχιστα. 

Training 42 <- 33)

Δοκιμάζω Frame skip = 4. Έτσι, επειδή θα αλλάζει action κάθε 4 frames, ελπίζω ο πράκτορας να είναι πιο σταθερός στις κινήσεις του και να μάθει να μένει μέσα στη θέση. Ωστόσο, μετά από 1.7Μ steps, δεν μοιάζει να υπάρχει καμία διαφορά με το training 33.

Training 43 <- 33)

Τον εκπαιδεύω να παρκάρει μόνο στο κάτω μισό του χάρτη. Η γραφική είναι σχεδόν πανομοιότυπη με την 33. (43Β) Τον εκπαιδεύω να παρκάρει σε οποιαδήποτε θέση. Ξανά, η γραφική συγκλίνει παράλληλα με την γραφική 33, ενώ εξετάζοντας τον πράκτορα, παρατηρώ ότι η συμπεριφορά του είναι παρόμοια με την 33. Άρα, βγάζουμε το συμπέρασμα ότι δεν υπάρχουν μεγάλες διαφορές στη δυσκολία του περιβάλλοντος μεταξύ fixed spot spawn και random spot spawn. Αυτό είναι λογικό, καθώς ο πράκτορας δέχεται το offset στα observations του, άρα δέχεται την πληροφορία της σχετικής απόστασης με τη θέση και όχι την ακριβή τοποθεσία της. (43C)** Fixed car spawn, random spot spawn. Μετά από 3M steps, η γραφική των rewards του πράκτορα συγκλίνει στην τιμή +3000. Εξετάζοντας τον πράκτορα, παρατηρώ ότι μένει ακίνητος κοντά στη θέση παρκινγκ. Ωστόσο, θέτοντας σε στοχαστικές τις actions του πράκτορα, παρατηρώ ότι σε 1000 επεισόδια αξιολόγησης καταφέρνει να παρκάρει σε 581 από αυτά. (43D) Αυξάνω το punishment for standing still.

Training 44 <- 33)

Απλοποιώ την reward function έχοντας ενιαία punishments ανεξάρτητα της απόστασης, ενώ προσθέτω punishment for being in the wrong angle. Ωστόσο, μετά από 2Μ steps, δεν βλέπω διαφορά από το training 33.

Training 45)

Q learning reward function. (45A) Αρχικά, η γραφική συγκλίνει στην τιμή -300. Παρατηρώ ότι ο πράκτορας έμαθε να κάθεται ακίνητος κοντά στη θέση parking, καθώς τότε τον επιβραβεύω λίγο. (45Β) Αλλάζω τη reward function, ώστε να τιμωρώ τον πράκτορα λιγότερο όταν βρίσκεται κοντά στη θέση, αλλά να μην τον επιβραβεύω κιόλας. Μετά από 500K steps, η γραφική συγκλίνει σε αρνητική τιμή. Εξετάζοντας τον πράκτορα, βλέπω ότι δεν έχει μάθει κάποια συγκεκριμένη συμπεριφορά. (45C) Αυξάνω τον ent_coef σε 0.05, ώστε να μάθει ο πράκτορας να εξερευνάει παραπάνω. Ωστόσο, μετά από 1.4Μ steps, παρατηρώ ότι η γραφική συγκλίνει σε αρνητική τιμή. Εξετάζοντας τον πράκτορα, παρατηρώ ότι τα actions του αλλάζουν υπερβολικά συχνά, οπότε ίσως να παραήταν υψηλός ο ent_coef. (45D) Δοκιμάζω ent_coef = 0.03. Ωστόσο, η γραφική συγκλίνει γρήγορα στην ίδια τιμή με πριν.

Training 46 <- 33)

Αυξάνω τον ent_coef σε 0.03. Επιθυμώ ο πράκτορας να εξερευνήσει περισσότερο το περιβάλλον. Ωστόσο, η γραφική αρχικά ακολουθεί αυτήν της εκπαίδευσης 33 και μετά συγκλίνει σε χειρότερη τιμή. (46Β) Αυξάνω τον ent_coef σε 0.1. Ωστόσο, η γραφική αρχικά ακολουθεί αυτήν της εκπαίδευσης 33 και μετά συγκλίνει στην ίδια τιμή με πριν.

Training 47 <- 33)

Αλλάζω τους κανόνες του παιχνιδιού, ώστε μόλις μπαίνει ο πράκτορας στη θέση, το παιχνίδι να σταματάει. Έτσι, επιδιώκω να διευκολύνω τον πράκτορα, ώστε να μάθει αρχικά, να κερδίζει το παιχνίδι έτσι κάθε φορά. Μετά από 7Μ steps, παρατηρώ ότι η γραφική των rewards συγκλίνει σε αρνητική τιμή κοντά στο 0, ενώ το success rate του συγκλίνει στο 30%. Εξετάζοντας τον πράκτορα, δεν βλέπω διαφορά στη συμπεριφορά του σε σχέση με το Training 33. Στα 1000 επεισόδια αξιολόγησης, ο πράκτορας κατάφερε να παρκάρει σε 333 από αυτά. (47B) Ent_coef = 0, γιατί είδα ότι ο πράκτορας αλλάζει συχνά αctions. Άρα, ίσως δεν χρειάζεται να τον ωθούμε κι εμείς να επιλέγει διαφορετικά actions. Ωστόσο, μετά από 2M steps, η γραφική των rewards του πράκτορα συγκλίνει σε πολύ μικρότερη τιμή σε σχέση με πριν . Το ίδιο συμβαίνει και με τη γραφική του success rate. Εξετάζοντας τον πράκτορα, παρατηρώ ότι επιλέγει επανειλημμένα τα ίδια actions, ακόμα και με στοχαστική πολιτική. Επομένως, χρειαζόμαστε τον ent_coef, για την καλύτερη εξερεύνηση του περιβάλλοντος.

Training 48 <- 33) **

Παρόμοια λογική με το 43C, δοκίμασα τη reward function του 33 με fixed car spawn και random spot spawn, αλλά με 4 διαφορετικές αρχικές γωνίες για το αμάξι (0, 90, 180, 270). Μετά από 7Μ steps, η γραφική των rewards συγκλίνει σε τιμή κοντά στο 0, αλλά εξετάζοντας τον πράκτορα βλέπω πως σπάνια καταφέρνει να παρκάρει στη θέση, συνήθως κινείται δίπλα της. Στα 1000 επεισόδια αξιολόγησης, ο πράκτορας κατάφερε να παρκάρει σε 0 από αυτά.

Training 49 <-43C) **

Δοκιμάζω Curriculum Learning με το μοντέλο 43C, που έμαθε να παρκάρει (581/1000) με σταθερό spawn. (49Α) Αρχικά, θα τον βάλω να μάθει να παρκάρει απ το ίδιο spawn, αλλά υπό διαφορετικές γωνίες στο διάστημα [-10,10]. Μετά από 4Μ steps, το success rate συγκλίνει στο 50%. Στα 1000 επεισόδια αξιολόγησης, ο πράκτορας κατάφερε να παρκάρει σε 546 από αυτά. (49B) Έπειτα, αυξάνω το δυνατό διάστημα της γωνίας υπό την οποία κάνει spawn ο πράκτορας. Μετά από 4M steps, το success rate συγκλίνει στο 35%. Στα 1000 επεισόδια αξιολόγησης, ο πράκτορας κατάφερε να παρκάρει σε 403 από αυτά.

Training 50 <- 47)

Δοκιμάζω να τελειώνει το επεισόδιο μόνο όταν παρκάρει ο πράκτορας. Όπως είναι λογικό, αρχικά παρατηρώ ότι το episode length είναι πάρα πολύ μεγάλο και το average reward πάρα πολύ μικρό. Όμως το θετικό είναι ότι αυτές οι καμπύλες μεταβάλλονται με απότομη θετική κλίση. Ωστόσο, οι καμπύλες συγκλίνουν όταν φτάνουν σε συγκρίσιμες τιμές με αυτές του Training 47. Συγκεκριμένα, το episode length συγκλίνει στην τιμή 2500 (τόσα steps χρειάζονται κατά ΜΟ για να παρκάρει ο πράκτορας) και το average reward συγκλίνει στην τιμή -8000, καθώς σε αυτό το διάστημα το αυτοκίνητο συγκρούεται με αντικείμενα και απομακρύνεται σημαντικά απ τη θέση παρκινγκ.

Training 51 <- 47)

Αλλάζω το punishment on distance σε reward on distance. Αυτό γιατί διάβασα πως: continuous negative reinforcement might make it harder for the agent to explore since it constantly receives penalties. Ωστόσο, μετά από 4Μ steps, η γραφική του success rate του πράκτορα είναι σημαντικά μικρότερη αυτής του Training 47.

Training 52 <- 47)

Πρόσθεσα αυξανόμενο punishment με βάση τα steps του πράκτορα. Έτσι, προσπαθώ να τον ωθήσω να τελειώσει το επεισόδιο το νωρίτερα. Ωστόσο, μετά από 2Μ steps, οι γραφικές έχουν την ίδια μορφή με το Training 47, ενώ το success rate είναι μικρότερο. 

----------------------------------------------------------------------------------------------------------------------------------------
ReLU

The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance. https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/

+ ReLU tends to converge faster than tanh
+ Simplicity: ReLU is computationally simple and efficient.
+ Gradient Propagation: ReLU does not saturate for very high positive values, which helps mitigate the vanishing gradient problem that tanh has.
- Dying ReLU Problem: Neurons can sometimes get stuck during training if they enter a region where the input is always negative, resulting in zero gradients.

Tanh

- A general problem with the tanh function is that it saturates. This means that large values snap to 1.0 and small values snap to -1. Further, the function is only really sensitive to changes around its mid-point of it input (0.5). Once saturated, it becomes challenging for the learning algorithm to continue to adapt the weights to improve the performance of the model. This is called the vanishing gradient problem and prevents deep (multi-layered) networks from learning effectively.
----------------------------------------------------------------------------------------------------------------------------------------


----------------------------------------------------------------------------------------------------------------------------------------
DROPOUT	

Dropout is a regularization technique commonly used in deep learning for tasks like image classification, natural language processing, and other supervised learning tasks. The main purpose of dropout is to prevent overfitting by randomly setting a fraction of input units to zero during training. This helps in improving the generalization of the model by reducing the interdependence between neurons.

In the context of deep reinforcement learning (RL), such as in algorithms like Deep Q Networks (DQN), Deep Deterministic Policy Gradient (DDPG), and others, dropout is generally not used for several reasons:

Instability: Dropout introduces randomness during training by setting some neurons to zero, which can be problematic in the context of RL algorithms. RL algorithms are sensitive to the stability of the learning process, and the randomness introduced by dropout can interfere with the learning dynamics and make it harder for the agent to learn an optimal policy.
Non-i.i.d. Data: In supervised learning tasks, dropout is effective because the training data is usually assumed to be independent and identically distributed (i.i.d.). However, in RL, the data generated during training is not i.i.d. The agent's interactions with the environment are sequential and correlated, making it challenging to apply dropout effectively.
Exploration vs. Exploitation: RL algorithms need to balance exploration (trying out new actions to discover the environment) and exploitation (leveraging known information to maximize rewards). Dropout can interfere with this balance by introducing unnecessary noise during both exploration and exploitation phases.
Overfitting vs. Underfitting: In supervised learning, the goal is often to prevent overfitting by using dropout. In RL, the main challenge is the trade-off between underfitting (not learning the optimal policy) and overfitting. Dropout may hinder the agent's ability to learn a good policy by preventing it from fully exploiting the information available in the data.
Instead of dropout, techniques like target network updates, experience replay, reward clipping, and exploration strategies (such as epsilon-greedy, softmax action selection, etc.) are commonly used in deep RL algorithms to stabilize training and improve performance.
----------------------------------------------------------------------------------------------------------------------------------------

----------------------------------------------------------------------------------------------------------------------------------------

Objectives in Reinforcement Learning

1. Generalization: Ensure the agent performs well not only on the specific scenarios seen during training but also on unseen scenarios. This is similar to avoiding overfitting in supervised learning.
2. Sample Efficiency: Train the agent to learn the optimal policy using a reasonable amount of data or interactions with the environment.
3. Stability and Convergence: Ensure the training process is stable and the agent’s policy converges to an optimal or near-optimal solution.
----------------------------------------------------------------------------------------------------------------------------------------

Stochastic policy 

A policy is a function that maps the input state to an action. A stochastic policy does not
choose action deterministically. It can be used to encourage exploration during training by allowing the agents
to occasionally execute low-confidence actions. Stochastic policy is always assumed in the formulation of policy
gradient based training algorithms.

----------------------------------------------------------------------------------------------------------------------------------------
