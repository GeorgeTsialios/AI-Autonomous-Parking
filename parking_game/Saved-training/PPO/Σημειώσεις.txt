Training 1)

Τα 1000 επεισόδια (600.000 steps) πήραν 25min στη CPU. Ωστόσο, παρατήρησα ότι ο πράκτορας των 600.000 steps κουνιόταν αργά. Αυτό μάλλον οφείλεται στο ότι ξέχασα το punishment για όταν κινείται αργά χωρίς να έχει παρκάρει για όταν η ταχύτητα του == 0 -το οποίο συμβαίνει σπάνια. Τώρα το αλλάζω, ώστε αυτό το punishment να εφαρμόζεται όταν η ταχύτητα είναι < 10.

Training 2)

Εκπαιδεύσαμε για 3.500.000 steps (5.800 επεισόδια) και χρειάστηκαν 2.5 ώρες. Ωστόσο, βλέπουμε πως η γραφική των average rewards δεν έχει βελτιωθεί καθόλου.

Training 3)

Έκανα κανονικοποίηση της εισόδου και των rewards. Στα 1000 επεισόδια (600.000 steps), η γραφική των average rewards είναι παρόμοια με αυτήν του Training 2. Επίσης, παρατηρώ ότι η γραφική στο τέλος έχει σταθεροποιηθεί στην τιμή -5.400. Αξιολογώντας τον πράκτορα των 600.000 steps, είδα ότι απλά μένει ακίνητος σε όλο το επεισόδιο, κι έτσι προκύπτει reward: -9 * 600 = -5.400.

Training 4)

Εφάρμοσα MaxAndSkip = 4, δηλ. ο πράκτορας παίρνει απόφαση κάθε 4 frames. Παρατηρώ ότι πάλι το reward σταθεροποιήθηκε στο -5.400. Ωστόσο, είδα ότι η εκπαίδευση διήρκησε περισσότερη ώρα (συγκεκριμένα τα 600.000 steps χρειάστηκαν 34'). Έψαξα τι ακριβώς κάνει η κλάση MaxAndSkipEnv. Αφενός κάνει αυτό που θέλω, δηλ. ο πράκτορας επαναλαμβάνει την ίδια action για 4 frames, το οποίο είναι επιθυμητό για να μην διαλέγει πάρα πολύ συχνά διαφορετικά actions (παίρνει υπερβολικά γρήγορα αποφάσεις), να έχει πιο ομαλή οδήγηση. Έτσι, επιταχύνεται κι η εκπαίδευση. Ωστόσο, η κλάση αυτή κάνει επίσης Max-Pooling Over Frames, δηλ. επιστρέφει τη μέγιστη τιμή του κάθε pixel σε αυτά τα skipped frames. Αυτό είναι κάτι επιθυμητό όταν έχουμε περιβάλλον Atari, αλλά όχι στην περίπτωση μου, όπου τα observations δεν είναι frames αλλά vectors. Για αυτό κατέβασα τον κώδικα της κλάσης και τον άλλαξα, ώστε να αφήσω μόνο την 1η λειτουργία (άρα την ονόμασα SkipEnv class).

Training 5)

Θα εκπαιδεύσω τον πράκτορα με την custom SkipEnv class μου. Παρατηρώ ότι η γραφική των average rewards με την SkipEnv class μου είναι παρόμοια με αυτήν με την MaxAndSkipEnv και συγκλίνει στην τιμή -5.400. Αυτό είναι λογικό καθώς πρέπει να αλλάξουμε την reward function για να αλλάξει αυτό. Ωστόσο, δεν παρατηρήθηκε speed up μεταξύ της SkipEnv class και της MaxAndSkipEnv, δηλ. τα fps τους είναι ίδια κι έτσι χρειάστηκαν πάλι 34' για τα 600.000 steps. Παρόλα αυτά, και οι δύο κλάσεις συγκλίνουν πολύ γρηγορότερα από την περίπτωση χωρίς frame skip (PPO_3). Για αυτό, θα τις προτιμήσουμε στη συνέχεια.

----------------------------------------------------------------------------------------------------------------------------------------
Προσπάθησα να επιταχύνω τη διαδικασία της εκπαίδευσης, ενισχύωντας το hardware πάνω στο οποίο εκπαιδεύονται τα μοντέλα. Συγκεκριμένα, η βιβλιοθήκη stableBaselines3 χρησιμοποιεί Vectorized Environments: Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Vectorized environments in reinforcement learning offer several advantages:

Parallelism: By running multiple instances of an environment in parallel, vectorized environments can significantly speed up the data collection process. This is particularly beneficial in reinforcement learning, where algorithms often require large amounts of data to learn effectively.

Efficiency: Vectorized environments can make better use of hardware resources, such as multi-core CPUs or GPUs, by distributing the workload across multiple cores or processing units. This leads to more efficient use of the available computational power.

Υπάρχουν 2 ειδών vectorized environments στη βιβλιοθήκη stable-baselines3:

-DummyVecEnv is a simple implementation of VecEnv (VecEnv is an abstract class that serves as the base for all vectorized environments in Gym) that runs each environment in a sequential manner within a single process. Despite its name, it's useful for testing and development because it simplifies debugging by avoiding the complexities of multiprocessing. However, because it runs environments sequentially, it does not offer the same performance benefits as parallelized implementations when it comes to training time. It is preferrable in the case of a simple environment such as Cartpole-v1, because then the overhead of multiprocess outweighs the environment computation time.
-SubprocVecEnv is an implementation of VecEnv that runs each environment in its own subprocess. This allows for true parallelism, taking advantage of multiple CPU cores, which can significantly speed up the training process. However, the use of multiple processes can make debugging more challenging and may introduce overhead due to inter-process communication.

Προσπάθησα να χρησιμοποιήσω SubprocVecEnv αντί του default DummyVecEnv, για να αξιοποιήσω τους 4 CPU cores του υπολογιστή μου, ωστόσο προέκυψε το εξής πρόβλημα: SubprocVecEnv creates separate processes for each environment instance to speed up training by running them in parallel. Each process needs to serialize (pickle) the environment to send it to the child processes. However, if your environment includes non-serializable objects like pygame.surface.Surface, this process fails. Για να λυθεί αυτό το πρόβλημα θα έπρεπε να αλλάξω σημαντικά τον κώδικα για το ίδιο το παιχνίδι και για αυτό δεν προχώρησα σε αυτήν την κατεύθυνση.


Προσπάθησα να εκπαιδεύσω το μοντέλο χρησιμοποιώντας τη GPU του υπολογιστή μου και όχι τη CPU. Γενικά, η εκπαίδευση αλγόριθμών μηχανικής μάθησης επιταχύνεται όταν χρησιμοποποιείται η GPU, καθώς προσφέρει significant improvements in computational efficiency, parallel processing capabilities, and overall training speed. Πιο συγκεκριμένα, πολλοί RL algorithms, especially those involving deep reinforcement learning (e.g., Deep Q-Networks (DQN), Proximal Policy Optimization (PPO)), rely heavily on deep neural networks. Training these networks involves a large number of matrix multiplications and other operations that benefit from GPU acceleration.
Για να χρησιμοποιήσουμε τη GPU κατά την εκπαίδευση, έπρεπε πρώτα να εγκαταστήσουμε το CUDA Toolkit της NVidia.Το CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and application programming interface (API) model created by NVIDIA. It enables developers to use NVIDIA GPUs for general purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units). Στη συνέχεια, έπρεπε να εγκαταστήσουμε τη cuDNN library της NVidia, which provides optimized implementations for deep learning operations. Τέλος, έπρεπε να κατεβάσουμε άλλη έκδοση της PyTorch (σε αυτήν τη βιβλιοθήκη είναι γραμμένη η stable-baselines3), η οποία να υποστηρίζει GPU Support.
----------------------------------------------------------------------------------------------------------------------------------------

Training 6)

Εκπαίδευσα χρησιμοποιώντας τη GPU μου για 1000 επεισόδια, ώστε να δω το speed up στον χρόνο εκπαίδευσης σε σχέση με πριν. Προφανώς η γραφική είναι σχεδόν πανομοιότυπη με πριν, αφού δεν άλλαξα τίποτα στο μοντέλο. Προς μεγάλη μου έκπληξη, προέκυψε πως η εκπαίδευση στη GPU ήταν πιο αργή (πήρε 49') από την εκπαίδευση στη CPU (πήρε 34'). Αυτό ίσως οφείλεται στο ότι το λάπτοπ μου διαθέτει μία παρωχημένη GPU (την NVIDIA GeForce MX110). Ευτυχώς, αλλάζοντας την παράμετρο device της κλάσης PPO ορίζουμε σε ποια συσκευή επιθυμούμε να πραγματοποιηθεί η εκπαίδευση.

Training 7)

Μέχρι στιγμής, το average reward του πράκτορα συγκλίνει στην τιμή -5.400. Αυτό σημαίνει ότι ο πράκτορας επιλέγει να μένει ακίνητος από την αρχή μέχρι το τέλος του επεισοδίου. Έτσι, παίρνει το punishment για το ότι δεν έχει παρκάρει (-7) και το punishment για το ότι έχει μικρή ταχύτητα (-2). Όμως, έτσι αποφεύγει το τεράστιο συγκριτικά punishment για τα collisions (-1000). Άρα, ο πράκτορας φοβάται τόσο μην συγκρουστεί με κάτι, που επιλέγει να μην παίξει καθόλου. Όμως έτσι, ο πράκτορας έχει κολλήσει σε αυτό το τοπικό μέγιστο (local optimum). Για αυτό ΜΕΙΩΣΑ το punishment της σύγκρουσης σε 100. Βλέπω ότι με 600.000 steps η γραφική συγκλίνει και πάλι στο -5400. Άρα, σταματάω την εκπαίδευση.

----------------------------------------------------------------------------------------------------------------------------------------
Επιλέγω smoothing = 0.99 για τις καμπύλες των average rewards. Έτσι, βλέπω την πιο εξομαλυμένη μορφή των καμπυλών η οποία θεωρώ μας δίνει την πιο χρήσιμη πληροφορία σχετικά με την τάση της γραφικής, χωρίς να αποπροσανατολιζόμαστε από τις συχνές βυθίσεις και μέγιστα. Άλλωστε, η κανονική μορφή της γραφικής (χωρίς smoothing) φαίνεται με αχνό χρώμα.
----------------------------------------------------------------------------------------------------------------------------------------

Training 8)

Αφαίρεσα το punishment για όταν δεν έχει παρκάρει. Θεωρώ ότι ίσως μπερδεύει τον πράκτορα και δεν μπορεί να εξηγήσει που οφείλεται. Το average reward είναι σίγουρα πολύ καλύτερο από πριν (όπως είναι λογικό), όμως δεν βλέπω ότι η γραφική ξεκινάει να συγκλίνει από νωρίς στην τιμή -1000, κι έτσι σταματάω την εκπαίδευση.

Training 9)

Άλλαξα τον συντελεστή εντροπίας ent_coef από 0 σε 0.01. Έτσι, εφαρμόζουμε Entropy regularization η οποία ενθαρρύνει τον πράκτορα να εξερευνήσει το περιβάλλον, ωθώντας σε πιο ίση κατανομή των actions του πράκτορα. Ωστόσο, πρέπει να μην το παρακάνουμε με το Entropy regularization, γιατί τότε οι actions του πράκτορα θα είναι random. Η γραφική παράσταση αυξάνεται και συγκλίνει κοντά στην τιμή 0. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει μάθει να μην μένει ακίνητος, ωστόσο συγκρούεται συχνά με άλλα αντικείμενα και η κίνηση του δεν είναι πάντα προς τον στόχο, αλλά μοιάζει τυχαία.

----------------------------------------------------------------------------------------------------------------------------------------
Γίνεται να συνεχίσεις την εκπαίδευση σε ένα ήδη υπάρχον μοντέλο και η γραφική στο Tensorboard να συνεχίσει από το σημείο που σταμάτησε, αλλά ΜΟΝΟ αν συνεχίσεις την πιο πρόσφατη εκπαίδευση. Αυτό γιατί, το νέο log αρχείο θα πάει να το βάλει στον πιο πρόσφατο φάκελο. Έτσι, αν έχεις παρεμβάλει άλλη εκπαίδευση μετά από αυτήν που θες να συνεχίσεις, θα πρέπει πρώτα να μετακινήσεις τον φάκελο της παρεμβαλόμενης αλλού. Μπορείς να τον επαναφέρεις έπειτα.
----------------------------------------------------------------------------------------------------------------------------------------

Training 10)

Άλλαξα το reward/punishment για το self.difference (αν βελτιώνει τη θέση του ή όχι) σε punishment ανάλογα με την απόσταση του από τη θέση. Έτσι, θέλω να μάθει να προτιμάει να βρίσκεται όσο το δυνατόν πιο κοντά στη θέση. Βλέπουμε την αναμενόμενη αυξητική τάση στην αρχή της γραφικής, ωστόσο μετά συγκλίνει, αρά δεν έχει νόημα να εκπαιδεύσουμε για παραπάνω επεισόδια. Επίσης, παρατηρώ ότι παρά το smoothing = 0.6 στη γραφική, αυτή κάνει απότομες βυθίσεις και μέγιστα. Αυτό μπορεί να οφείλεται στο ότι το learning rate του αλγορίθμου είναι πολύ μεγάλο κι έτσι ο πράκτορας είναι υπερευαίσθητος στα rewards που δέχεται και αντιδρά υπερβολικά με βάση αυτά.
Εξετάζοντας τον πράκτορα, παρατήρησα την εξής ενδιαφέρουσα συμπεριφορά. Ο πράκτορας έχει κολλήσει σε ένα local optimum, όπου η πολιτική του είναι απλά να κάνει γύρω γύρω την πίστα. Έτσι αποφεύγει το τεράστιο punishment (-100) του collision και το punishment για όταν μένει ακίνητος (-2). Παίρνει μόνο τα punishments λόγω της απόστασης του από τη θέση, τα οποία όμως είναι μικρά.

Training 11)

Δοκιμάζω να μειώσω το learning rate του αλγορίθμου. Η default τιμή του από την stable-baselines3 για τον αλγόριθμο PPO είναι η 0.0003 κι εγώ το υποδεκαπλασίασα, δηλ. το πήγα στην τιμή 0.00003. Η καμπύλη πλέον είναι πράγματι πιο ομαλή και ο πράκτορας μαθαίνει πιο αργά, αλλά σταθερά.

Training 12)

Επανέφερα το default learning rate. Σκέφτηκα για αρχή, να βγάλω τα punishments για collision και moving too slow. Πλέον απλά τιμωρώ τον πράκτορα με βάση την απόσταση του από τη θέση parking. Άρα, θέλω να τον δω να πηγαίνει στη θέση αυτή κάθε φορά, χωρίς να τον τρομάζουν οι συγκρούσεις με άλλα αντικείμενα. Μετά από 400.000 steps, παρατηρώ ότι η γραφική δεν έχει την αναμενόμενη αυξητική τάση και για αυτό σταματάω την εκπαίδευση. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κινείται πολύ αργά.

Training 13)

Επαναφέρω το punishment moving too slow. Η γραφική έχει μία πολύ μικρή αυξητική τάση. Εξετάζοντας τον πράκτορα, παρατηρώ ότι πλέον δεν κινείται αργά, αλλά δεν τον βλέπω να κινείται προς τη θέση parking.

Training 14)

Αυξάνω το punishment της απόστασης απ τη θέση, ώστε να μάθει ο πράκτορας ότι είναι σημαντικό να κινείται προς τη θέση parking. Δυστυχώς, παρατηρώ ότι η γραφική των rewards δεν αυξάνει καθόλου, αλλά συγκλίνει.

Training 15)

Ανακάλυψα ότι η MLPpolicy που χρησιμοποιεί ο PPO πράκτορας μου, χρησιμοποιεί την tanh activation function, η οποία δουλεύει καλύτερα με εισόδους κανονικοποιημένες στο διάστημα [-1,1]. Για αυτό, κανονικοποίησα όλες τις εισόδους στο διάστημα αυτό. Η γραφική έχει πολύ μικρή θετική κλίση και με αυτόν τον ρυθμό δεν θα φτάσει ποτέ σε θετικά αποτελέσματα.

Training 16)

Δοκιμάζω χωρίς το Frame Skip = 4. Η γραφική δεν δείχνει αυξητική τάση. Εξετάζοντας τον πράκτορα, βλέπω ότι μένει πολύ ακίνητος.

Training 17)

Αφαιρώ και τον ent_coef, μήπως αυτός ωθεί τον πράκτορα σε τυχαίες actions, με αποτέλεσμα να μένει ακίνητος. Η γραφική συγκλίνει στην ίδια τιμή με αυτήν της εκπαίδευσης 15, αλλά σε λιγότερο χρόνο. Άρα μάλλον δεν χρειαζόμαστε το Frame Skip και τον ent_coef.

-----------------------------------------------------------------------------------------------------------------------------
2. Consider the Complexity of Your Task
Begin with a simple NN architecture and gradually increase the network's size if necessary. The complexity of your task should guide the network size.

Simple tasks: 1-2 hidden layers with 64 neurons each.
Moderately complex tasks: 2-3 hidden layers with 128-256 neurons each.
Highly complex tasks: 3-4 hidden layers with 256-512 neurons each.

4. Monitor Training and Adjust
Track the performance of your agent during training using TensorBoard or other monitoring tools. If the agent is underperforming, consider increasing the number of neurons or layers.

5. Avoid Overfitting
A very large network can overfit to the training data. Regularly validate the agent's performance on different initial conditions (starting positions and parking spot locations) to ensure it generalizes well.
-----------------------------------------------------------------------------------------------------------------------------

Training 18)

Αλλάζω την τοπολογία του ΝΝ, αντιγράφοντας αυτήν του Samuel Arzt. Πλέον το ΝΝ αποτελείται από 3 hidden layers και το καθένα περιέχει 128 νευρώνες. Η γραφική των rewards είχε αρνητική κλίση lol. Εξετάζοντας τον πράκτορα, παρατήρησα ότι συγκρουόταν διαδοχικά με αντικείμενα. 

Training 19)

Επαναφέρω το punishment for collision. Η γραφική έχει απότομη θετική κλίση στην αρχή, ωστόσο αρχίζει να συγκλίνει σε μόλις 50k επεισόδια. Εξετάζοντας τον πράκτορα, παρατηρώ ότι κάθεται πολύ ακίνητος.

Training 20)

Αλλάζω τα βάρη, ώστε να δώσω έμφαση στο να μην μένει ακίνητος ο πράκτορας. Η γραφική του πράκτορα συγκλίνει πολύ γρήγορα.

Training 21)

Αλλάζω κάποιες hyperparameters, αντιγράφοντας τον Samuel Arzt. Η γραφική του πράκτορα συγκλίνει πολύ γρήγορα.


------------------ΑΝΑΚΑΛΥΨΑ ΤΟ BUG ΜΕ ΤΑ LEFT, RIGHT-----------------------------

Training 22)

Βάζω πάλι τον ent_coef=0.01 για να mix things up. Η γραφική παράσταση έχει μία μικρή θετική κλίση και ο πράκτορας βελτιώνει το average reward του αργά αλλά σταθερά. Με 4 million steps εκπαίδευσης, βλέπω ότι ο πράκτορας ακολουθεί τη στρατηγική του να τρέχει γύρω γύρω την πίστα. Μετά από 7M steps (5 hours 30 min) η γραφική συγκλίνει και δεν έχει νόημα να συνεχίσουμε την εκπαίδευση.