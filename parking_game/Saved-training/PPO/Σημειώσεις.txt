Training 1)

Τα 1000 επεισόδια (600.000 steps) πήραν 25min στη CPU. Ωστόσο, παρατήρησα ότι ο πράκτορας των 600.000 steps κουνιόταν αργά. Αυτό μάλλον οφείλεται στο ότι ξέχασα το punishment για όταν κινείται αργά χωρίς να έχει παρκάρει για όταν η ταχύτητα του == 0 -το οποίο συμβαίνει σπάνια. Τώρα το αλλάζω, ώστε αυτό το punishment να εφαρμόζεται όταν η ταχύτητα είναι < 10.

Training 2)

Εκπαιδεύσαμε για 3.500.000 steps (5.800 επεισόδια) και χρειάστηκαν 2.5 ώρες. ωστόσο, βλέπουμε πως η γραφική των average rewards δεν έχει βελτιωθεί καθόλου.

Training 3)

Έκανα κανονικοποίηση της εισόδου και των rewards. Στα 1000 επεισόδια (600.000 steps), η γραφική των average rewards είναι παρόμοια με αυτήν του Training 2. Επίσης, παρατηρώ ότι η γραφική στο τέλος έχει σταθεροποιηθεί στην τιμή -5.400. Αξιολογώντας τον πράκτορα των 600.000 steps, είδα ότι απλά μένει ακίνητος σε όλο το επεισόδιο, κι έτσι προκύπτει reward: -9 * 600 = -5.400.

Training 4)

Εφάρμοσα MaxAndSkip = 4, δηλ. ο πράκτορας παίρνει απόφαση κάθε 4 frames. Παρατηρώ ότι πάλι το reward σταθεροποιήθηκε στο -5.400. Ωστόσο, είδα ότι η εκπαίδευση διήρκησε περισσότερη ώρα (συγκεκριμένα τα 600.000 steps χρειάστηκαν 34'). Έψαξα τι ακριβώς κάνει η κλάση MaxAndSkipEnv. Αφενός κάνει αυτό που θέλω, δηλ. ο πράκτορας επαναλαμβάνει την ίδια action για 4 frames, το οποίο είναι επιθυμητό για να μην διαλέγει πάρα πολύ συχνά διαφορετικά actions (παίρνει υπερβολικά γρήγορα αποφάσεις), να έχει πιο ομαλή οδήγηση. Έτσι, επιταχύνεται κι η εκπαίδευση. Ωστόσο, η κλάση αυτή κάνει επίσης Max-Pooling Over Frames, δηλ. επιστρέφει τη μέγιστη τιμή του κάθε pixel σε αυτά τα skipped frames. Αυτό είναι κάτι επιθυμητό όταν έχουμε περιβάλλον Atari, αλλά όχι στην περίπτωση μου, όπου τα observations δεν είναι frames αλλά vectors. Για αυτό κατέβασα τον κώδικα της κλάσης και τον άλλαξα, ώστε να αφήσω μόνο την 1η λειτουργία (άρα την ονόμασα SkipEnv class).

Training 5)

Θα εκπαιδεύσω τον πράκτορα με την custom SkipEnv class μου. Παρατηρώ ότι η γραφική των average rewards με την SkipEnv class μου είναι παρόμοια με αυτήν με την MaxAndSkipEnv και συγκλίνει στην τιμή -5.400. Αυτό είναι λογικό καθώς πρέπει να αλλάξουμε την reward function για να αλλάξει αυτό. Ωστόσο, δεν παρατηρήθηκε speed up μεταξύ της SkipEnv class και της MaxAndSkipEnv, δηλ. τα fps τους είναι ίδια κι έτσι χρειάστηκαν πάλι 34' για τα 600.000 steps. Παρόλα αυτά, και οι δύο κλάσεις συγκλίνουν πολύ γρηγορότερα από την περίπτωση χωρίς frame skip (PPO_3). Για αυτό, θα τις προτιμήσουμε στη συνέχεια.

----------------------------------------------------------------------------------------------------------------------------------------
Προσπάθησα να επιταχύνω τη διαδικασία της εκπαίδευσης, ενισχύωντας το hardware πάνω στο οποίο εκπαιδεύονται τα μοντέλα. Συγκεκριμένα, η βιβλιοθήκη stableBaselines3 χρησιμοποιεί Vectorized Environments: Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Vectorized environments in reinforcement learning offer several advantages:

Parallelism: By running multiple instances of an environment in parallel, vectorized environments can significantly speed up the data collection process. This is particularly beneficial in reinforcement learning, where algorithms often require large amounts of data to learn effectively.

Efficiency: Vectorized environments can make better use of hardware resources, such as multi-core CPUs or GPUs, by distributing the workload across multiple cores or processing units. This leads to more efficient use of the available computational power.

Υπάρχουν 2 ειδών vectorized environments στη βιβλιοθήκη stable-baselines3:

-DummyVecEnv is a simple implementation of VecEnv (VecEnv is an abstract class that serves as the base for all vectorized environments in Gym) that runs each environment in a sequential manner within a single process. Despite its name, it's useful for testing and development because it simplifies debugging by avoiding the complexities of multiprocessing. However, because it runs environments sequentially, it does not offer the same performance benefits as parallelized implementations when it comes to training time. It is preferrable in the case of a simple environment such as Cartpole-v1, because then the overhead of multiprocess outweighs the environment computation time.
-SubprocVecEnv is an implementation of VecEnv that runs each environment in its own subprocess. This allows for true parallelism, taking advantage of multiple CPU cores, which can significantly speed up the training process. However, the use of multiple processes can make debugging more challenging and may introduce overhead due to inter-process communication.

Προσπάθησα να χρησιμοποιήσω SubprocVecEnv αντί του default DummyVecEnv, για να αξιοποιήσω τους 4 CPU cores του υπολογιστή μου, ωστόσο προέκυψε το εξής πρόβλημα: SubprocVecEnv creates separate processes for each environment instance to speed up training by running them in parallel. Each process needs to serialize (pickle) the environment to send it to the child processes. However, if your environment includes non-serializable objects like pygame.surface.Surface, this process fails. Για να λυθεί αυτό το πρόβλημα θα έπρεπε να αλλάξω σημαντικά τον κώδικα για το ίδιο το παιχνίδι και για αυτό δεν προχώρησα σε αυτήν την κατεύθυνση.


Προσπάθησα να εκπαιδεύσω το μοντέλο χρησιμοποιώντας τη GPU του υπολογιστή μου και όχι τη CPU. Γενικά, η εκπαίδευση αλγόριθμών μηχανικής μάθησης επιταχύνεται όταν χρησιμοποποιείται η GPU, καθώς προσφέρει significant improvements in computational efficiency, parallel processing capabilities, and overall training speed. Πιο συγκεκριμένα, πολλοί RL algorithms, especially those involving deep reinforcement learning (e.g., Deep Q-Networks (DQN), Proximal Policy Optimization (PPO)), rely heavily on deep neural networks. Training these networks involves a large number of matrix multiplications and other operations that benefit from GPU acceleration.
Για να χρησιμοποιήσουμε τη GPU κατά την εκπαίδευση, έπρεπε πρώτα να εγκαταστήσουμε το CUDA Toolkit της NVidia.Το CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and application programming interface (API) model created by NVIDIA. It enables developers to use NVIDIA GPUs for general purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units). Στη συνέχεια, έπρεπε να εγκαταστήσουμε τη cuDNN library της NVidia, which provides optimized implementations for deep learning operations. Τέλος, έπρεπε να κατεβάσουμε άλλη έκδοση της PyTorch (σε αυτήν τη βιβλιοθήκη είναι γραμμένη η stable-baselines3), η οποία να υποστηρίζει GPU Support.
----------------------------------------------------------------------------------------------------------------------------------------

Training 6)

Εκπαίδευσα χρησιμοποιώντας τη GPU μου για 1000 επεισόδια, ώστε να δω το speed up στον χρόνο εκπαίδευσης σε σχέση με πριν. Προφανώς η γραφική είναι σχεδόν πανομοιότυπη με πριν, αφού δεν άλλαξα τίποτα στο μοντέλο. Προς μεγάλη μου έκπληξη, προέκυψε πως η εκπαίδευση στη GPU ήταν πιο αργή (πήρε 49') από την εκπαίδευση στη CPU (πήρε 34'). Αυτό ίσως οφείλεται στο ότι το λάπτοπ μου διαθέτει μία παρωχημένη GPU (την NVIDIA GeForce MX110). Ευτύχως, αλλάζοντας την παράμετρο device της κλάσης PPO ορίζουμε σε ποια συσκευή επιθυμούμε να πραγματοποιηθεί η εκπαίδευση.

Training 7)

Μέχρι στιγμής, το average reward του πράκτορα συγκλίνει στην τιμή -5.400. Αυτό σημαίνει ότι ο πράκτορας επιλέγει να μένει ακίνητος από την αρχή μέχρι το τέλος του επεισοδίου. Έτσι, παίρνει το punishment για το ότι δεν έχει παρκάρει (-7) και το punishment για το ότι έχει μικρή ταχύτητα (-2). Όμως, έτσι αποφεύγει το τεράστιο συγκριτικά punishment για τα collisions (-1000). Άρα, ο πράκτορας φοβάται τόσο μην συγκρουστεί με κάτι, που επιλέγει να μην παίξει καθόλου. Όμως έτσι, ο πράκτορας έχει κολλήσει σε αυτό το τοπικό μέγιστο (local optimum). Για αυτό ΜΕΙΩΣΑ το punishment της σύγκρουσης σε 100. Βλέπω ότι με 300.000 steps η γραφική έχει αυξηθεί ελάχιστα σε σχέση με πριν και έχει αρχίσει να συγκλίνει. Άρα, σταματάω την εκπαίδευση.

Training 8)

Αφαίρεσα το punishment για όταν δεν έχει παρκάρει. Θεωρώ ότι ίσως μπερδεύει τον πράκτορα και δεν μπορεί να εξηγήσει που οφείλεται. Το average reward είναι σίγουρα πολύ καλύτερο από πριν (όπως είναι λογικό), όμως δεν βλέπω η γραφική να έχει μεγάλη θετική κλίση, δηλ. ο πράκτορας να βελτιώνει κατακόρυφα την απόδοση του. Για αυτό, σταμάτησα νωρίς την εκπαίδευση.

Training 9)

Άλλαξα τον συντελεστή εντροπίας ent_coef από 0 σε 0.01. Έτσι, εφαρμόζουμε Entropy regularization η οποία ενθαρρύνει τον πράκτορα να εξερευνήσει το περιβάλλον, ωθώντας σε πιο ίση κατανομή των actions του πράκτορα. Ωστόσο, πρέπει να μην το παρακάνουμε με το Entropy regularization, γιατί τότε οι actions του πράκτορα θα είναι random. Η γραφική παράσταση αυξάνεται και συγκλίνει κοντά στην τιμή 0. Εξετάζοντας τον πράκτορα, παρατηρώ ότι έχει μάθει να μην μένει ακίνητος, ωστόσο συγκρούεται συχνά με άλλα αντικείμενα και η κίνηση του δεν είναι πάντα προς τον στόχο, αλλά μοιάζει τυχαία.


----------------------------------------------------------------------------------------------------------------------------------------
Γίνεται να συνεχίσεις την εκπαίδευση σε ένα ήδη υπάρχον μοντέλο και η γραφική στο Tensorboard να συνεχίσει από το σημείο που σταμάτησε, αλλά ΜΟΝΟ αν συνεχίσεις την πιο πρόσφατη εκπαίδευση. Αυτό γιατί, το νέο log αρχείο θα πάει να το βάλει στον πιο πρόσφατο φάκελο. Έτσι, αν έχεις παρεμβάλει άλλη εκπαίδευση μετά από αυτήν που θες να συνεχίσεις, θα πρέπει πρώτα να μετακινήσεις τον φάκελο της παρεμβαλόμενης αλλού. Μπορείς να τον επαναφέρεις έπειτα.
----------------------------------------------------------------------------------------------------------------------------------------

Training 10)

Άλλαξα το reward/punishment για το self.difference (αν βελτιώνει τη θέση του ή όχι) σε punishment ανάλογα με την απόσταση του από τη θέση. Έτσι, θέλω να μάθει να προτιμάει να βρίσκεται όσο το δυνατόν πιο κοντά στη θέση.
