Training 1)

Ο πράκτορας δεν έχει μάθει τίποτα, παρά τις 6 ώρες εκπαίδευσης. Το ενδιαφέρον είναι ο τρόπος με τον οποίο εκμεταλλεύεται τη reward function. Συγκεκριμένα, κερδίζει +1 όταν η απόσταση του από τη θέση parking είναι πιο μικρή στο τωρινό frame από ότι ήταν στο προηγούμενο. Έτσι, ο πράκτορας έμαθε να κάνει ελάχιστες κινήσεις προς τα μπροστά σε κάθε frame, ώστε να επιβραβεύεται με +1 κάθε φορά.

Training 2)

Αλλάξαμε τη reward function, ώστε να δίνει επιβράβευση +1 στον πράκτορα όταν η απόσταση του από τη θέση parking είναι πιο μικρή στο τωρινό frame από ότι ήταν στο προηγούμενο ΑΛΛΑ ΚΑΙ η ταχύτητα του (κατά απόλυτη τιμή) είναι μεγαλύτερη από 0.5. Έτσι περιορίσαμε τις ελάχιστες, διαδοχικές κινήσεις του πράκτορα προς τα μπροστά.

50' εκπαίδευσης, με 25000 επεισόδια. Όμως ο πράκτορας στα πρώτα 15000 επεισόδια κάθεται ακίνητος. Γενικά τείνει να κάθεται συχνά ακίνητος, κι έτσι βλέπουμε ότι το mean reward κυμαίνεται κοντά στο 0. Αυτό όμως δεν είναι καλό, γιατί έτσι δεν εξερευνεί νέες καταστάσεις και δεν μαθαίνει.

Training 3)

Πλέον τιμωρούμε τον πράκτορα όταν κάθεται ακίνητος (φυσικά, εφόσον δεν βρίσκεται εντός της θέσης πάρκινγκ, γιατί τότε θέλουμε να το κάνει αυτό).

Το mean reward στις πρώτες επαναλήψεις είναι παααρα πολύ αρνητικό. Αυτό μάλλον οφείλεται στο γεγονός ότι ενθαρρύνουμε τον πράκτορα να κινείται κι έτσι αυτός συγκρούεται με το περιβάλλον, το οποίο οδηγεί σε μεγάλη τιμωρία. Ωστόσο, το mean reward αυξάνεται καθώς περνάνε τα επεισόδια, το οποίο είναι καλό σημάδι.

Έχουμε πρόβλημα με το μεγάλο μέγεθος του Q table. Συγκεκριμένα, έχουμε 354.375 στοιχεία στον Q table (ζευγάρια state-action). Μετά από 12 ώρες εκπαίδευσης και με πολύ υψηλή πιθανότητα εξερεύνησης (από το 1 έχει μειωθεί σταδιακά μέχρι το 0.82), παρατηρούμε ότι ο πράκτορας έχει εξερευνήσει 230.465 ζευγάρια state-action. Όσο συνεχίζεται η εκπαίδευση, αυτό το νούμερο αυξάνεται όλο και πιο αργά. Επομένως, θα πάρει πάρα πολύ χρόνο ο πράκτορας να γεμίσει τον Q table κι έκτοτε θα χρειαστεί χρόνος εκπαίδευσης για να κάνει exploit. Για αυτό, επιλέξαμε να αλλάξουμε το state space.

Training 4)

Τώρα μικρύναμε κι άλλο το state space, και έχουμε 9072 στοιχεία στον Q table (4 radar * 2 τιμές το κάθε radar * 3 τιμές για την ταχύτητα * 7 τιμές για τη γωνία * 3 τιμές για το self.difference). Το ενθαρρυντικό είναι ότι ο πράκτορας έμαθε ότι όταν ένα radar ενεργοποιείται, πρέπει να σταματήσει να πηγαίνει προς αυτήν την κατεύθυνση, ώστε να μην συγκρουστεί. Όμως, ο πράκτορας δυσκολεύεται να καταλάβει προς τα που πρέπει να πάει (δεν καταλαβαίνει το self.difference). Για αυτό, θα αλλάξω το self.difference σε relative position ως προς τη θέση parking (offset_x και offset_y).

Training 5)

Μικρύναμε το grid size, δηλ. βάλαμε τον πράκτορα να κάνει spawn κάθε φορά κοντά στη θέση parking. Έτσι, θα του είναι πιο εύκολο να παρκάρει κι ελπίζουμε να το ανακαλύψει κατά την εκπαίδευση.


					Τα 1000 επεισόδια αξιολόγησης είναι σταθερά, ίδια κάθε φορά.


Training 6)

Αλλάξαμε το state space, δίνοντας στον πράκτορα τις εξής πληροφορίες: 4 radar (στο διάστημα [0,1]), offset_x και offset_y (στο διάστημα [0,1]), parked (στο διάστημα [0,1]), vel (στο διάστημα [-1,1]), angle (στο διάστημα [-3,3]). Επίσης μειώσαμε τις δυνατές ενέργειες σε 6, αφαιρώντας το σκέτο left and right, γιατί συχνά ο πράκτορας καθόταν ακίνητος και τα επέλεγε. Έτσι, πλέον έχουμε 2688 καταστάσεις * 7 = 18816 στοιχείά στον Q table. Με εκπαίδευση σε 8000 επεισόδια, η γραφική των rewards έχει την αναμενόμενη αυξητική τάση ενώ προκύπτουν λίγα επιτυχή παρκαρίσματα κατά την εκπαίδευση. Ωστόσο, αξιολογώντας τον πράκτορα στα 1000 επεισόδια, προκύπτει 0.1% success rate.

Training 6B)

Αφαίρεσα το action Nothing και έθεσα το ελάχιστο velocity που θεωρείται ότι το αμάξι είναι ακίνητο σε 0.5. Τα αποτελέσματα είναι παρόμοια με πριν.

Training 6C)

Έθεσα ένα μικρό passive punishment στον πράκτορα, όσο δεν παρκάρει. Τα αποτελέσματα είναι παρόμοια με πριν.

Training 7) 

Αποθηκεύουμε τις καταστάσεις που περνάει ο πράκτορας σε αρχείο .txt και όταν εκπαιδεύουμε φορτώνουμε τις καταστάσεις αυτές σε λίστα. Έτσι, ο Q table περιέχει μόνο τις δυνατές καταστάσεις (αυτές όπου ο πράκτορας έχει περάσει σε προηγούμενη εκπαίδευση) κι όχι όλες τις 2688.
Ο πράκτορας εκπαιδεύτηκε για 4:17:18.

Training 8)

Ο πράκτορας εκπαιδεύτηκε για 6:04:23. Ωστόσο, πέρασε μόνο από 5336 καταστάσεις, ενώ το σύνολο των δυνατών καταστάσεων ανέρχεται σε 6162. Το περίεργο είναι ότι η γραφική παράσταση παρουσιάζει αυξητική τάση στην αρχή της εκπαίδευσης, αλλά μετά η απόδοση του πράκτορα μειώνεται και συγκλίνει σε μία μειωμένη τιμή.

Training 9)

Παρατηρούμε ξανά ότι μειώνεται η απόδοση του πράκτορα όταν τον εκπαιδεύουμε κάνοντας μόνο exploit. Επομένως, όταν σταματάμε το exploration-exploitation, θα σταματάμε και την εκπαίδευση. Επίσης, πλέον θεωρούμε τον πράκτορα ακίνητο όταν έχει ταχύτητα <0.5 κι αυτό έδειξε καλύτερα αποτελέσματα.
Με decay_rate = 0.01 (46 λεπτά εκπαίδευσης), το reward του πράκτορα έφτασε έως το -30 (ο πράκτορας έχει εξερευνήσει 3982 states)

Training 10)

Με decay_rate = 0.001 (1 ώρα και 50 λεπτά εκπαίδευσης), το reward του πράκτορα έφτασε πάλι έως το -30 (ο πράκτορας έχει εξερευνήσει 4667 states)

Training 11)

Με a = 0.1 αντί για 0.9 και decay_rate = 0.01 (24 λεπτά εκπαίδευσης), η γραφική είναι παρόμοια με την προηγούμενη, ενώ ο πράκτορας έχει εξερευνήσει 3976 states.

Training 12)

Πλέον, μειώνουμε και το a με την πάροδο του χρόνου, όπως και το e (με decay rate = 0.01). Τα αποτελέσματα σε 23' εκπαίδευσης είναι λίγο καλύτερα από πριν.

Training 13)

Χρησιμοποιούμε decay rate = 0.005 για το a και decay rate = 0.01 για το e. Σε 23' εκπαίδευσης, τα αποτελέσματα είναι λίγο καλύτερα, καθώς το reward έφτασε το -25.

Training 14)

Χρησιμοποιούμε decay rate = 0.005 για το a και το e. Αλλάξαμε το state space, αφαιρώντας το στοιχείο parked και βάζοντας 3 δυνατές τιμές στα offset_x και offset_y. Έτσι, αλλάξαμε και τη reward function ώστε να επιβραβεύουμε τον πράκτορα όταν βρίσκεται στην ίδια ευθεία με τη θέση parking στον άξονα x ή y. Το state space μας τώρα έχει 3024 καταστάσεις * 6 actions (βγάλαμε και το nothing για να μην κάθεται ακίνητος ο πράκτορας) = 18144 στοιχεία στον Q table. Μετά από 1000 επεισόδια εκπαίδευσης, ο πράκτορας έχει εξερευνήσει 5286 ζευγάρια state-action. Ωστόσο, αξιολογώντας τον πράκτορα σε 1000 επεισόδια, έχει 0% success rate.

Training 15)

Χρησιμοποιούμε decay rate = 0.0005 για το a και το e. Μετά από 8000 επεισόδια εκπαίδευσης, ο πράκτορας έχει αυξήσει το reward του και το μοντέλο των 7000 επεισοδίων έχει την καλύτερη επίδοση (0.9% success rate - δηλ. πάρκαρε σε 9 επεισόδια) στα 1000 επεισόδια αξιολόγησης.

Training 16)

Χρησιμοποιούμε decay rate = 0.0001 για το a και το e. Μετά από 20000 επεισόδια εκπαίδευσης, ο πράκτορας δεν έχει αυξήσει το reward του σε σχέση με πριν. Στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 17000 επεισοδίων έχει την καλύτερη επίδοση (0.7% success rate) στα 1000 επεισόδια αξιολόγησης. Άρα, η μεγαλύτερη εκπαίδευση δεν βοήθησε.

Training 17)

Επανέφερα το self.difference στη reward function. Προσπαθώ να δώσω στον πράκτορα να καταλάβει προς τα που βρίσκεται ο στόχος. Μετά από 8000 επεισόδια εκπαίδευσης, η γραφική των rewards έχει την αναμενόμενη αυξητική τάση. Στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 4500 επεισοδίων έχει την καλύτερη επίδοση (0.2% success rate) στα 1000 επεισόδια αξιολόγησης.

Training 18)

Μετά από 20000 επεισόδια εκπαίδευσης, η γραφική των rewards δεν έχει βελτιωθεί σε σχέση με πριν, ενώ το success rate κατά την εκπαίδευση είναι μικρότερο σε σχέση με πριν. Άρα η μεγαλύτερη εκπαίδευση δεν βοήθησε.

Training 18B)

Δοκιμάζω μία πολύ απλούστερη reward function. Μετά από 8000 επεισόδια εκπαίδευσης, η γραφική των rewards έχει την αναμενόμενη αυξητική τάση. Στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 5500 επεισοδίων έχει την καλύτερη επίδοση (1.2% success rate) στα 1000 επεισόδια αξιολόγησης.

Training 18C)

Μετά από 20000 επεισόδια εκπαίδευσης, η γραφική των rewards έχει την αναμενόμενη αυξητική τάση. Στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 5500 επεισοδίων έχει την καλύτερη επίδοση (1.2% success rate).

Training 19)

Εξετάζουμε την ιδέα να σταματάμε το επεισόδιο όταν ο πράκτορας συγκρουστεί με κάτι. Το σκεπτικό είναι πως έτσι δεν θα χάνουμε χρόνο με ανεπιθύμητες συμπεριφορές και θα επιταχύνουμε την εκπαίδευση. Έτσι, όταν ο πράκτορας συγκρούεται με άλλο αντικείμενο, τότε παίρνει μεγάλο punishment (-50) και προχωράμε στο επόμενο επεισόδιο. Χρησιμοποιούμε decay rate = 0.005 για το a και το e. Μετά από 1000 επεισόδια εκπαίδευσης, η γραφική των rewards δεν δείχνει αυξητική τάση, άρα ίσως χρειάζεται να μεταβάλουμε το decay rate.

Training 20)

Χρησιμοποιούμε decay rate = 0.0001 για το a και το e. Μετά από 20000 επεισόδια εκπαίδευσης, ο πράκτορας δεν έχει αυξήσει το reward του σε σχέση με πριν (μάλιστα, ήδη από το επεισόδιο 10000 το reward του πράκτορα συγκλίνει κοντά στην τιμή -90), επομένως η μεγαλύτερη εκπαίδευση δεν βοήθησε.

Training 14 -> 21)

Αλλάξαμε τη reward function από το Training 14, ώστε να βάλουμε και το self.difference. Χρησιμοποιούμε decay rate = 0.005 για το a και το e. Σε 1000 επεισόδια, η γραφική των rewards είναι καλύτερη από πριν. Επίσης, πετύχαμε 0.2% success rate σε 1000 επεισόδια αξιολόγησης.

Training 22)

Χρησιμοποιούμε decay rate = 0.0005 για το a και το e. Σε 8000 επεισόδια εκπαίδευσης (19 λεπτά), η γραφική των rewards φτάνει λίγο πιο ψηλά σε σχέση με πριν. Στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 8000 επεισοδίων έχει την καλύτερη επίδοση (1.8% success rate).

Training 23)

Χρησιμοποιούμε decay rate = 0.0001 για το a και το e. Σε 20000 επεισόδια εκπαίδευσης (46 λεπτά), η γραφική των rewards είναι λίγο χειρότερη σε σχέση με πριν, άρα δεν έχει νόημα να εκπαιδεύσουμε με περισσότερα επεισόδια. Στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 17000 επεισοδίων έχει την καλύτερη επίδοση (1.5% success rate). Ωστόσο, παρατηρώ το φαινόμενο ο πράκτορας να φτάνει πάνω από τη θέση παρκινγκ και να πηγαίνει μπρος πίσω, ώστε να μένει στην ίδια θέση και να παίρνει το μικρό reward για το ότι βρίσκεται στην κατάλληλη ευθεία στον άξονα y. Έτσι, ο πράκτορας κολλάει σε αυτό το τοπικό μέγιστο (local maximum). Για αυτό, θα μεταβάλουμε τη reward function.

Training 24)

Άλλαξα την reward function, τιμωρώντας λίγο τον πράκτορα όταν ΔΕΝ βρίσκεται στην κατάλληλη ευθεία στον άξονα x ή στον άξονα y. Με 8000 επεισόδια εκπαίδευσης, η γραφική των rewards έχει την αναμενόμενη αυξητική τάση. Στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 7500 επεισοδίων έχει την καλύτερη επίδοση (0.5% success rate).

Training 24B)

Τιμωρούμε περισσότερο τον πράκτορα όταν δεν βρίσκεται στην κατάλληλη ευθεία στον άξονα x ή στον άξονα y. Με 20000 επεισόδια εκπαίδευσης, η γραφική των rewards έχει την αναμενόμενη αυξητική τάση και συγκλίνει, ενώ στα 1000 επεισόδια αξιολόγησης, το μοντέλο των 18000 επεισοδίων έχει την καλύτερη επίδοση (1.9% success rate).

Training 24C)

Απλοποίησα τη reward function, αφαιρώντας τις παραπάνω τιμωρίες. Προσπαθώ να εξαλείψω το τοπικό μέγιστο κατά το οποίο ο πράκτορας κινείται μπρος-πίσω πάνω από τη θέση. Κατάφερα να εξαλείψω το συγκεκριμένο, ωστόσο εμφανίζονται άλλα τοπικά μέγιστα και ο πράκτορας συχνά επαναλαμβάνει την ίδια ακολουθία κινήσεων.

Training 24D)

Εκπαιδεύω τον πράκτορα στο εύκολο επεισόδιο (seed = 30). Μεταβάλοντας κατάλληλα τα βάρη της reward function, ο πράκτορας μαθαίνει να παρκάρει κάθε φορά. Προσπάθησα να εκπαιδεύσω αυτόν τον πράκτορα σε άλλο εύκολο επεισόδιο (seed = 41) και ξεκινώντας με α = 0.5, ωστόσο αξιολογώντας τον είδα ότι έμαθε να παρκάρει στο 2ο επεισόδιο και ξέχασε πως να παρκάρει στο 1ο.

Training 25)

Αλλάξαμε τα βάρη στην reward function. Δίνουμε περισσότερη έμφαση στο να μειώνει την απόσταση ο πράκτορας από τη θέση parking. Χρησιμοποιούμε decay rate = 0.0005 για το a και το e. Σε 8000 επεισόδια εκπαίδευσης (17 λεπτά), η γραφική των rewards φτάνει για πρώτη φορά στα θετικά. Επίσης, πετύχαμε 1.6% success rate στα 1000 επεισόδια αξιολόγησης.

Training 26)

Χρησιμοποιούμε decay rate = 0.0001 για το a και το e. Σε 20000 επεισόδια εκπαίδευσης (46 λεπτά), η γραφική των rewards είναι λίγο χειρότερη σε σχέση με πριν, άρα δεν έχει νόημα να εκπαιδεύσουμε με περισσότερα επεισόδια. Επίσης, πετύχαμε 0.4% success rate στα 1000 επεισόδια αξιολόγησης.



Training 22->27)

Το καλύτερο μοντέλο μέχρι στιγμής ήταν το 22, το οποίο πέτυχε 1.8% success rate. Τώρα το δοκιμάζουμε με fixed spawns για το αμάξι και τη θέση (μάλιστα το αμάξι κάνει spawn πολύ κοντά στη θέση). Με decay rate = 0.0005 για το a και το e, σε 8000 επεισόδια εκπαίδευσης η γραφική των rewards φτάνει σε πολύ ψηλά επίπεδα. Ωστόσο, αξιολογώντας το στο ίδιο επεισόδιο, παρατηρούμε ότι ναι μεν ο πράκτορας πετυχαίνει υψηλό reward (581.7), αλλά δεν παρκάρει ποτέ (η γραφική προφανώς είναι σταθερή, αφού στην αξιολόγηση ο πράκτορας κάνει μόνο exploit, άρα αφού κάθε επεισόδιο είναι το ίδιο, έχει την ίδια ακριβώς συμπεριφορά κάθε φορά). Εξετάζοντας τη συμπεριφορά του πράκτορα, βλέπουμε πως μπαίνει αμέσως στη θέση πάρκινγκ και ύστερα κινείται για τον υπόλοιπο χρόνο του επεισοδίου, κερδίζοντας έτσι συνεχώς rewards. Άρα, πρέπει να μεταβάλουμε τα rewards, ώστε να καταλάβει ο πράκτορας ότι είναι προτιμότερο να παρκάρει στη θέση, παρά να μπαινοβγαίνει σε αυτήν.

Training 28)

Μεταβάλαμε τις τιμές των rewards και εκπαιδεύσαμε εκ νέου για 8000 επεισόδια.

ΣΥΝΕΙΔΗΤΟΠΟΙΗΣΑ ΕΝΑ ΜΕΓΑΛΟ ΛΑΘΟΣ:

Τόσο καιρό, η ανίχνευση του τερματισμού του παιχνιδιού γινόταν με βάση τον χρόνο. Όταν το αμάξι παρέμενε ακίνητο (self.vel <0.5) στη θέση πάρκινγκ για 1 sec, τότε θεωρούταν επιτυχές παρκάρισμα. Αυτό δούλευε καλά όταν τρέχαμε τα επεισόδια σε πραγματικό χρόνο, αλλά όχι όταν τρέχαμε renderless, καθώς τότε ένα επεισόδιο δεν διαρκεί 30sec, αλλά 0.1 sec κατά μέσο όρο. Για αυτό ήταν πολύ σπάνιο να προκύψει επιτυχές επεισόδιο όταν εκπαιδεύαμε (renderless) αλλά και όταν αξιολογούσαμε με τα 1000 επεισόδια (renderless).

Διορθώσαμε αυτό το λάθος, αλλάζοντας τον έλεγχο από τον χρόνο στα frames. Δηλαδή πλέον δεν ελέγχουμε αν ο πράκτορας βρίσκεται ακίνητος στη θέση parking για 1 sec, αλλά για 20 frames, τα οποία σε πραγματικό χρόνο ισοδυναμούν με 1 sec (λόγω 20 fps). Ωστόσο, αυτό θα δουλεύει σωστά και renderless.

Έτσι, πλέον ο πράκτορας παρκάρει σωστά στο συγκεκριμένο επεισόδιο (seed = 30) κάθε φορά (success rate = 100%), και με 7500 επεισόδια εκπαίδευσης πετυχαίνει reward = 210.

Training 28B)

Τώρα δοκιμάζω να εκπαιδεύσω το μοντέλο 28 σε ένα άλλο συγκεκριμένο επεισόδιο (seed=22), πιο δύσκολο αυτή τη φορά. Με 8000 επεισόδια εκπαίδευσης έχουμε 0% success rate και πολύ μεγάλο αρνητικό reward (-4300). Με 15000 επεισόδια εκπαίδευσης, η γραφική είναι ίδια με πριν (συγκλίνει στο -4300) κι έτσι δεν έχει νόημα να συνεχίσουμε. Βλέπω ότι το πρόβλημα είναι πως το αμάξι πηγαίνει πολύ αργά και δεν προλαβαίνει να φτάσει στη θέση parking. Για αυτό μεγαλώνω το punishment for standing still από -0.4 σε -2.

Training 28C)

Με 8000 επεισόδια εκπαίδευσης, η γραφική είναι ίδια με πριν. Όμως βλέπουμε πως το αμάξι έχει μάθει να κινείται προς τη θέση παρκινγκ, δηλ. έχει κάνει πρόοδο σε σχέση με πριν. Ωστόσο, δεν προλαβαίνει να φτάσει στη θέση.

Training 28D)

Αυξήσαμε τα βήματα του κάθε επεισοδίου από 600 σε 900 (δηλ. 45 sec πραγματικού χρόνου), για να προλάβει ο πράκτορας να ανακαλύψει τη θέση παρκινγκ. Σε 8000 επεισόδια εκπαίδευσης (20 λεπτά), η γραφική των rewards είχε την επιθυμητή αυξητική τάση, ένω υπήρξαν λίγα επεισόδια κατά την εκπαίδευση στα οποία ο πράκτορας πάρκαρε.

Training 28E)

Σε 20000 επεισόδια εκπαίδευσης, τα αποτελέσματα είναι πανομοιότυπα με πριν. Άρα πρέπει να δοκιμάσουμε κάτι άλλο.

Training 28F)

Θα δοκιμάσουμε curriculum learning. Συγκεκριμένα, θα πάρουμε το μοντέλο 28 που έμαθε να παρκάρει στο εύκολο επεισόδιο (seed=30) και θα το εκπαιδεύσουμε στο δύσκολο επεισόδιο (seed=22). Σε 8000 επεισόδια εκπαίδευσης (900 steps/episode - μοντέλο 28 8000) και ξεκινώντας από a = 0.4, τα αποτελέσματα είναι παρόμοια με το 28D, δηλ. με αρχικές τιμές 0 στον Q table. Δοκίμασα και το μοντέλο 28 5000 με 600 steps/episode, αλλά τα αποτελέσματα είναι ίδια με πριν.

Training 28G)

Εκπαιδεύουμε τον πράκτορα διαδοχικά σε 10 διαφορετικά επεισόδια (8000 επεισόδια εκπαίδευσης). Η γραφική των rewards έχει την αναμενόμενη αυξητική τάση και βλέπουμε ότι πετυχαίνουμε καλά ποσοστά success rate κατά την εκπαίδευση, που αγγίζουν το 22%.

Training 28H)

Έθεσα το gamma από 0.9 σε 0.99. Τα αποτελέσματα ήταν χειρότερα. Το θέτω σε 0.95. Τα αποτελέσματα είναι καλύτερα από το 0.99, αλλά και πάλι χειρότερα από το 0.9.

Training 28I)

Γυρνάμε το gamma πίσω στο 0.9. Τώρα εκπαιδεύουμε για 20000 επεισόδια. Τα αποτελέσματα είναι χειρότερα από ότι στα 8000 επεισόδια.



Training 29)

Δοκίμασα να εκπαιδεύσω το μοντέλο του Training 28 χωρίς fixed spawns. Εκπαιδεύσαμε για 8000 επεισόδια και είχαμε mean reward -4353 ~ -4200, δλδ σε κάθε ένα από τα 600 frames του επεισοδίου έπαιρνε reward -7 επειδή δεν είχε παρκάρει. Αξιολογώντας το μοντέλο στα 1000 επεισόδια εκπαίδευσης, προέκυψε success rate μόλις 0.1%.

Training 29B)

Εκπαιδεύσαμε για 20000 επεισόδια. Τα αποτελέσματα είναι χειρότερα από πριν.


Training 22->30)

Δοκιμάζω να εκπαιδεύσω το μοντέλο του 22 (καλύτερο μέχρι στιγμής, 0.9% success rate), τώρα που διόρθωσα το bug με τον τερματισμό των επεισοδίων. Εκπαιδεύουμε για 8000 επεισόδια, η γραφική της εκπαίδευσης είναι σχεδόν ίδια με αυτήν του 22. To μοντέλο των 8000 επεισοδίων πέτυχε όμως μικρότερο success rate (0.1%). Ωστόσο, το μοντέλο των 5000 επεισοδίων, είχε success rate = 0.8%, αλλά χειρότερο reward. Επομένως, χρειάζεται να ενισχύσουμε τη σχέση μεταξύ reward και success. Για αυτό, θα επιβραβεύσουμε περισσότερο το terminated.

Training 31)

Με εκπαίδευση σε 8000 επεισόδια, η γραφική των rewards είναι ίδια με πριν, αλλά πλέον το μοντέλο των 8000 πετυχαίνει 0.8% success rate.

Training 32)

Με εκπαίδευση σε 20000 επεισόδια, η γραφική των rewards είναι ίδια με πριν. Στα 1000 επεισόδια αξιολόγησης, πετυχαίνει 0.2% success rate. Όμως, το μοντέλο των 19000 επεισοδίων, πετυχαίνει 4% success rate.

Training 33)

Με εκπαίδευση σε 360000 επεισόδια, η γραφική των rewards συγκλίνει λίγο πιο ψηλά από ότι πριν. Στα 1000 επεισόδια αξιολόγησης, πετυχαίνει 0.5% success rate. Όμως, το μοντέλο των 310000 επεισοδίων, πετυχαίνει 1.8% success rate. Άρα τα αποτελέσματα είναι χειρότερα από πριν.
