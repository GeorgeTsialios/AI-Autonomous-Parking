Problem: The set of sensors used (raycasts) is not correct for certain type of obstacles (lighting poles).That's because the AI can't tell where thin objects are unless they are exactly at a depth sensor. Therefore, it might crash into a lightning pole and learn nothing, because the sensors were clear.
Solution: Boxcasts / Spherecasts / Colliders instead of raycasts but they increase training time.


- In the video, the parking spot was predetermined. It would be more realistic to have a set starting point for the car, with one random empty parking spot. This way we avoid overfitting and achieve generalization to other parking spots. However, this will increase training time. Another approach would be to train the model just like in this video and then train it on other parking spots. That is called Transfer learning.
Overfitting is always a concern in RL. It depends on what your overall goal is. If your goal is to create a realistic automated parking assistant, you need to randomize the car spawn, the parking spot and even the parking lot shape, add parallel parking, pedestrians, other moving cars etc.


Proximal Policy Optimization (PPO)

Learning Mechanism: A policy gradient method that optimizes a surrogate objective function to prevent the policy from changing too much at each iteration, enhancing training stability.
Strengths: Strikes a balance between sample efficiency, ease of implementation, and training stability. Works well in both discrete and continuous action spaces.
Limitations: Can be slower to converge in environments with very high-dimensional action spaces.
Use Cases: Suitable for a wide range of problems, from robotic control to playing complex games like Dota 2.

Comparison:

Sample Efficiency: How many samples (state-action-reward tuples) the algorithm needs to learn an effective policy.
Stability and Reliability: The consistency of the learning process and the algorithm’s robustness to hyperparameter settings.
Scalability: How well the algorithm performs as the complexity of the environment increases.
Computational Requirements: The computational resources needed for training, including memory and processing power.
Suitability for Task Types: Whether the algorithm is better suited for discrete or continuous action spaces, and the complexity of the task.

STABLE BASELINES 3
set of reliable RL algorithms implementations. It has an easy to use API/ interface.
Reliable because:
- The implementation is tested against published results from the original paper of each algorithm until they reproduce the same result.
- They run many tests such as unit and performance tests and have a 96% code coverage
- They have an active community with more than 3 million downloads and they fix the issues the community finds
- Complete documentation with many έτοιμα examples and many tutorials


Similar to both unsupervised and supervised learning, reinforcement learning also involves two tasks: attribute selection and model selection. Attribute selection is defining the set of observations for the robot that best help it complete its objective (inputs), while model selection is defining the form of the policy (algorithm, mapping from observations to actions) and its parameters. In practice, training behaviors is an iterative process that may require changing the attribute and model choices.
All three branches of machine learning involve a training phase and an inference phase. At a high-level, the training phase involves building a model using the provided data (for example training the agent to park), while the inference phase involves applying this model to new, previously unseen, data (for example parking in a new environment).

PyTorch
Open source library. It facilitates training and inference on CPUs and GPUs in a desktop or server.
One part of training models with PyTorch is setting the values of certain model (algorithm) attributes (called hyperparameters). Finding the right values of these hyperparameters can require a few iterations. That's why we leverage a visualization tool called TENSORBOARD. It allows the visualization of certain agent attributes (for example rewards) throughout training which can be helpful in both building intuitions for the different hyperparameters and setting the optimal values for your environment.


INTRINSIC REWARDS

In reinforcement learning, the end goal for the Agent is to discover a behavior (a Policy) that maximizes a reward.
Typically, a reward is defined by your environment, and corresponds to reaching some goal. These are what we refer to as extrinsic rewards, as they are defined external of the learning algorithm.
Rewards, however, can be defined outside of the environment as well, to encourage the agent to behave in certain ways, or to aid the learning of the true extrinsic reward. We refer to these rewards as intrinsic reward signals. The total reward that the agent will learn to maximize can be a mix of extrinsic and intrinsic reward signals.

curiosity: represents an intrinsic reward signal that encourages exploration in sparse-reward environments


Imitation Learning: it is often easier to simply demonstrate the behavior we want an agent to perform, rather than attempting to have it learn via trial-and-error methods. Imitation Learning uses pairs of observations and actions from a demonstration of playing the game to learn a policy. Imitation learning can either be used alone or in conjunction with reinforcement learning to reduce the training time in sparse-reward environments.
The ML-Agents Toolkit provides a way to learn directly from demonstrations, as well as use them to help speed up reward-based training (RL). We include two algorithms called Behavioral Cloning (BC) and Generative Adversarial Imitation Learning (GAIL).
If you use them in comjuction with RL training (especially in environments that have sparse rewards) using pre-recorded demonstrations, you can generally enable both GAIL and Behavioral Cloning at LOW strengths in addition to having an extrinsic reward.


Reinforcement Learning Problems:
-The training data that are generated are dependent from the policy (the agent generates them by interacting with the environment). We do not have a static dataset like in supervised learning. This causes instability in the training process.
-Very high sensitivity to hyperparameter tuning.
Video from Arxiv Insights


Reinforcement learning (PP0) vs genetic/evolutionary algorithms (NEAT)
episodes	vs	  generations
reward function (provide feedback after each action of the agent) vs fitness function (only provide feedback at the end of an epoch)
rate an action vs rate an individual's entire performance
RL algorithms more goal oriented since they use mathematical formulas in order to adapt their policies to maximize their cumulative reward vs randomly changing the genetic information of the individuals which performed best in the previous generation
Genetic algoritmhs have the advantage of being much easier to implement, however the are not a good fit for the parking problem, since the rewards are too sparse.