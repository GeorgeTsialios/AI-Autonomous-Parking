
---------------------------------------------------ΚΕΦΑΛΑΙΟ 4 ΠΑΙΧΝΙΔΙ---------------------------------------------------------------------

- In the video, the parking spot was predetermined. It would be more realistic to have a set starting point for the car, with one random empty parking spot. This way we avoid overfitting and achieve generalization to other parking spots. However, this will increase training time. Another approach would be to train the model just like in this video and then train it on other parking spots. That is called Transfer learning.
Overfitting is always a concern in RL. It depends on what your overall goal is. If your goal is to create a realistic automated parking assistant, you need to randomize the car spawn, the parking spot and even the parking lot shape, add parallel parking, pedestrians, other moving cars etc.

----------------------------------------------------ΚΕΦΑΛΑΙΟ 5 ΕΚΠΑΙΔΕΥΣΗ-----------------------------------------------------------------


STABLE BASELINES 3
set of reliable RL algorithms implementations. It has an easy to use API/ interface.
Reliable because:
- The implementation is tested against published results from the original paper of each algorithm until they reproduce the same result.
- They run many tests such as unit and performance tests and have a 96% code coverage
- They have an active community with more than 3 million downloads and they fix the issues the community finds
- Complete documentation with many έτοιμα examples and many tutorials

TENSORBOARD
One part of training models with is setting the values of certain model (algorithm) attributes (called hyperparameters). Finding the right values of these hyperparameters can require a few iterations. That's why we leverage a visualization tool called TENSORBOARD. It allows the visualization of certain agent attributes (for example rewards) throughout training which can be helpful in both building intuitions for the different hyperparameters and setting the optimal values for your environment.

RL PROBLEMS
-The training data that are generated are dependent from the policy (the agent generates them by interacting with the environment). We do not have a static dataset like in supervised learning. This causes instability in the training process.
-Very high sensitivity to hyperparameter tuning.


----------------------------------------------------ΚΕΦΑΛΑΙΟ 6 ΣΥΓΚΡΙΣΗ--------------------------------------------------------------------

How do you compare different deep reinforcement Learning (RL) algorithms like DQN, PPO, A3C, etc.?
Comparing different Deep Reinforcement Learning (RL) algorithms like Deep Q-Network (DQN), Proximal Policy Optimization (PPO), Asynchronous Advantage Actor-Critic (A3C), and others requires considering various factors such as their learning mechanisms, performance, computational requirements, and suitability for different types of problems. Here's a comparative overview of these algorithms:

Sample Efficiency: How many samples (state-action-reward tuples) the algorithm needs to learn an effective policy.
Stability and Reliability: The consistency of the learning process and the algorithm’s robustness to hyperparameter settings.
Scalability: How well the algorithm performs as the complexity of the environment increases.
Computational Requirements: The computational resources needed for training, including memory and processing power.
Suitability for Task Types: Whether the algorithm is better suited for discrete or continuous action spaces, and the complexity of the task.

----------------------------------------------------MEΛΛΟΝΤΙΚΕΣ ΒΕΛΤΙΩΣΕΙΣ-----------------------------------------------------------------

Problem: The set of sensors used (raycasts) is not correct for certain type of obstacles (lighting poles).That's because the AI can't tell where thin objects are unless they are exactly at a depth sensor. Therefore, it might crash into a lightning pole and learn nothing, because the sensors were clear.
Solution: Boxcasts / Spherecasts / Colliders instead of raycasts but they increase training time.


INTRINSIC REWARDS

In reinforcement learning, the end goal for the Agent is to discover a behavior (a Policy) that maximizes a reward.
Typically, a reward is defined by your environment, and corresponds to reaching some goal. These are what we refer to as extrinsic rewards, as they are defined external of the learning algorithm.
Rewards, however, can be defined outside of the environment as well, to encourage the agent to behave in certain ways, or to aid the learning of the true extrinsic reward. We refer to these rewards as intrinsic reward signals. The total reward that the agent will learn to maximize can be a mix of extrinsic and intrinsic reward signals.

curiosity: represents an intrinsic reward signal that encourages exploration in sparse-reward environments


Imitation Learning: it is often easier to simply demonstrate the behavior we want an agent to perform, rather than attempting to have it learn via trial-and-error methods. Imitation Learning uses pairs of observations and actions from a demonstration of playing the game to learn a policy. Imitation learning can either be used alone or in conjunction with reinforcement learning to reduce the training time in sparse-reward environments.
The ML-Agents Toolkit provides a way to learn directly from demonstrations, as well as use them to help speed up reward-based training (RL). We include two algorithms called Behavioral Cloning (BC) and Generative Adversarial Imitation Learning (GAIL).
If you use them in comjuction with RL training (especially in environments that have sparse rewards) using pre-recorded demonstrations, you can generally enable both GAIL and Behavioral Cloning at LOW strengths in addition to having an extrinsic reward.


Reinforcement learning (PP0) vs genetic/evolutionary algorithms (NEAT)
episodes	vs	  generations
reward function (provide feedback after each action of the agent) vs fitness function (only provide feedback at the end of an epoch)
rate an action vs rate an individual's entire performance
RL algorithms more goal oriented since they use mathematical formulas in order to adapt their policies to maximize their cumulative reward vs randomly changing the genetic information of the individuals which performed best in the previous generation
Genetic algoritmhs have the advantage of being much easier to implement, however the are not a good fit for the parking problem, since the rewards are too sparse.
