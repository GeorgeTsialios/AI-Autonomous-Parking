<!DOCTYPE html>
<!-- saved from url=(0101)https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#how-to-evaluate-an-rl-algorithm -->
<html class="writer-html5" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation</title>
      <link rel="stylesheet" type="text/css" href="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/pygments.css">
      <link rel="stylesheet" type="text/css" href="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/theme.css">
      <link rel="stylesheet" type="text/css" href="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/copybutton.css">
      <link rel="stylesheet" type="text/css" href="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/baselines_theme.css">

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script type="text/javascript" async="" src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/analytics.js.download"></script><script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/jquery.js.download"></script>
        <script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/_sphinx_javascript_frameworks_compat.js.download"></script>
        <script data-url_root="../" id="documentation_options" src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/documentation_options.js.download"></script>
        <script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/doctools.js.download"></script>
        <script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/sphinx_highlight.js.download"></script>
        <script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/clipboard.min.js.download"></script>
        <script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/copybutton.js.download"></script>
        <script async="async" src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/readthedocs-doc-embed.js.download"></script>
    <script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/theme.js.download"></script>
    <link rel="index" title="Index" href="https://stable-baselines3.readthedocs.io/en/master/genindex.html">
    <link rel="search" title="Search" href="https://stable-baselines3.readthedocs.io/en/master/search.html">
    <link rel="next" title="Reinforcement Learning Resources" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl.html">
    <link rel="prev" title="Getting Started" href="https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html"> 

<!-- RTD Extra Head -->

<link rel="stylesheet" href="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/readthedocs-doc-embed.css" type="text/css">

<script type="application/json" id="READTHEDOCS_DATA">{"ad_free": false, "api_host": "https://readthedocs.org", "builder": "sphinx", "canonical_url": null, "docroot": "/docs/", "features": {"docsearch_disabled": false}, "global_analytics_code": "UA-17997319-1", "language": "en", "page": "guide/rl_tips", "programming_language": "py", "project": "stable-baselines3", "proxied_api_host": "/_", "source_suffix": ".rst", "subprojects": {}, "theme": "sphinx_rtd_theme", "user_analytics_code": "", "version": "master"}</script>

<!--
Using this variable directly instead of using `JSON.parse` is deprecated.
The READTHEDOCS_DATA global variable will be removed in the future.
-->
<script type="text/javascript">
READTHEDOCS_DATA = JSON.parse(document.getElementById('READTHEDOCS_DATA').innerHTML);
</script>

<script type="text/javascript" src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/readthedocs-analytics.js.download" async="async"></script>

<!-- end RTD <extrahead> -->
<script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/js" type="text/javascript" async=""></script><script src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/ethicalads.min.js.download" type="text/javascript" async="" id="ethicaladsjs"></script></head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">

          
          
          <a href="https://stable-baselines3.readthedocs.io/en/master/index.html" class="icon icon-home">
            Stable Baselines3
              <img src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/logo.webp" class="logo" alt="Logo">
          </a>
              <div class="version">
                master
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="https://stable-baselines3.readthedocs.io/en/master/search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs">
    <input type="hidden" name="check_keywords" value="yes">
    <input type="hidden" name="area" value="default">
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current" aria-expanded="true">
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html">Getting Started</a></li>
<li class="toctree-l1 current" aria-expanded="true"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#" aria-expanded="false"><button class="toctree-expand" title="Open/close menu"></button>Reinforcement Learning Tips and Tricks</a><ul>
<li class="toctree-l2 current" aria-expanded="true"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#general-advice-when-using-reinforcement-learning"><button class="toctree-expand" title="Open/close menu"></button>General advice when using Reinforcement Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#tl-dr">TL;DR</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#current-limitations-of-rl">Current Limitations of RL</a></li>
<li class="toctree-l3 current" aria-expanded="true"><a class="reference internal current" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#how-to-evaluate-an-rl-algorithm" aria-expanded="true">How to evaluate an RL algorithm?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#which-algorithm-should-i-use"><button class="toctree-expand" title="Open/close menu"></button>Which algorithm should I use?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions"><button class="toctree-expand" title="Open/close menu"></button>Discrete Actions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions-single-process">Discrete Actions - Single Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions-multiprocessed">Discrete Actions - Multiprocessed</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions"><button class="toctree-expand" title="Open/close menu"></button>Continuous Actions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions-single-process">Continuous Actions - Single Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions-multiprocessed">Continuous Actions - Multiprocessed</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#goal-environment">Goal Environment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#tips-and-tricks-when-creating-a-custom-environment">Tips and Tricks when creating a custom environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#tips-and-tricks-when-implementing-an-rl-algorithm">Tips and Tricks when implementing an RL algorithm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl.html">Reinforcement Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/algos.html">RL Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html">Vectorized Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html">Policy Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html">Using Custom Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html">Callbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html">Tensorboard Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html">RL Baselines3 Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html">SB3 Contrib</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/sbx.html">Stable Baselines Jax (SBX)</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/imitation.html">Imitation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/migration.html">Migrating from Stable-Baselines</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/checking_nan.html">Dealing with NaNs and infs</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/developer.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/save_format.html">On saving and loading</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/export.html">Exporting models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RL Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/base.html">Base RL Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html">A2C</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html">DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html">DQN</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/her.html">HER</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html">PPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/sac.html">SAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/td3.html">TD3</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html">Atari Wrappers</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/env_util.html">Environments Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/envs.html">Custom Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/distributions.html">Probability Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/evaluation.html">Evaluation Helper</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html">Gym Environment Checker</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/monitor.html">Monitor Wrapper</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/noise.html">Action Noise</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/utils.html">Utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/misc/changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/misc/projects.html">Projects</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="https://stable-baselines3.readthedocs.io/en/master/index.html">Stable Baselines3</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="https://stable-baselines3.readthedocs.io/en/master/index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Reinforcement Learning Tips and Tricks</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_tips.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="reinforcement-learning-tips-and-tricks">
<span id="rl-tips"></span><h1>Reinforcement Learning Tips and Tricks<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#reinforcement-learning-tips-and-tricks" title="Permalink to this heading"></a></h1>
<p>The aim of this section is to help you run reinforcement learning experiments.
It covers general advice about RL (where to start, which algorithm to choose, how to evaluate an algorithm, …),
as well as tips and tricks when using a custom environment or implementing an RL algorithm.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We have a <a class="reference external" href="https://www.youtube.com/watch?v=Ikngt0_DXJg">video on YouTube</a> that covers
this section in more details. You can also find the <a class="reference external" href="https://araffin.github.io/slides/rlvs-tips-tricks/">slides here</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We also have a <a class="reference external" href="https://youtu.be/eZ6ZEpCi6D8">video on Designing and Running Real-World RL Experiments</a>, slides <a class="reference external" href="https://araffin.github.io/slides/design-real-rl-experiments/">can be found online</a>.</p>
</div>
<section id="general-advice-when-using-reinforcement-learning">
<h2>General advice when using Reinforcement Learning<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#general-advice-when-using-reinforcement-learning" title="Permalink to this heading"></a></h2>
<section id="tl-dr">
<h3>TL;DR<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#tl-dr" title="Permalink to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Read about RL and Stable Baselines3</p></li>
<li><p>Do quantitative experiments and hyperparameter tuning if needed</p></li>
<li><p>Evaluate the performance using a separate test environment (remember to check wrappers!)</p></li>
<li><p>For better performance, increase the training budget</p></li>
</ol>
<p>Like any other subject, if you want to work with RL, you should first read about it (we have a dedicated <a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl.html">resource page</a> to get you started)
to understand what you are using. We also recommend you read Stable Baselines3 (SB3) documentation and do the <a class="reference external" href="https://github.com/araffin/rl-tutorial-jnrr19">tutorial</a>.
It covers basic usage and guide you towards more advanced concepts of the library (e.g. callbacks and wrappers).</p>
<p style="
    color: greenyellow;
">Reinforcement Learning differs from other machine learning methods in several ways. The data used to train the agent is collected
through interactions with the environment by the agent itself (compared to supervised learning where you have a fixed dataset for instance).
This dependence can lead to vicious circle: if the agent collects poor quality data (e.g., trajectories with no rewards), then it will not improve and continue to amass
bad trajectories.</p>
<p style="
    color: greenyellow;
">This factor, among others, explains that results in RL may vary from one run to another (i.e., when only the seed of the pseudo-random generator changes).
For this reason, you should always do several runs to have quantitative results.</p>
<p>Good results in RL are generally dependent on finding appropriate hyperparameters. Recent algorithms (PPO, SAC, TD3, DroQ) normally require little hyperparameter tuning,
however, <em>don’t expect the default ones to work</em> on any environment.</p>
<p>Therefore, we <em>highly recommend you</em> to take a look at the <a class="reference external" href="https://github.com/DLR-RM/rl-baselines3-zoo">RL zoo</a> (or the original papers) for tuned hyperparameters.
A best practice when you apply RL to a new problem is to do automatic hyperparameter optimization. Again, this is included in the <a class="reference external" href="https://github.com/DLR-RM/rl-baselines3-zoo">RL zoo</a>.</p>
<p>When applying RL to a custom problem, you should always normalize the input to the agent (e.g. using <code class="docutils literal notranslate"><span class="pre">VecNormalize</span></code> for PPO/A2C)
and look at common preprocessing done on other environments (e.g. for <a class="reference external" href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/">Atari</a>, frame-stack, …).
Please refer to <em>Tips and Tricks when creating a custom environment</em> paragraph below for more advice related to custom environments.</p>
</section>
<section id="current-limitations-of-rl">
<h3>Current Limitations of RL<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#current-limitations-of-rl" title="Permalink to this heading"></a></h3>
<p>You have to be aware of the current <a class="reference external" href="https://www.alexirpan.com/2018/02/14/rl-hard.html">limitations</a> of reinforcement learning.</p>
<p>Model-free RL algorithms (i.e. all the algorithms implemented in SB) are usually <em>sample inefficient</em>. They require a lot of samples (sometimes millions of interactions) to learn something useful.
That’s why most of the successes in RL were achieved on games or in simulation only. For instance, in this <a class="reference external" href="https://www.youtube.com/watch?v=aTDkYFZFWug">work</a> by ETH Zurich, the ANYmal robot was trained in simulation only, and then tested in the real world.</p>
<p style="
    color: greenyellow;
">As a general advice, to obtain better performances, you should augment the budget of the agent (number of training timesteps).</p>
<p>In order to achieve the desired behavior, expert knowledge is often required to design an adequate reward function.
This <em>reward engineering</em> (or <em>RewArt</em> as coined by <a class="reference external" href="http://www.freekstulp.net/">Freek Stulp</a>), necessitates several iterations. As a good example of reward shaping,
you can take a look at <a class="reference external" href="https://xbpeng.github.io/projects/DeepMimic/index.html">Deep Mimic paper</a> which combines imitation learning and reinforcement learning to do acrobatic moves.</p>
<p>One last limitation of RL is the instability of training. That is to say, you can observe during training a huge drop in performance.
This behavior is particularly present in <code class="docutils literal notranslate"><span class="pre">DDPG</span></code>, that’s why its extension <code class="docutils literal notranslate"><span class="pre">TD3</span></code> tries to tackle that issue.
Other method, like <code class="docutils literal notranslate"><span class="pre">TRPO</span></code> or <code class="docutils literal notranslate"><span class="pre">PPO</span></code> make use of a <em>trust region</em> to minimize that problem by avoiding too large update.</p>
</section>
<section id="how-to-evaluate-an-rl-algorithm">
<h3>How to evaluate an RL algorithm?<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#how-to-evaluate-an-rl-algorithm" title="Permalink to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pay attention to environment wrappers when evaluating your agent and comparing results to others’ results. Modifications to episode rewards
or lengths may also affect evaluation results which may not be desirable. Check <code class="docutils literal notranslate"><span class="pre">evaluate_policy</span></code> helper function in <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/common/evaluation.html#eval"><span class="std std-ref">Evaluation Helper</span></a> section.</p>
</div>
<p>Because most algorithms use exploration noise during training, you need a separate test environment to evaluate the performance
of your agent at a given time. It is recommended to periodically evaluate your agent for <code class="docutils literal notranslate"><span class="pre">n</span></code> test episodes (<code class="docutils literal notranslate"><span class="pre">n</span></code> is usually between 5 and 20)
and average the reward per episode to have a good estimate.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We provide an <code class="docutils literal notranslate"><span class="pre">EvalCallback</span></code> for doing such evaluation. You can read more about it in the <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html#callbacks"><span class="std std-ref">Callbacks</span></a> section.</p>
</div>
<p style="
    color: greenyellow;
">As some policies are stochastic by default (e.g. A2C or PPO), you should also try to set <cite>deterministic=True</cite> when calling the <cite>.predict()</cite> method,
this frequently leads to better performance.
Looking at the training curve (episode reward function of the timesteps) is a good proxy but underestimates the agent true performance.</p>
<p>We highly recommend reading <a class="reference external" href="https://arxiv.org/abs/2304.01315">Empirical Design in Reinforcement Learning</a>, as it provides valuable insights for best practices when running RL experiments.</p>
<p>We also suggest reading <a class="reference external" href="https://arxiv.org/abs/1709.06560">Deep Reinforcement Learning that Matters</a> for a good discussion about RL evaluation,
and <a class="reference external" href="https://araffin.github.io/post/rliable/">Rliable: Better Evaluation for Reinforcement Learning</a> for comparing results.</p>
<p>You can also take a look at this <a class="reference external" href="https://openlab-flowers.inria.fr/t/how-many-random-seeds-should-i-use-statistical-power-analysis-in-deep-reinforcement-learning-experiments/457">blog post</a>
and this <a class="reference external" href="https://github.com/hill-a/stable-baselines/issues/199">issue</a> by Cédric Colas.</p>
</section>
</section>
<section id="which-algorithm-should-i-use">
<h2>Which algorithm should I use?<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#which-algorithm-should-i-use" title="Permalink to this heading"></a></h2>
<p>There is no silver bullet in RL, you can choose one or the other depending on your needs and problems.
The first distinction comes from your action space, i.e., do you have discrete (e.g. LEFT, RIGHT, …)
or continuous actions (ex: go to a certain speed)?</p>
<p>Some algorithms are only tailored for one or the other domain: <code class="docutils literal notranslate"><span class="pre">DQN</span></code> supports only discrete actions, while <code class="docutils literal notranslate"><span class="pre">SAC</span></code> is restricted to continuous actions.</p>
<p>The second difference that will help you decide is whether you can parallelize your training or not.
If what matters is the wall clock training time, then you should lean towards <code class="docutils literal notranslate"><span class="pre">A2C</span></code> and its derivatives (PPO, …).
Take a look at the <a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html">Vectorized Environments</a> to learn more about training with multiple workers.</p>
<p>To accelerate training, you can also take a look at <a class="reference external" href="https://github.com/araffin/sbx">SBX</a>, which is SB3 + Jax, it has less features than SB3 but can be up to 20x faster than SB3 PyTorch thanks to JIT compilation of the gradient update.</p>
<p>In sparse reward settings, we either recommend using either dedicated methods like HER (see below) or population-based algorithms like ARS (available in our <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib"><span class="std std-ref">contrib repo</span></a>).</p>
<p>To sum it up:</p>
<section id="discrete-actions">
<h3>Discrete Actions<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions" title="Permalink to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This covers <code class="docutils literal notranslate"><span class="pre">Discrete</span></code>, <code class="docutils literal notranslate"><span class="pre">MultiDiscrete</span></code>, <code class="docutils literal notranslate"><span class="pre">Binary</span></code> and <code class="docutils literal notranslate"><span class="pre">MultiBinary</span></code> spaces</p>
</div>
<section id="discrete-actions-single-process">
<h4>Discrete Actions - Single Process<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions-single-process" title="Permalink to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">DQN</span></code> with extensions (double DQN, prioritized replay, …) are the recommended algorithms.
We notably provide <code class="docutils literal notranslate"><span class="pre">QR-DQN</span></code> in our <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib"><span class="std std-ref">contrib repo</span></a>.
<code class="docutils literal notranslate"><span class="pre">DQN</span></code> is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer).</p>
</section>
<section id="discrete-actions-multiprocessed">
<h4>Discrete Actions - Multiprocessed<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#discrete-actions-multiprocessed" title="Permalink to this heading"></a></h4>
<p>You should give a try to <code class="docutils literal notranslate"><span class="pre">PPO</span></code> or <code class="docutils literal notranslate"><span class="pre">A2C</span></code>.</p>
</section>
</section>
<section id="continuous-actions">
<h3>Continuous Actions<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions" title="Permalink to this heading"></a></h3>
<section id="continuous-actions-single-process">
<h4>Continuous Actions - Single Process<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions-single-process" title="Permalink to this heading"></a></h4>
<p>Current State Of The Art (SOTA) algorithms are <code class="docutils literal notranslate"><span class="pre">SAC</span></code>, <code class="docutils literal notranslate"><span class="pre">TD3</span></code>, <code class="docutils literal notranslate"><span class="pre">CrossQ</span></code> and <code class="docutils literal notranslate"><span class="pre">TQC</span></code> (available in our <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib"><span class="std std-ref">contrib repo</span></a> and <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/sbx.html#sbx"><span class="std std-ref">SBX (SB3 + Jax) repo</span></a>).
Please use the hyperparameters in the <a class="reference external" href="https://github.com/DLR-RM/rl-baselines3-zoo">RL zoo</a> for best results.</p>
<p>If you want an extremely sample-efficient algorithm, we recommend using the <a class="reference external" href="https://twitter.com/araffin2/status/1575439865222660098">DroQ configuration</a> in <a class="reference external" href="https://github.com/araffin/sbx">SBX</a> (it does many gradient steps per step in the environment).</p>
</section>
<section id="continuous-actions-multiprocessed">
<h4>Continuous Actions - Multiprocessed<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#continuous-actions-multiprocessed" title="Permalink to this heading"></a></h4>
<p>Take a look at <code class="docutils literal notranslate"><span class="pre">PPO</span></code>, <code class="docutils literal notranslate"><span class="pre">TRPO</span></code> (available in our <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/guide/sb3_contrib.html#sb3-contrib"><span class="std std-ref">contrib repo</span></a>) or <code class="docutils literal notranslate"><span class="pre">A2C</span></code>. Again, don’t forget to take the hyperparameters from the <a class="reference external" href="https://github.com/DLR-RM/rl-baselines3-zoo">RL zoo</a> for continuous actions problems (cf <em>Bullet</em> envs).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Normalization is critical for those algorithms</p>
</div>
</section>
</section>
<section id="goal-environment">
<h3>Goal Environment<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#goal-environment" title="Permalink to this heading"></a></h3>
<p>If your environment follows the <code class="docutils literal notranslate"><span class="pre">GoalEnv</span></code> interface (cf <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/her.html#her"><span class="std std-ref">HER</span></a>), then you should use
HER + (SAC/TD3/DDPG/DQN/QR-DQN/TQC) depending on the action space.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is an important hyperparameter for experiments with <a class="reference internal" href="https://stable-baselines3.readthedocs.io/en/master/modules/her.html#her"><span class="std std-ref">HER</span></a></p>
</div>
</section>
</section>
<section id="tips-and-tricks-when-creating-a-custom-environment">
<h2>Tips and Tricks when creating a custom environment<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#tips-and-tricks-when-creating-a-custom-environment" title="Permalink to this heading"></a></h2>
<p>If you want to learn about how to create a custom environment, we recommend you read this <a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html">page</a>.
We also provide a <a class="reference external" href="https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb">colab notebook</a> for a concrete example of creating a custom gym environment.</p>
<p>Some basic advice:</p>
<ul class="simple">
<li><p style="
    color: greenyellow;
">always normalize your observation space if you can, i.e. if you know the boundaries</p></li>
<li><p style="
    color: greenyellow;
">normalize your action space and make it symmetric if it is continuous (see potential problem below) A good practice is to rescale your actions so that they lie in [-1, 1]. This does not limit you, as you can easily rescale the action within the environment.</p></li>
<li><p style="
    color: greenyellow;
">start with a shaped reward (i.e. informative reward) and a simplified version of your problem</p></li>
<li><p>debug with random actions to check if your environment works and follows the gym interface (with <code class="docutils literal notranslate"><span class="pre">check_env</span></code>, see below)</p></li>
</ul>
<p>Two important things to keep in mind when creating a custom environment are avoiding breaking the Markov assumption
and properly handle termination due to a timeout (maximum number of steps in an episode).
For example, if there is a time delay between action and observation (e.g. due to wifi communication), you should provide a history of observations as input.</p>
<p>Termination due to timeout (max number of steps per episode) needs to be handled separately.
You should return <code class="docutils literal notranslate"><span class="pre">truncated</span> <span class="pre">=</span> <span class="pre">True</span></code>.
If you are using the gym <code class="docutils literal notranslate"><span class="pre">TimeLimit</span></code> wrapper, this will be done automatically.
You can read <a class="reference external" href="https://arxiv.org/abs/1712.00378">Time Limit in RL</a>, take a look at the <a class="reference external" href="https://youtu.be/eZ6ZEpCi6D8">Designing and Running Real-World RL Experiments video</a> or <a class="reference external" href="https://www.youtube.com/watch?v=Ikngt0_DXJg">RL Tips and Tricks video</a> for more details.</p>
<p>We provide a helper to check that your environment runs without error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre id="codecell0"><span></span><span class="kn">from</span> <span class="nn">stable_baselines3.common.env_checker</span> <span class="kn">import</span> <span class="n">check_env</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">CustomEnv</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="c1"># It will check your custom environment and output additional warnings if needed</span>
<span class="n">check_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell0">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p>If you want to quickly try a random agent on your environment, you can also do:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre id="codecell1"><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">YourEnv</span><span class="p">()</span>
<span class="n">obs</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Random action</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre><button class="copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#codecell1">
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copy" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#000000" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <title>Copy to clipboard</title>
  <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
  <rect x="8" y="8" width="12" height="12" rx="2"></rect>
  <path d="M16 8v-2a2 2 0 0 0 -2 -2h-8a2 2 0 0 0 -2 2v8a2 2 0 0 0 2 2h2"></path>
</svg>
    </button></div>
</div>
<p><strong>Why should I normalize the action space?</strong></p>
<p style="
    color: greenyellow;
">Most reinforcement learning algorithms rely on a Gaussian distribution (initially centered at 0 with std 1) for continuous actions.
So, if you forget to normalize the action space when using a custom environment,
this can harm learning and can be difficult to debug (cf attached image and <a class="reference external" href="https://github.com/hill-a/stable-baselines/issues/473">issue #473</a>).</p>
<figure class="align-default">
<img alt="../_images/mistake.png" src="./Reinforcement Learning Tips and Tricks — Stable Baselines3 2.4.0a7 documentation_files/mistake.webp">
</figure>
<p>Another consequence of using a Gaussian distribution is that the action range is not bounded.
That’s why clipping is usually used as a bandage to stay in a valid interval.
A better solution would be to use a squashing function (cf <code class="docutils literal notranslate"><span class="pre">SAC</span></code>) or a Beta distribution (cf <a class="reference external" href="https://github.com/hill-a/stable-baselines/issues/112">issue #112</a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This statement is not true for <code class="docutils literal notranslate"><span class="pre">DDPG</span></code> or <code class="docutils literal notranslate"><span class="pre">TD3</span></code> because they don’t rely on any probability distribution.</p>
</div>
</section>
<section id="tips-and-tricks-when-implementing-an-rl-algorithm">
<h2>Tips and Tricks when implementing an RL algorithm<a class="headerlink" href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#tips-and-tricks-when-implementing-an-rl-algorithm" title="Permalink to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We have a <a class="reference external" href="https://www.youtube.com/watch?v=7-PUg9EAa3Y">video on YouTube about reliable RL</a> that covers
this section in more details. You can also find the <a class="reference external" href="https://araffin.github.io/slides/tips-reliable-rl/">slides online</a>.</p>
</div>
<p>When you try to reproduce a RL paper by implementing the algorithm, the <a class="reference external" href="http://joschu.net/docs/nuts-and-bolts.pdf">nuts and bolts of RL research</a>
by John Schulman are quite useful (<a class="reference external" href="https://www.youtube.com/watch?v=8EcdaCk9KaQ">video</a>).</p>
<p>We <em>recommend following those steps to have a working RL algorithm</em>:</p>
<ol class="arabic simple">
<li><p>Read the original paper several times</p></li>
<li><p>Read existing implementations (if available)</p></li>
<li><p style="
    color: greenyellow;
">Try to have some “sign of life” on toy problems</p></li>
<li><p style="
    color: greenyellow;
">Validate the implementation by making it run on harder and harder envs (you can compare results against the RL zoo).
You usually need to run hyperparameter optimization for that step.</p></li>
</ol>
<p>You need to be particularly careful on the shape of the different objects you are manipulating (a broadcast mistake will fail silently cf. <a class="reference external" href="https://github.com/hill-a/stable-baselines/pull/76">issue #75</a>)
and when to stop the gradient propagation.</p>
<p>Don’t forget to handle termination due to timeout separately (see remark in the custom environment section above),
you can also take a look at <a class="reference external" href="https://github.com/DLR-RM/stable-baselines3/issues/284">Issue #284</a> and <a class="reference external" href="https://github.com/DLR-RM/stable-baselines3/issues/633">Issue #633</a>.</p>
<p>A personal pick (by @araffin) for environments with gradual difficulty in RL with continuous actions:</p>
<ol class="arabic simple">
<li><p>Pendulum (easy to solve)</p></li>
<li><p>HalfCheetahBullet (medium difficulty with local minima and shaped reward)</p></li>
<li><p>BipedalWalkerHardcore (if it works on that one, then you can have a cookie)</p></li>
</ol>
<p>in RL with discrete actions:</p>
<ol class="arabic simple">
<li><p>CartPole-v1 (easy to be better than random agent, harder to achieve maximal performance)</p></li>
<li><p>LunarLander</p></li>
<li><p>Pong (one of the easiest Atari game)</p></li>
<li><p>other Atari games (e.g. Breakout)</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html" class="btn btn-neutral float-left" title="Getting Started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl.html" class="btn btn-neutral float-right" title="Reinforcement Learning Resources" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr><div><div id="rtd-sidebar" data-ea-publisher="readthedocs" data-ea-type="image" data-ea-manual="true" data-ea-style="stickybox" class="ethical-rtd" data-ea-keywords="python|readthedocs-project-603701|readthedocs-project-stable-baselines3|reinforcement-learning|rl" data-ea-campaign-types="community|house|paid"></div></div>

  <div role="contentinfo">
    <p>© Copyright 2021-2024, Stable Baselines3.
      <span class="commit">Revision <code>bd3c0c65</code>.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org/">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="Versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: master
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions"><!-- Inserted RTD Footer -->

<div class="injected">

  

      
      
      
      <dl>
        <dt>Versions</dt>
        
        <dd class="rtd-current-item">
          <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html">master</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v2.3.2/guide/rl_tips.html">v2.3.2</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v2.3.0/guide/rl_tips.html">v2.3.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v2.2.1/guide/rl_tips.html">v2.2.1</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v2.1.0/guide/rl_tips.html">v2.1.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v2.1.0_a/guide/rl_tips.html">v2.1.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v2.0.0/guide/rl_tips.html">v2.0.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v1.8.0/guide/rl_tips.html">v1.8.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v1.7.0/guide/rl_tips.html">v1.7.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v1.6.2/guide/rl_tips.html">v1.6.2</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v1.5.0/guide/rl_tips.html">v1.5.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v1.4.0/guide/rl_tips.html">v1.4.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v1.0/guide/rl_tips.html">v1.0</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/v0.11.1/guide/rl_tips.html">v0.11.1</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/sde/guide/rl_tips.html">sde</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/feat-gymnasium-support/guide/rl_tips.html">feat/gymnasium-support</a>
        </dd>
        
        <dd>
          <a href="https://stable-baselines3.readthedocs.io/en/chores-update-deps/guide/rl_tips.html">chores/update-deps</a>
        </dd>
        
      </dl>
      
      

      
      
      <dl>
        <dt>Downloads</dt>
        
        <dd><a href="https://stable-baselines3.readthedocs.io/_/downloads/en/master/pdf/">PDF</a></dd>
        
        <dd><a href="https://stable-baselines3.readthedocs.io/_/downloads/en/master/htmlzip/">HTML</a></dd>
        
        <dd><a href="https://stable-baselines3.readthedocs.io/_/downloads/en/master/epub/">Epub</a></dd>
        
      </dl>
      
      

      
      <dl>
        
        <!-- These are kept as relative links for internal installs that are http -->
        <dt>On Read the Docs</dt>
        <dd>
          <a href="https://readthedocs.org/projects/stable-baselines3/">Project Home</a>
        </dd>
        <dd>
          <a href="https://readthedocs.org/projects/stable-baselines3/builds/">Builds</a>
        </dd>
        <dd>
          <a href="https://readthedocs.org/projects/stable-baselines3/downloads/">Downloads</a>
        </dd>
      </dl>
      

      

      
      <dl>
        <dt>On GitHub</dt>
        <dd>
          <a href="https://github.com/DLR-RM/stable-baselines3/blob/master/docs/guide/rl_tips.rst">View</a>
        </dd>
        
        <dd>
          <a href="https://github.com/DLR-RM/stable-baselines3/edit/master/docs/guide/rl_tips.rst">Edit</a>
        </dd>
        
      </dl>
      
      

      
      <dl>
        <dt>Search</dt>
        <dd>
          <div style="padding: 6px;">
            
            <form id="flyout-search-form" class="wy-form" target="_blank" action="https://readthedocs.org/projects/stable-baselines3/search/" method="get">
              <input type="text" name="q" aria-label="Search docs" placeholder="Search docs">
              </form>
          </div>
        </dd>
      </dl>
      

      <hr>

      
        <small>
          <span>Hosted by <a href="https://readthedocs.org/">Read the Docs</a></span>
          <span> · </span>
          <a href="https://docs.readthedocs.io/page/privacy-policy.html">Privacy Policy</a>
        </small>
      

      

</div>
</div>
  </div>
<script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 


</body></html>