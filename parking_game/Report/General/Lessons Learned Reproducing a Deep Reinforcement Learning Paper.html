<!DOCTYPE html>
<!-- saved from url=(0105)http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Lessons Learned Reproducing a Deep Reinforcement Learning Paper</title>
    <meta name="description" content="">

    <link rel="stylesheet" href="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/main.css">
    <link rel="canonical" href="http://amid.fish/reproducing-deep-rl">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script type="text/javascript" async="" src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/analytics.js.download"></script><script type="text/javascript" async="" src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/js"></script><script async="" src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/js(1)"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-111504376-1');
</script>

    <link type="application/atom+xml" rel="alternate" href="http://amid.fish/feed.xml" title="Amid Fish">
<script type="text/javascript" async="" src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/embed.js.download"></script></head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="http://amid.fish/">Amid Fish</a>

    <nav class="site-nav">
      <a href="http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
	  <h1 class="post-title">Lessons Learned Reproducing a Deep Reinforcement Learning Paper</h1>
    <p class="post-meta">Apr 6, 2018</p>
  </header>

  <article class="post-content">
    <p>There are a lot of neat things going on in deep reinforcement learning. One of
the coolest things from last year was OpenAI and DeepMind’s work on training an
agent using feedback from a human rather than a classical reward signal.
There’s a great blog post about it at <a href="https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/">Learning from Human
Preferences</a>,
and the original paper is at <a href="https://arxiv.org/pdf/1706.03741.pdf">Deep Reinforcement Learning from Human
Preferences</a>.</p>

<figure style="width: 400px; margin-right: auto; margin-left: auto">
  <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/humanfeedbackjump.gif">
  <figcaption style="padding-top: 10px">Learn some deep reinforcement learning, and you too can train a noodle to do backflip. From <a href="https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/">Learning from Human Preferences</a>.</figcaption>
</figure>

<p>I’ve seen a few recommendations that reproducing papers is a good way of
levelling up machine learning skills, and I decided this could be an
interesting one to try with. It was indeed a <a href="http://github.com/mrahtz/learning-from-human-preferences">super fun
project</a>, and I’m
happy to have tackled it - but looking back, I realise it wasn’t exactly the
experience I thought it would be.</p>

<p>If you’re thinking about reproducing papers too, here are some notes on what
surprised me about working with deep RL.</p>

<hr>

<p>First, in general, <strong>reinforcement learning turned out to be a lot trickier
than expected</strong>.</p>

<p style="
    color: rgb(0 255 0);
">A big part of it is that right now, reinforcement learning is really sensitive.
There are a lot of details to get <em>just</em> right, and if you don’t get them
right, it can be difficult to diagnose where you’ve gone wrong.</p>

<p>Example 1: after finishing the basic implementation, training runs just weren’t
succeeding. I had all sorts of ideas about what the problem might be, but after
a couple of months of head scratching, it turned out to be because of problems
with normalization of rewards and pixel data at a key stage<sup id="fnref:normproblems" role="doc-noteref"><a href="http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM#fn:normproblems" class="footnote" rel="footnote">1</a></sup>.
Even with the benefit of hindsight, there were no obvious clues pointing in
that direction: the accuracy of the reward predictor network the pixel data
went into was just fine, and it took a long time to occur to me to examine the
rewards predicted carefully enough to notice the reward normalization bug.
Figuring out what the problem was happened almost accidentally, noticing a
small inconsistency that eventually lead to the right path.</p>

<p>Example 2: doing a final code cleanup, I realised I’d implemented dropout kind
of wrong. The reward predictor network takes as input a pair of video clips,
each processed identically by two networks with shared weights. If you add
dropout and you’re not careful about giving it the same random seed in each
network, you’ll drop out differently for each network, so the video clips won’t
be processed identically. As it turned out, though, fixing it completely broke
training, despite prediction accuracy of the network looking exactly the same!</p>

<figure style="width: 740px; margin-right: auto; margin-left: auto">
  <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/broken_dropout.png">
  <figcaption style="padding-top: 10px">Spot which one is broken. Yeah, I don't see it either.</figcaption>
</figure>

<p>I get the impression this is a pretty common story (e.g. <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Deep Reinforcement
Learning Doesn’t Work Yet</a>).
My takeaway is that, starting a reinforcement learning project, you should
<strong>expect to get stuck like you get stuck on a math problem</strong>. It’s not like my
experience of programming in general so far where you get stuck but there’s
usually a clear trail to follow and you can get unstuck within a couple of days
at most. It’s more like when you’re trying to solve a puzzle, there are no
clear inroads into the problem, and the only way to proceed is to try things
until you find the key piece of evidence or get the key spark that lets you
figure it out.</p>

<p>A corollary is to <strong>try and be as sensitive as possible in noticing
confusion</strong>.</p>

<p>There were a lot of points in this project where the only clues came from
noticing some small thing that didn’t make sense. For example, at some point it
turned out that taking the difference between frames as features made things
work much better. It was tempting to just forge ahead with the new features,
but I realised I was confused about <em>why</em> it made such a big difference for the
simple environment I was working with back then. It was only by following that
confusion and realising that taking the difference between frames zeroed out
the background that gave the hint of a problem with normalization.</p>

<p>I’m not entirely sure how to make one’s mind do more of this, but my best
guesses at the moment are:</p>

<ul>
  <li>Learn to <strong>recognise what confusion <em>feels</em> like</strong>. There are a lot of
different shades of the “something’s not quite right” feeling. Sometimes it’s
code you know is ugly. Sometimes it’s worry about wasting time on the wrong
thing. But sometimes it’s that <em>you’ve seen something you didn’t expect</em>:
confusion. Being able to recognise that exact shade of discomfort is
important, so that you can…</li>
  <li>Develop the habit of following through on confusion. There are some
sources of discomfort that it can be better to ignore in the moment (e.g.
code smell while prototyping), but confusion isn’t one of them. It seems
important to really <strong>commit yourself to <em>always</em> investigate whenever you
notice confusion</strong>.</li>
</ul>

<p>In any case: expect to get stuck for several weeks at a time. (And have
confidence you will be able to get to the other side if you keep at it, paying
attention to those small details.)</p>

<hr>

<p>Speaking of differences to past programming experiences, a second major
learning experience was the <strong>difference in mindset required for working with
long iteration times</strong>.</p>

<p>Debugging seems to involve four basic steps:</p>

<ul>
  <li>Gather evidence about what the problem might be.</li>
  <li>Form hypotheses about the problem based on the evidence you have so far.</li>
  <li>Choose the most likely hypothesis, implement a fix, and see what happens.</li>
  <li>Repeat until the problem goes away.</li>
</ul>

<p>In most of the programming I’ve done before, I’ve been used to rapid feedback.
If something doesn’t work, you can make a change and see what difference it
makes within seconds or minutes. Gathering evidence is very cheap.</p>

<p>In fact, in rapid-feedback situations, gathering evidence can be a lot cheaper
than forming hypotheses. Why spend 15 minutes carefully considering everything
that could be causing what you see when you can check the first idea that jumps
to mind in a fraction of that (and gather more evidence in the process)? To put
it another way: if you have rapid feedback, you can narrow down the hypothesis
space a lot faster by trying things than thinking carefully.</p>

<p>If you keep that strategy when each run takes 10 hours, though, you can easily
waste a <em>lot</em> of time. Last run didn’t work? OK, I think it’s this thing. Let’s
set off another run to check. Coming back the next morning: still doesn’t work?
OK, maybe it’s this other thing. Let’s set off another run. A week later, you
still haven’t solved the problem.</p>

<p>Doing multiple runs at the same time, each trying a different thing, can help
to some extent, but a) unless you have access to a cluster you can end up
racking up a lot of costs on cloud compute (see below), and b) because of the
kinds of difficulties with reinforcement learning mentioned above, if you try
to iterate too quickly, you might never realise what kind of evidence you
actually need.</p>

<p>Switching from <strong>experimenting a lot and thinking a little</strong> to <strong>experimenting
a little and thinking a lot</strong> was a key turnaround in productivity. When
debugging with long iteration times, you really need to <em>pour</em> time into the
hypothesis-forming step - thinking about what all the possibilities are, how
likely they seem on their own, and how likely they seem in light of everything
you’ve seen so far. Spend as much time as you need, even if it takes 30
minutes, or an hour. Reserve experiments for once you’ve fleshed out the
hypothesis space as thoroughly as possible and know which pieces of evidence
would allow you to best distinguish between the different possibilities.</p>

<p>(It’s especially important to be deliberate about this if you’re working on
something as a side project. If you’re only working on it for an hour a day and
each iteration takes a day to run, the number of runs you can do per week ends
up feeling a precious commodity you have to make the most of.  It’s easy to
then feel a sense of pressure to spend your working hour each day rushing to
figure out something to do for that day’s run. Another turnaround was being
willing to spend several days just <em>thinking</em>, not starting any runs, until I
felt really confident I had a strong hypothesis about what the problem was.)</p>

<p style="
    color: rgb(0 255 0);
">A key enabler of the switch to thinking more was <strong>keeping a much more detailed
work log</strong>. Working without a log is fine when each chunk of progress takes
less than a few hours, but anything longer than that and it’s easy to forget
what you’ve tried so far and end up just going in circles. The log format I
converged on was:</p>

<ul>
  <li style="
    color: rgb(0 255 0);
">Log 1: what specific output am I working on right now?</li>
  <li style="
    color: rgb(0 255 0);
">Log 2: thinking out loud - e.g. hypotheses about the current problem, what to
work on next</li>
  <li style="
    /* color: rgb(0 255 0); */
">Log 3: record of currently ongoing runs along with a short reminder of what
question each run is supposed to answer</li>
  <li style="
    color: rgb(0 255 0);
">Log 4: results of runs (TensorBoard graphs, any other significant
observations), separated by type of run (e.g. by environment the agent is
being trained in)</li>
</ul>

<p>I started out with relatively sparse logs, but towards the end of the project
my attitude moved more towards “log absolutely everything going through my
head”. The overhead was significant, but I think it was worth it - partly
because some debugging required cross-referencing results and thoughts that
were days or weeks apart, and partly for (at least, this is my impression)
general improvements in thinking quality from the massive upgrade to effective
mental RAM.</p>

<figure style="width: 740px; margin-right: auto; margin-left: auto">
  <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/rl_logs.jpg">
  <figcaption style="padding-top: 10px">A typical day's log.</figcaption>
</figure>

<hr>

<p>In terms of <strong>getting the most out of the experiments you do run</strong>, there are
two things I started experimenting with towards the end of the project which
seem like they could be helpful in the future.</p>

<p>First, adopting an attitude of <strong>log all the metrics you can</strong> to maximise the
amount of evidence you gather on each run. There are obvious metrics like
training/validation accuracy, but it might also be worth spending a good chunk
of time at the start of the project brainstorming and researching which other
metrics might be important for diagnosing potential problems.</p>

<p>I might be making this recommendation partly out of hindsight bias where I
<em>know</em> which metrics I should have started logging earlier. It’s hard to
predict which metrics will be useful in advance. Still, heuristics that might
be useful are:</p>

<ul>
  <li>For every important component in the system, consider what <em>can</em> be measured
about it. If there’s a database, measure how quickly it’s growing in size.
If there’s a queue, measure how quickly items are being processed.</li>
  <li>For every complex procedure, measure how long different parts of it take. If
you’ve got a training loop, measure how long each batch takes to run. If
you’ve got a complex inference procedure, measure how long each sub-inference
takes. Those times are going to help a lot for performance debugging later
on, and can sometimes reveal bugs that are otherwise hard to spot. (For
example, if you see something taking longer and longer, it might be because
of a memory leak.)</li>
  <li>Similarly, consider profiling memory usage of different components. Small
memory leaks can be indicative of all sorts of things.</li>
</ul>

<p>Another strategy is to look at what other people are measuring. In the context
of deep reinforcement learning, John Schulman has some good tips in his <a href="https://www.youtube.com/watch?v=8EcdaCk9KaQ">Nuts
and Bolts of Deep RL talk</a>
(<a href="http://joschu.net/docs/nuts-and-bolts.pdf">slides</a>; <a href="https://github.com/williamFalcon/DeepRLHacks">summary
notes</a>). For policy gradient
methods, I’ve found policy entropy in particular to be a good indicator of
whether training is going anywhere - much more sensitive than per-episode
rewards.</p>

<figure style="margin-right: auto; margin-left: auto">
  <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/entropies.png">
  <figcaption style="padding-top: 10px">Examples of unhealthy and healthy
  policy entropy graphs. Failure mode 1 (left): convergence to constant entropy (random choice among a subset of actions). Failure mode 2 (centre): convergence to zero entropy (choosing the same action every time). Right: policy entropy from a successful Pong training run.</figcaption>
</figure>

<p>When you do see something suspicious in metrics recorded, remembering to
<em>notice confusion</em>, err on the side of assuming it’s something important rather
than just e.g. an inefficient implementation of some data structure. (I missed
a multithreading bug for several months by ignoring a small but mysterious
decay in frames per second.)</p>

<p>Debugging is much easier if you can see all your metrics in one place. I like
to have as much as possible on TensorBoard. Logging arbitrary metrics with
TensorFlow can be awkward, though, so <strong>consider checking out
<a href="https://github.com/mrahtz/easy-tf-log">easy-tf-log</a></strong>, which provides an easy
<code class="language-plaintext highlighter-rouge">tflog(key, value)</code> interface without any extra setup.</p>

<p>A second thing that seems promising for getting more out of runs is
<strong>taking the time to try and predict failure in advance</strong>.</p>

<p>Thanks to hindsight bias, failures often seem obvious in retrospect. But the
<em>really</em> frustrating thing is when the failure mode is obvious <em>before you’ve
even observed what it was</em>. You know when you’ve set off a run, you come back
the next day, you see it’s failed, and even before you’ve investigated, you
realise, “Oh, it must have been because I forgot to set the frobulator”? That’s
what I’m talking about.</p>

<p>The neat thing is that sometimes you can trigger that kind of
half-hindsight-realisation in advance. It does take conscious effort, though -
really stopping for a good five minutes before launching a run to think about
what might go wrong. The particular script I found most helpful to go through
was: <sup id="fnref:murphyjitsu" role="doc-noteref"><a href="http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM#fn:murphyjitsu" class="footnote" rel="footnote">2</a></sup></p>

<ol>
  <li>Ask yourself, “How surprised would I be if this run failed?”</li>
  <li>If the answer is ‘not very surprised’, put yourself in the shoes of
future-you where the run <em>has</em> failed, and ask, “If I’m here, what might
have gone wrong?”</li>
  <li>Fix whatever comes to mind.</li>
  <li>Repeat until the answer to question 1 is “very surprised” (or at least “as
surprised as I can get”).</li>
</ol>

<p>There are always going to be failures you couldn’t have predicted, and
sometimes you still miss obvious things, but this does at least seem to <em>cut
down</em> on the number of times something fails in a way you feel <em>really</em> stupid
for not having thought of earlier.</p>

<hr>

<p>Finally, though, <strong>the biggest surprise with this project was just how long it
took</strong> - and related, the amount of compute resources it needed.</p>

<p>The first surprise was in terms of calendar time. My original estimate was that
as a side project it would take about 3 months. It actually took around <em>8
months</em>. (And the original estimate was supposed to be pessimistic!) Some of
that was down to underestimating how many hours each stage would take, but a
big chunk of the underestimate was failing to anticipate other things coming up
outside the project.  It’s hard to say how well this generalises, but <strong>for
side projects, taking your original (already pessimistic) time estimates and
doubling them</strong> might not be a bad rule-of-thumb.</p>

<p>The more interesting surprise was in how many hours each stage actually took.
The main stages of my initial project plan were basically:</p>

<figure style="margin-right: auto; margin-left: auto">
  <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/pretime.png" style="width: 396px; margin-right: auto;
  margin-left: auto">
</figure>

<p>Here’s how long each stage <em>actually</em> took.</p>

<figure style="margin-right: auto; margin-left: auto">
  <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/posttime.png" style="width: 671px; margin-right: auto;
  margin-left: auto">
</figure>

<p>It wasn’t writing code that took a long time - it was debugging it. In fact,
getting it working on even a <a href="https://github.com/mrahtz/gym-moving-dot">supposedly-simple
environment</a> took <em>four times</em> as
long as initial implementation. (This is the first side project where I’ve been
keeping track of hours, but experiences with past machine learning projects
have been similar.)</p>

<p>(Side note: be careful about designing from scratch what you hope should be an
‘easy’ environment for reinforcement learning. In particular, think carefully
about a) whether your rewards really convey the right information to be able to
solve the task - yes, this is easy to mess up - and b) whether rewards depend
only on previous observations or also on current action. The latter, in
particular, might be relevant if you’re doing any kind of reward prediction,
e.g. with a critic.)</p>

<p><strong>Another surprise was the amount of compute time needed.</strong> I was lucky having
access to my university’s cluster - only CPU machines, but that was fine for
some tasks. For work which needed a GPU (e.g. to iterate quickly on some small
part) or when the cluster was too busy, I experimented with two cloud services:
VMs on <a href="https://console.cloud.google.com/projectselector/compute/instances?supportedpurview=project">Google Cloud Compute
Engine</a>,
and <a href="http://floydhub.com/">FloydHub</a>.</p>

<p>Compute Engine is fine if you just want shell access to a GPU machine, but I
tried to do as much as possible on FloydHub. FloydHub is basically a cloud
compute service targeted at machine learning. You run <code class="language-plaintext highlighter-rouge">floyd run python
awesomecode.py</code> and FloydHub sets up a container, uploads your code to it, and
runs the code. The two key things which make FloydHub awesome are:</p>

<ul>
  <li>Containers come preinstalled with GPU drivers and common libraries. (Even in
2018, I wasted a good few hours fiddling with CUDA versions while upgrading
TensorFlow on the Compute Engine VM.)</li>
  <li>Each run is automatically archived. For each run, the code used, the exact
command used to start the run, any command-line output, and any data outputs
are saved automatically, and indexed through a web interface.</li>
</ul>

<figure style="margin-right: auto; margin-left: auto"> <a href="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/floydhub.png"><img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/floydhub.png"></a> <figcaption style="padding-top: 10px">FloydHub's web interface. Top: index of past runs,
and overview of a single run. Bottom: both the code used for each run and any
data output from the run are automatically archived.</figcaption> </figure>

<p>I can’t stress enough how important that second feature is. For any project
this long, detailed records of what you’ve tried and the ability to reproduce
past experiments are an absolute must. Version control software can help, but
a) managing large outputs can be painful, and b) requires extreme diligence.
(For example, if you’ve set off some runs, then make a small change and launch
another run, when you commit the results of the first runs, is it going to be
clear which code was used?) You could take careful notes or roll your own
system, but with FloydHub, <em>it just works</em> and you save <em>so</em> much mental
energy.</p>

<p>(Update: check out some example FloydHub runs at
<a href="https://www.floydhub.com/mrahtz/projects/learning-from-human-preferences">https://www.floydhub.com/mrahtz/projects/learning-from-human-preferences</a>.)</p>

<p>Other things I like about FloydHub are:</p>
<ul>
  <li>Containers are automatically shut down once the run is finished. Not having
to worry about checking runs to see whether they’ve finished and the VM can
be turned off is a big relief.</li>
  <li>Billing is much more straightforward than with cloud VMs. You pay for usage
in, say, 10-hour blocks, and you’re charged immediately. That makes keeping
weekly budgets much easier.</li>
</ul>

<p>The one pain point I’ve had with FloydHub is that you can’t customize
containers. If your code has a lot of dependencies, you’ll need to install them
at the start of every run. That limits the rate at which you can iterate on
short runs. You <em>can</em> get around this, though, by creating a ‘dataset’ which
contains the changes to the filesystem from installing dependencies, then
copying files from that dataset at the start of each run (e.g.
<a href="https://github.com/mrahtz/learning-from-human-preferences/blob/master/floydhub_utils/create_floyd_base.sh"><code class="language-plaintext highlighter-rouge">create_floyd_base.sh</code></a>).
It’s awkward, but still probably less awkward than having to deal with GPU
drivers.</p>

<p>FloydHub is a little more expensive than Compute Engine: as of writing,
$1.20/hour for a machine with a K80 GPU, compared to about $0.85/hour for a
similarly-specced VM (though less if you don’t need as much as 61 GB of RAM).
Unless your budget is really limited, I think the extra convenience of FloydHub
is worth it. The only case where Compute Engine can be a lot cheaper is doing a
lot of runs in parallel, which you can stack up on a single large VM.</p>

<p style="
    color: greenyellow;
">(A third option is Google’s new
<a href="https://colab.research.google.com/">Colaboratory</a> service, which gives you a
hosted Jupyter notebook with free access to a single K80 GPU. Don’t be put off
by Jupyter: you can execute arbitrary commands, and set up shell access if you
really want it. The main drawbacks are that your code doesn’t keep running if
you close the browser window, and there are time limits on how long you can run
before the container hosting the notebook gets reset. So it’s not suitable for
doing long runs, but can be useful for quick prototyping on a GPU.)</p>

<p>In total, the project took:</p>

<ul>
  <li><strong>150 hours of GPU time and 7,700 hours (wall time × cores) of CPU time</strong> on
Compute Engine,</li>
  <li><strong>292 hours of GPU time</strong> on FloydHub,</li>
  <li>and <strong>1,500 hours (wall time, 4 to 16 cores) of CPU time</strong> on my university’s
cluster.</li>
</ul>

<p style="
    color: greenyellow;
">I was horrified to realise that in total, that added up to <strong>about $850</strong> ($200
on FloydHub, $650 on Compute Engine) over the 8 months of the project.</p>

<p style="
    color: greenyellow;
">Some of that’s down to me being ham-fisted (see the above section on mindset
for slow iteration). Some of it’s down to the fact that reinforcement learning
is still so sample-inefficient that runs do just take a long time (up to 10
hours to train a Pong agent that beats the computer every time).</p>

<p>But a big chunk of it was down to a horrible surprise I had during the final
stages of the project: <strong style="
    color: greenyellow;
">reinforcement learning can be so unstable that you
need to repeat every run multiple times with different seeds to be confident</strong>.</p>

<p>For example, once I thought everything was basically working, I sat down to
make end-to-end tests for the environments I’d been working with. But I was
having trouble getting even the simplest environment I’d been working with,
<a href="https://github.com/mrahtz/gym-moving-dot">training a dot to move to the centre of a
square</a>, to train successfully.  I
went back to the FloydHub job that had originally worked and re-ran three
copies. It turned out that the hyperparameters I thought were fine actually
only succeeded one out of three times.</p>

<figure style="margin-right: auto; margin-left: auto">
  <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/failed_reproductions.png">
  <figcaption style="padding-top: 10px">It's not uncommon for two out of three random seeds (red/blue) to fail.</figcaption>
</figure>

<p>To give a visceral sense of how much compute that means you need:</p>

<ul>
  <li>Using A3C with 16 workers, Pong would take about 10 hours to train.</li>
  <li>That’s 160 hours of CPU time.</li>
  <li>Running 3 random seeds, that 480 hours (20 days) of CPU time.</li>
</ul>

<p>In terms of costs:</p>

<ul>
  <li>FloydHub charges about $0.50 per hour for an 8-core machine.</li>
  <li>So 10 hours costs about $5 per run.</li>
  <li><strong>Running 3 different random seeds at the same time, that’s $15 per run.</strong></li>
</ul>

<p><strong>That’s, like, 3 sandwiches every time you want to test an idea.</strong></p>

<p>Again, from <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Deep Reinforcement Learning Doesn’t Work
Yet</a>, that kind of
instability seems normal and accepted right now. In fact, even “Five random
seeds (a common reporting metric) may not be enough to argue significant
results, since with careful selection you can get non-overlapping confidence
intervals.”</p>

<p>(All of a sudden the $25,000 of AWS credits that the <a href="https://blog.openai.com/openai-scholars/">OpenAI Scholars
programme</a> provides doesn’t seem
quite so crazy. That probably <em>is</em> about the amount you need to give someone so
that compute isn’t a worry at all.)</p>

<p>My point here is that <strong>if you want to tackle a deep reinforcement learning
project, make sure you know what you’re getting yourself into</strong>. Make sure
you’re prepared for how much time it could take and how much it might cost.</p>

<hr>

<p>Overall, reproducing a reinforcement learning paper was a fun side project to
try. But looking back, thinking about which skills it actually levelled up, I’m
also wondering whether reproducing a paper was really the best use of time over
the past months.</p>

<p>On one hand, I definitely feel like my machine learning <em>engineering</em> ability
improved a lot. I feel more confident in being able to recognise common RL
implementation mistakes; my workflow got a whole lot better; and from this
particular paper I got to learn a bunch about Distributed TensorFlow and
asynchronous design in general.</p>

<p>On the other hand, I don’t feel like my machine learning <em>research</em> ability
improved much (which is, in retrospect, what I was actually aiming for). Rather
than implementation, the much more difficult part of research seems to be
coming up with ideas that are interesting but also <em>tractable and concrete</em>;
ideas which give you the best bang-for-your-buck for the time you <em>do</em> spend
implementing. Coming up with interesting ideas seems to be a matter of a)
having a large vocabulary of concepts to draw on, and b) having good ‘taste’
for ideas (e.g. what kind of work is likely to be useful to the community). I
think a better project for both of those might have been to, say, read
influential papers and write summaries and critical analyses of them.</p>

<p>So I think my main meta-takeaway from this project is that <strong>it’s worth
thinking carefully whether you want to level up engineering skills or research
skills</strong>. Not that there’s no overlap; but if you’re particularly weak on one
of them you might be better off with a project specifically targeting that one.</p>

<p>If you want to level up both, a better project might be to read papers until
you find something you’re really interested in that comes with clean code, and
trying to implement an extension to it.</p>

<hr>

<p>If you <em>do</em> want to tackle a deep RL project, here are some more specific
things to watch out for.</p>

<h4 id="choosing-papers-to-reproduce">Choosing papers to reproduce</h4>

<ul>
  <li>Look for papers with few moving parts. Avoid papers which require multiple
parts working together in coordination.</li>
</ul>

<h4 id="reinforcement-learning">Reinforcement learning</h4>

<ul>
  <li style="
    color: greenyellow;
">If you’re doing anything that involves an RL algorithm as a component in a
larger system, don’t try and implement the RL algorithm yourself. It’s a fun
challenge, and you’ll learn a lot, but RL is unstable enough at the moment
that you’ll never be sure whether your system doesn’t work because of a bug
in your RL implementation or because of a bug in your larger system.</li>
  <li>Before doing anything, see how easily an agent can be trained on your
environment with a baseline algorithm.</li>
  <li>Don’t forget to normalize observations. <em>Everywhere</em> that observations might
be being used. <sup id="fnref:norm2" role="doc-noteref"><a href="http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM#fn:norm2" class="footnote" rel="footnote">3</a></sup></li>
  <li>Write end-to-end tests as soon as you think you’ve got something working.
Successful training can be more fragile than you expected.</li>
  <li>If you’re working with OpenAI Gym environments, note that with <code class="language-plaintext highlighter-rouge">-v0</code>
environments, 25% of the time, the current action is ignored and the previous
action is repeated (to make the environment less deterministic). Use <code class="language-plaintext highlighter-rouge">-v4</code>
environments if you don’t want that extra randomness. Also note that
environments by default only give you every 4th frame from the emulator,
matching the early DeepMind papers. Use <code class="language-plaintext highlighter-rouge">NoFrameSkip</code> environments if you
don’t want that. For a fully deterministic environment that gives you exactly
what the emulator gives you, use e.g.  <code class="language-plaintext highlighter-rouge">PongNoFrameskip-v4</code>.</li>
</ul>

<h4 id="general-machine-learning">General machine learning</h4>

<ul>
  <li>Because of how long end-to-end tests take to run, you’ll waste a lot of time
if you have to do major refactoring later on. Err on the side of implementing
things well the first time rather than hacking something up and saving
refactoring for later.</li>
  <li>Initialising a model can easily take ~ 20 seconds. That’s a painful amount of
time to waste because of e.g. syntax errors. If you don’t like using IDEs, or
you can’t because you’re editing on a server with only shell access, it’s
worth investing the time to set up a linter for your editor. (For Vim, I like
<a href="https://github.com/w0rp/ale">ALE</a> with <em>both</em>
<a href="https://www.pylint.org/">Pylint</a> and
<a href="http://flake8.pycqa.org/en/latest/">Flake8</a>. Though Flake8 is more of a
style checker, it can catch some things that Pylint can’t, like wrong
arguments to a function.) Either way, every time you hit a stupid error while
trying to start a run, invest time in making your linter catch it in the
future.</li>
  <li>It’s not just dropout you have to be careful about implementing in networks
with weight-sharing - it’s also batchnorm. Don’t forget there are
normalization statistics and extra variables in the network to match.</li>
  <li>Seeing regular spikes in memory usage while training? It might be that your
validation batch size is too large.</li>
  <li>If you’re seeing strange things when using Adam as an optimizer, it might be
because of Adam’s momentum. Try using an optimizer without momentum like
RMSprop, or disable Adam’s momentum by setting β1 to zero.</li>
</ul>

<h4 id="tensorflow">TensorFlow</h4>

<ul>
  <li>If you want to debug what’s happening with some node buried deep in the
middle of your graph, check out
<a href="https://www.tensorflow.org/api_docs/python/tf/Print"><code class="language-plaintext highlighter-rouge">tf.Print</code></a>, an
identity operation which prints the value of its input every time the graph
is run.</li>
  <li>If you’re saving checkpoints only for inference, you can save a lot of space
by omitting optimizer parameters from the set of variables that are saved.</li>
  <li><code class="language-plaintext highlighter-rouge">session.run()</code> can have a large overhead. Group up multiple calls in a batch
wherever possible.</li>
  <li>If you’re getting out-of-GPU-memory errors when trying to run more than one
TensorFlow instance on the same machine, it could just be because one of your
instances is trying to reserve all the GPU memory, rather than because your
models are too large. This is TensorFlow’s default behaviour. To tell
TensorFlow to only reserve the memory it needs, see the
<a href="https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth"><code class="language-plaintext highlighter-rouge">allow_growth</code></a>
option.</li>
  <li>If you want to access the graph from multiple things running at once, it
looks like you <em>can</em> access the same graph from multiple threads, but there’s
a lock somewhere which only allows one thread at a time to actually do
anything. This seems to be distinct from the Python global interpreter lock,
which TensorFlow is <a href="https://stackoverflow.com/questions/38206695/python-parallelizing-gpu-and-cpu-work">supposed
to</a>
release before doing heavy lifting. I’m uncertain about this, and didn’t have
time to debug more thoroughly, but if you’re in the same boat, it might be
simpler to just use multiple processes and replicate the graph between them
with <a href="http://amid.fish/distributed-tensorflow-a-gentle-introduction">Distributed
TensorFlow</a>.</li>
  <li>Working with Python, you get used to not having to worry about overflows. In
TensorFlow, though, you still need to be careful:</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">255</span><span class="p">,</span> <span class="mi">200</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="mi">199</span></code></pre></figure>

<ul>
  <li>Be careful about using <code class="language-plaintext highlighter-rouge">allow_soft_placement</code> to fall back to a CPU if a GPU
isn’t available. If you’ve accidentally coded something that can’t be run on
a GPU, it’ll be silently moved to a CPU. For example:</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"/device:GPU:0"</span><span class="p">):</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">[...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c1"># Seems to work fine. But with allow_soft_placement=False
</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

<span class="c1"># we get
</span>
<span class="c1"># Cannot assign a device for operation 'strided_slice_5':
# Could not satisfy explicit device specification '/device:GPU:0'
# because no supported kernel for GPU devices is available.</span></code></pre></figure>

<ul>
  <li>I don’t know how many operations there are like this that can’t be run on a
GPU, but to be safe, do CPU fallback manually:</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">gpu_name</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">test</span><span class="p">.</span><span class="n">gpu_device_name</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">gpu_name</span> <span class="k">if</span> <span class="n">gpu_name</span> <span class="k">else</span> <span class="s">"/cpu:0"</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
    <span class="c1"># graph code</span></code></pre></figure>

<h4 id="mental-health">Mental health</h4>

<ul>
  <li>Don’t get addicted to TensorBoard. I’m serious. It’s the perfect example of
addiction through unpredictable rewards: most of the time you check how your
run is doing and it’s just pootling away, but as training progresses,
sometimes you check and all of the sudden - jackpot! It’s doing something
super exciting. If you start feeling urges to check TensorBoard every few
minutes, it might be worth setting rules for yourself about how often it’s
reasonable to check.</li>
</ul>

<hr>

<p>If you’ve read this far and haven’t been put off, awesome! If you’d like to get
into deep RL too, here are some resources for getting started.</p>

<ul>
  <li>Andrej Karpathy’s <a href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning: Pong from
Pixels</a> is a great introduction to
build motivation and intuition.</li>
  <li>For more on the theory of reinforcement learning, check out <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver’s
lectures</a>. There
isn’t much on deep RL (reinforcement learning using neural networks), but it
does teach the vocabulary you’ll need to be able to understand papers.</li>
  <li>John Schulman’s <a href="https://www.youtube.com/watch?v=8EcdaCk9KaQ">Nuts and Bolts of Deep RL
talk</a>
(<a href="http://joschu.net/docs/nuts-and-bolts.pdf">slides</a>; <a href="https://github.com/williamFalcon/DeepRLHacks">summary
notes</a>) has lots more tips
about practical issues you might run into.</li>
</ul>

<p>For a sense of the bigger picture of what’s going on in deep RL at the moment,
check out some of these.</p>

<ul>
  <li>Alex Irpan’s <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">Deep Reinforcement Learning Doesn’t Work
Yet</a> has a great overview
of where things are right now.</li>
  <li>Vlad Mnih’s talk on <a href="https://www.youtube.com/watch?v=bsuvM1jO-4w">Recent Advances and Frontiers in Deep
RL</a> has more examples of work on
some of the problems mentioned in Alex’s post.</li>
  <li>Sergey Levine’s <a href="https://www.youtube.com/watch?v=eKaYnXQUb2g">Deep Robotic
Learning</a> talk, with a focus on
improving generalization and sample efficiency in robotics.</li>
  <li>Pieter Abbeel’s <a href="https://www.youtube.com/watch?v=TyOooJC_bLY">Deep Learning for
Robotics</a> keynote at NIPS 2017
with some of the more recent tricks in deep RL.</li>
</ul>

<p>Good luck!</p>

<p><small>Thanks to <a href="http://agentydragon.com/about.html">Michal Pokorný</a> and Marko Thiel for thoughts on
a first draft on this post.</small></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:normproblems" role="doc-endnote">
      <p>Observations are fed into two different training loops, policy training and reward predictor training, and I’d forgotten to normalize observations for the second one. Also, calculating running statistics (specifically, variance) is tricky. Check out <a href="https://github.com/joschu/modular_rl/blob/master/modular_rl/running_stat.py">John Schulman’s code</a> for a good reference.&nbsp;<a href="http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM#fnref:normproblems" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:murphyjitsu" role="doc-endnote">
      <p>This is basically <a href="http://www.rationality.org/">CFAR’s</a> ‘MurphyJitsu’ script.&nbsp;<a href="http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM#fnref:murphyjitsu" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:norm2" role="doc-endnote">
      <p>As mentioned above, I was stuck for a good while because of forgetting to normalize observations used for training the reward predictor. Derp.&nbsp;<a href="http://amid.fish/reproducing-deep-rl?fbclid=IwAR1VPZm3FSTrV8BZ4UdFc2ExZy0olusmaewmloTPhpA4QOnHKRI2LLOz3mM#fnref:norm2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </article>

</div>

      </div>
    </div>

    
<div class="comments">
    <div class="wrapper">
        <div id="disqus_thread"><iframe id="dsq-app7651" name="dsq-app7651" allowtransparency="true" frameborder="0" scrolling="no" tabindex="0" title="Disqus" width="100%" src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/saved_resource.html" style="width: 1px !important; min-width: 100% !important; border: none !important; overflow: hidden !important; height: 5235px !important;" horizontalscrolling="no" verticalscrolling="no"></iframe></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES * * */
            // Required: on line below, replace text in quotes with your forum shortname
            var disqus_shortname = 'amidfish';
            var disqus_identifier = "reproducing-deep-rl";
            
            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
    </div>
</div>


    <footer class="site-footer">

  <div class="wrapper">

    <img src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/me.png" height="100" width="100" style="float: right">

    <h2 class="footer-heading">Amid Fish</h2>

    <p>is Matthew Rahtz's blog</p>
    <p>
    <a href="https://github.com/mrahtz">GitHub</a>,
    <a href="https://uk.linkedin.com/pub/matthew-rahtz/b8/a47/540">LinkedIn</a>,
    or say hello at
    <a href="mailto:matthew.rahtz@gmail.com">matthew.rahtz@gmail.com</a>!
    </p>

  </div>

</footer>


  


<iframe style="display: none;" src="./Lessons Learned Reproducing a Deep Reinforcement Learning Paper_files/saved_resource(1).html"></iframe></body></html>