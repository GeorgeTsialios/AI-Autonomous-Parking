-------------------------------------------------------------------------------------------------------------------------------------------
Προσπάθησα να επιταχύνω τη διαδικασία της εκπαίδευσης, ενισχύωντας το hardware πάνω στο οποίο εκπαιδεύονται τα μοντέλα. Συγκεκριμένα, η βιβλιοθήκη stableBaselines3 χρησιμοποιεί Vectorized Environments: Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Vectorized environments in reinforcement learning offer several advantages:

Parallelism: By running multiple instances of an environment in parallel, vectorized environments can significantly speed up the data collection process. This is particularly beneficial in reinforcement learning, where algorithms often require large amounts of data to learn effectively.

Efficiency: Vectorized environments can make better use of hardware resources, such as multi-core CPUs or GPUs, by distributing the workload across multiple cores or processing units. This leads to more efficient use of the available computational power.

Υπάρχουν 2 ειδών vectorized environments στη βιβλιοθήκη stable-baselines3:

-DummyVecEnv is a simple implementation of VecEnv (VecEnv is an abstract class that serves as the base for all vectorized environments in Gym) that runs each environment in a sequential manner within a single process. Despite its name, it's useful for testing and development because it simplifies debugging by avoiding the complexities of multiprocessing. However, because it runs environments sequentially, it does not offer the same performance benefits as parallelized implementations when it comes to training time. It is preferrable in the case of a simple environment such as Cartpole-v1, because then the overhead of multiprocess outweighs the environment computation time.
-SubprocVecEnv is an implementation of VecEnv that runs each environment in its own subprocess. This allows for true parallelism, taking advantage of multiple CPU cores, which can significantly speed up the training process. However, the use of multiple processes can make debugging more challenging and may introduce overhead due to inter-process communication.

Προσπάθησα να χρησιμοποιήσω SubprocVecEnv αντί του default DummyVecEnv, για να αξιοποιήσω τους 4 CPU cores του υπολογιστή μου, ωστόσο προέκυψε το εξής πρόβλημα: SubprocVecEnv creates separate processes for each environment instance to speed up training by running them in parallel. Each process needs to serialize (pickle) the environment to send it to the child processes. However, if your environment includes non-serializable objects like pygame.surface.Surface, this process fails. Για να λυθεί αυτό το πρόβλημα θα έπρεπε να αλλάξω σημαντικά τον κώδικα για το ίδιο το παιχνίδι και για αυτό δεν προχώρησα σε αυτήν την κατεύθυνση.

Προσπάθησα να εκπαιδεύσω το μοντέλο χρησιμοποιώντας τη GPU του υπολογιστή μου και όχι τη CPU. Γενικά, η εκπαίδευση αλγόριθμών μηχανικής μάθησης επιταχύνεται όταν χρησιμοποποιείται η GPU, καθώς προσφέρει significant improvements in computational efficiency, parallel processing capabilities, and overall training speed. Πιο συγκεκριμένα, πολλοί RL algorithms, especially those involving deep reinforcement learning (e.g., Deep Q-Networks (DQN), Proximal Policy Optimization (PPO)), rely heavily on deep neural networks. Training these networks involves a large number of matrix multiplications and other operations that benefit from GPU acceleration.
Για να χρησιμοποιήσουμε τη GPU κατά την εκπαίδευση, έπρεπε πρώτα να εγκαταστήσουμε το CUDA Toolkit της NVidia.Το CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and application programming interface (API) model created by NVIDIA. It enables developers to use NVIDIA GPUs for general purpose processing (an approach known as GPGPU, General-Purpose computing on Graphics Processing Units). Στη συνέχεια, έπρεπε να εγκαταστήσουμε τη cuDNN library της NVidia, which provides optimized implementations for deep learning operations. Τέλος, έπρεπε να κατεβάσουμε άλλη έκδοση της PyTorch (σε αυτήν τη βιβλιοθήκη είναι γραμμένη η stable-baselines3), η οποία να υποστηρίζει GPU Support.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Συχνά, επιλέγω smoothing = 0.99 για τις καμπύλες των average rewards. Έτσι, βλέπω την πιο εξομαλυμένη μορφή των καμπυλών η οποία θεωρώ μας δίνει την πιο χρήσιμη πληροφορία σχετικά με την τάση της γραφικής, χωρίς να αποπροσανατολιζόμαστε από τις συχνές βυθίσεις και μέγιστα. Άλλωστε, η κανονική μορφή της γραφικής (χωρίς smoothing) φαίνεται με αχνό χρώμα.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Γίνεται να συνεχίσεις την εκπαίδευση σε ένα ήδη υπάρχον μοντέλο και η γραφική στο Tensorboard να συνεχίσει από το σημείο που σταμάτησε, αλλά ΜΟΝΟ αν συνεχίσεις την πιο πρόσφατη εκπαίδευση. Αυτό γιατί, το νέο log αρχείο θα πάει να το βάλει στον πιο πρόσφατο φάκελο. Έτσι, αν έχεις παρεμβάλει άλλη εκπαίδευση μετά από αυτήν που θες να συνεχίσεις, θα πρέπει πρώτα να μετακινήσεις τον φάκελο της παρεμβαλόμενης αλλού. Μπορείς να τον επαναφέρεις έπειτα.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Entropy regularization 

Eνθαρρύνει τον πράκτορα να εξερευνήσει το περιβάλλον, ωθώντας σε πιο ίση κατανομή των actions του πράκτορα. Ωστόσο, αν δώσουμε υπερβολικά μεγάλες τιμές στον entropy coefficient, οι actions του πράκτορα θα είναι random.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
NN Architecture

- Consider the Complexity of Your Task
Begin with a simple NN architecture and gradually increase the network's size if necessary. The complexity of your task should guide the network size.

Simple tasks: 1-2 hidden layers with 64 neurons each.
Moderately complex tasks: 2-3 hidden layers with 128-256 neurons each.
Highly complex tasks: 3-4 hidden layers with 256-512 neurons each.

- Monitor Training and Adjust
Track the performance of your agent during training using TensorBoard or other monitoring tools. If the agent is underperforming, consider increasing the number of neurons or layers.

- Avoid Overfitting
A very large network can overfit to the training data. Regularly validate the agent's performance on different initial conditions (starting positions and parking spot locations) to ensure it generalizes well.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Continuous vs Discrete Action spaces

Proximal Policy Optimization (PPO) is a versatile reinforcement learning algorithm that works well with both discrete and continuous action spaces.
In Discrete Action spaces the action selection is simpler and thus is often more stable and require less fine-tuning. It is often used in games (e.g. Atari games) where the actions are discrete such as "move left", "move right".
Continuous Action spaces allow for more precise control, which is essential for tasks like Robotics (controlling the joint angles of a robotic arm) or Control (balancing a pole).
In general, if your task involves a limited set of actions, using a discrete action space is often easier and more stable. However, If precise control is crucial inyour task, a continuous action space is preferrable, but it may require more careful tuning of hyperparameters.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Reward Normalization

It can be beneficial, but it's not strictly necessary to normalize them to a specific range eg [0,1]. Instead, what matters is that the reward signals are structured in a way that helps the learning process. Namely, large variations in reward values can make training unstable. That is because, rewards or punishments that are severely larger than the rest might dominate the network's updates.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Activation Functions

ReLU

The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance. https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/

+ ReLU tends to converge faster than tanh
+ Simplicity: ReLU is computationally simple and efficient.
+ Gradient Propagation: ReLU does not saturate for very high positive values, which helps mitigate the vanishing gradient problem that tanh has.
- Dying ReLU Problem: Neurons can sometimes get stuck during training if they enter a region where the input is always negative, resulting in zero gradients.

Tanh

- A general problem with the tanh function is that it saturates. This means that large values snap to 1.0 and small values snap to -1. Further, the function is only really sensitive to changes around its mid-point of it input (0.5). Once saturated, it becomes challenging for the learning algorithm to continue to adapt the weights to improve the performance of the model. This is called the vanishing gradient problem and prevents deep (multi-layered) networks from learning effectively.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Dropout	

Dropout is a regularization technique commonly used in deep learning for tasks like image classification, natural language processing, and other supervised learning tasks. The main purpose of dropout is to prevent overfitting by randomly setting a fraction of input units to zero during training. This helps in improving the generalization of the model by reducing the interdependence between neurons.

In the context of deep reinforcement learning (RL), such as in algorithms like Deep Q Networks (DQN), Deep Deterministic Policy Gradient (DDPG), and others, dropout is generally not used for several reasons:

Instability: Dropout introduces randomness during training by setting some neurons to zero, which can be problematic in the context of RL algorithms. RL algorithms are sensitive to the stability of the learning process, and the randomness introduced by dropout can interfere with the learning dynamics and make it harder for the agent to learn an optimal policy.
Non-i.i.d. Data: In supervised learning tasks, dropout is effective because the training data is usually assumed to be independent and identically distributed (i.i.d.). However, in RL, the data generated during training is not i.i.d. The agent's interactions with the environment are sequential and correlated, making it challenging to apply dropout effectively.
Exploration vs. Exploitation: RL algorithms need to balance exploration (trying out new actions to discover the environment) and exploitation (leveraging known information to maximize rewards). Dropout can interfere with this balance by introducing unnecessary noise during both exploration and exploitation phases.
Overfitting vs. Underfitting: In supervised learning, the goal is often to prevent overfitting by using dropout. In RL, the main challenge is the trade-off between underfitting (not learning the optimal policy) and overfitting. Dropout may hinder the agent's ability to learn a good policy by preventing it from fully exploiting the information available in the data.
Instead of dropout, techniques like target network updates, experience replay, reward clipping, and exploration strategies (such as epsilon-greedy, softmax action selection, etc.) are commonly used in deep RL algorithms to stabilize training and improve performance.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------

Objectives in Reinforcement Learning

1. Generalization: Ensure the agent performs well not only on the specific scenarios seen during training but also on unseen scenarios. This is similar to avoiding overfitting in supervised learning.
2. Sample Efficiency: Train the agent to learn the optimal policy using a reasonable amount of data or interactions with the environment.
3. Stability and Convergence: Ensure the training process is stable and the agent’s policy converges to an optimal or near-optimal solution.
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
Stochastic policy 

A policy is a function that maps the input state to an action. A stochastic policy does not
choose action deterministically. It can be used to encourage exploration during training by allowing the agents
to occasionally execute low-confidence actions. Stochastic policy is always assumed in the formulation of policy
gradient based training algorithms.

model.predict(, deterministic = True/False)

This parameter corresponds to "Whether to use deterministic or stochastic actions". So the thing is when you are selecting an action according to given state, the actor_network gives you a probability distribution. For example for two possible actions a1 and a2: [0.25, 0.75]. If you use deterministic=True, the result will be action a2 since it has more probability. In the case of deterministic=False, the result action will be selected with given probabilities [0.25, 0.75]. So, basically Deterministic Policy when deterministic=True and Stochastic Policy otherwise.
https://www.analyticsvidhya.com/blog/2023/12/deterministic-vs-stochastic/
-------------------------------------------------------------------------------------------------------------------------------------------


-------------------------------------------------------------------------------------------------------------------------------------------
gSDE

Generalized State-Dependent Exploration is an exploration strategy that adds noise to the actions taken by the policy. This noise is state-dependent, meaning it varies based on the current state of the agent, which helps the agent explore the environment more efficiently and effectively.

Παράμετρος sde_sample_freq της μεθόδου PPO: This parameter specifies how frequently a new noise matrix is sampled. The noise matrix is used to add exploration noise to the actions. Lower sde_sample_freq values (eg 1, 3) provide more frequent sampling, which can lead to more diverse exploration (beneficial in highly dynamic environments) but can also introduce instability. On the contrary, higher sde_sample_freq values (eg 10, 100) leads to less diverse exploration but more stable (beneficial in stable environments).
https://paperswithcode.com/method/gsde
https://serp.ai/generalized-state-dependent-exploration/
-------------------------------------------------------------------------------------------------------------------------------------------