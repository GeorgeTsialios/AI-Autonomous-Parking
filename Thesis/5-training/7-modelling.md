## Μοντελοποίηση προβλήματος {#sec:training:modeling}

Σε αυτήν την ενότητα παρουσιάζεται η τελική μοντελοποίηση του προβλήματος αυτόματης στάθμευσης, μέσω της οποίας επιτεύχθηκαν τα καλύτερα αποτελέσματα των πρακτόρων. Συγκεκριμένα, θα εξεταστεί πρώτα, η αρχιτεκτονική των νευρωνικών δικτύων και στη συνέχεια, θα αναλυθεί η συνάρτηση ανταμοιβής.

### Αρχιτεκτονική νευρωνικών δικτύων {#sec:training:modeling:nn}

#### Διακριτός χώρος ενεργειών {.unnumbered}

Η τελική αρχιτεκτονική του νευρωνικού δικτύου, για την περίπτωση των αλγορίθμων με διακριτό χώρο ενεργείων (PPO), φαίνεται στην *Εικόνα @fig:training:modeling:nn-discrete*:

![Αρχιτεκτονική νευρωνικού δικτύου αλγορίθμών με διακριτό χώρο ενεργειών.](5-training/figures/NN-discrete.png){#fig:training:modeling:nn-discrete width=100%}

Αρχικά, παρατηρούμε πως το νευρωνικό δίκτυο αποτελείται από 2 πλήρως συνδεδεμένα κρυφά επίπεδα, των 64 νευρώνων το καθένα, όπως και στην αρχική δημοσίευση του αλγορίθμου. Στη συνέχεια, αξίζει να σταθούμε στην κατάρτιση του χώρου καταστάσεων, δηλαδή της εισόδου του πράκτορα, που απαρτίζεται από τα εξής 12 στοιχεία:

- **Radar 1-8**: οι ακτίνες ραντάρ λειτουργούν ως αισθητήρες για το περιβάλλον του πράκτορα, παρέχοντας τις αποστάσεις του από άλλα αντικείμενα σε 8 κατευθύνσεις.
- **Offset_x, Offset_y**: αποτελούν τη σχετική απόσταση του πράκτορα, από την ελεύθερη θέση στάθμευσης. Επιλέχθηκαν αντί της απλής απόστασης πράκτορα-θέσης, καθώς έτσι, πέραν από την πληροφορία του πόσο απέχει ο πράκτορας από τον στόχο του, παρέχεται σε αυτόν και η κατεύθυνση προς την οποία πρέπει να κινηθεί. Για παράδειγμα, όταν η είσοδος Offset_x είναι αρνητική, αυτό σημαίνει πως ο πράκτορας βρίσκεται στα αριστερά της θέσης στάθμευσης, ενώ όταν η είσοδος αυτή μηδενιστεί, ο πράκτορας βρίσκεται στον ίδιο οριζόντιο άξονα με τη θέση στάθμευσης. Επομένως, οι είσοδοι Offset_x και Offset_y βοηθούν τον πράκτορα να αναγνωρίσει την κατεύθυνση προς την οποία πρέπει να κινηθεί (π.χ. δεξία και πάνω), καθώς και να κατανοήσει πως ο στόχος του θα επιτευχθεί, όταν οι δύο αυτές τιμές εισόδων συγκλίνουν στο 0.
- **Velocity**:  η ταχύτητα του πράκτορα τον ενημερώνει αφενός για το πόσο γρήγορα κινείται και αφετέρου, για το αν κινείται προς τα εμπρός ή προς τα πίσω. Έτσι, η συγκεκριμένη είσοδος βοηθάει τον πράκτορα να προβλέψει πότε πρόκειται να συγκρουστεί με κάποιο αντικείμενο, καθώς και να κατανοήσει πως ο στόχος της στάθμευσης θα επιτευχθεί, όταν η είσοδος αυτή μηδενιστεί.
- **Angle**: η γωνία του πράκτορα, ως προς τον άξονα x, τον ενημερώνει για τον προσανατολισμό του. Επομένως, όπως και πριν, η είσοδος αυτή βοηθάει τον πράκτορα να προβλέψει πότε πρόκειται να συγκρουστεί με κάποιο αντικείμενο, καθώς και να κατανοήσει πως ο στόχος του θα επιτευχθεί, όταν η συγκεκριμένη είσοδος συγκλίνει είτε στην τιμή 0 (το αυτοκίνητο είναι στραμμένο προς τα κάτω), είτε στην τιμή $\pm$ 1 (το αυτοκίνητο είναι στραμμένο προς τα πάνω).

Κάθε μία από τις παραπάνω εισόδους έχει υποστεί κανονικοποίηση, ώστε να βρίσκεται στο διάστημα [-1, 1], καθώς, όπως βλέπουμε και στην *Εικόνα @fig:training:modeling:nn-discrete*, το επίπεδο εισόδου του δικτύου χρησιμοποιεί την συνάρτηση ενεργοποίησης Tanh.

Τέλος, ο χώρος ενεργειών αποτελείται από 9 διακριτές ενέργειες. Η δράση της κάθε ενέργειας είναι αυτονόητη από το όνομα της και αντιστοιχεί στην πίεση κανενός, ενός ή δύο βελών του πληκτρολογίου. Αξίζει μόνο να σημειώσουμε, πως οι ενέργειες `LEFT` και `RIGHT` προκαλούν απλώς την περιστροφή του αυτοκινήτου προς την αντίστοιχη κατεύθυνση και είναι χρήσιμες, μόνο όταν το αυτοκίνητο έχει ήδη, κάποια αρχική ταχύτητα. Αντίθετα, όταν το αυτοκίνητο είναι ακίνητο, οι ενέργειες αυτές δεν θα έχουν κανένα αποτέλεσμα. Ο αλγόριθμος PPO επιλέγει μία τιμή στο διάστημα [0, 1] για κάθε ενέργεια, η οποία υποδηλώνει την πιθανότητα επιλογής της. Κατά την εκπαίδευση, χρησιμοποιείται στοχαστική πολιτική και οι ενέργειες επιλέγονται με βάση αυτήν την κατανομή πιθανοτήτων, ενώ κατά την αξιολόγηση, χρησιμοποιείται ντετερμινιστική πολιτική και επιλέγεται απλώς η ενέργεια με τη μεγαλύτερη πιθανότητα.

#### Συνεχής χώρος ενεργειών {.unnumbered}

Η τελική αρχιτεκτονική του νευρωνικού δικτύου, για την περίπτωση των αλγορίθμων με συνεχή χώρο ενεργείων (SAC, TD3, DDPG), φαίνεται στην *Εικόνα @fig:training:modeling:nn-continuous*:

![Αρχιτεκτονική νευρωνικού δικτύου αλγορίθμών με συνεχή χώρο ενεργειών.](5-training/figures/NN-continuous.png){#fig:training:modeling:nn-continuous width=100%}

Αρχικά, οι αλγόριθμοι που χρησιμοποιούνται σε αυτήν την περίπτωση ανήκουν στην κατηγορία δράστη-κριτή, επομένως το νευρωνικό δίκτυο που εξετάζουμε είναι αυτό του δράστη. Όπως και πριν, το δίκτυο αποτελείται από 2 κρυφά επίπεδα, όμως τώρα το πλήθος των νευρώνων σε αυτά, είναι πολύ μεγαλύτερο. Συγκεκριμένα, ο αλγόριθμος SAC χρησιμοποιεί 256 νευρώνες σε κάθε κρυφό επίπεδο, ενώ οι  αλγόριθμοι TD3 και DDPG χρησιμοποιούν 400 νευρώνες στο πρώτο κρυφό επίπεδο και 300 στο δεύτερο. Οι τιμές αυτές είναι οι προκαθορισμένες από τη βιβλιοθήκη Stable-Baselines3.

Οι είσοδοι του δικτύου είναι ίδιες με πριν, με τη διαφορά πως πλέον, κανονικοποιούνται στο διάστημα [0, 1]. Αντίθετα, οι έξοδοι έχουν μειωθεί σε μόλις δύο: μία που αντιστοιχεί στην ισχύ του πράκτορα (`THROTTLE`) και μία που αντιστοιχεί στον προσανατολισμό του (`STEERING`). Οι δύο αυτές έξοδοι, λαμβάνουν τιμές στο διάστημα [-1, 1]. Συγκεκριμένα, κάθε αλγόριθμος εξάγει δύο τιμές, για κάθε ενέργεια: τη μέση τιμή της ενέργειας (μ) και την τυπική απόκλιση της (σ). Όταν η πολιτική του αλγορίθμου είναι στοχαστική (SAC κατά την εκπαίδευση), η επιλογή της τελικής τιμής κάθε ενέργειας γίνεται μέσω δειγματοληψίας, από την κατανομη που περιγράφεται από τη μέση τιμή και την τυπική απόκλιση. Αντίθετα, όταν η πολιτική είναι ντετερμινιστική (SAC κατά την αξιολόγηση, TD3, DDPG), χρησιμοποιείται απλά η μέση τιμή, ως τελική τιμή της ενέργειας. Βέβαια, αφού προκύψουν οι τελικές τιμές για τις δύο αυτές εξόδους, ο κώδικας μας θα τις μεταφράσει σε μία από τις 9 διακριτές ενέργειες, που αναφέρθηκαν παραπάνω. Μέσω της μετατροπής αυτής, οι αλγόριθμοι που χρησιμοποιούν συνεχή χώρο ενεργείων, ελέγχουν κι αυτοί το αυτοκίνητο με τα βέλη του πληκτρολογίου, όπως θα έκανε ένας άνθρωπος που παίζει το παιχνίδι.

### Συνάρτηση ανταμοιβής {#sec:training:modeling:reward}

#### Αραιές ανταμοιβές - αλγόριθμος PPO {.unnumbered}

Η συνάρτηση ανταμοιβής που πέτυχε την καλύτερη επίδοση των πρακτόρων του αλγορίθμου PPO, στο επίπεδο δυσκολίας 3 - άμεση στάθμευση, δίνεται παρακάτω, υπό μορφή ψευδοκώδικα:

![Συνάρτηση με αραιές ανταμοιβές - Άμεση στάθμευση.](5-training/figures/rf1-instant.pdf){#fig:training:modeling:reward-instant}

Παρατηρούμε αρχικά, πως οι ανταμοιβές σε αυτήν την περίπτωση είναι αραιές, καθώς η συνάρτηση ανταμοιβής δεν οδηγεί τον πράκτορα προς τον στόχο, αλλά απλώς τον επιβραβέυει, όταν φτάσει σε αυτόν ($+500$). Βέβαια, υπάρχουν κάποιες δευτερεύουσες ανταμοιβές, όπως η επιβράβευση του πράκτορα όταν κινείται ($+2$), προκειμένου να τον ενθαρρύνουμε να εξερευνήσει το περιβάλλον, αλλά και η τιμωρία του, όταν συγκρούεται με κάποιο αντικείμενο ($-10$), ώστε να τον ωθήσουμε να βελτιώσει την πολιτική του. Ακόμα, η τιμωρία σε κάθε βήμα ($-3$) είναι απαραίτητη, καθώς χωρίς αυτήν, ο πράκτορας πετύχαινε θετική ανταμοιβή μόνο με την κίνηση του, κι έτσι έμαθε να κάνει πρώτα έναν γύρο του χάρτη και μετά να παρκάρει (reward hacking).

Ωστόσο, όταν δοκιμάσαμε να επανεκπαίδευσουμε αυτόν τον πράκτορα στο επόμενο επίπεδο δυσκολίας, αυτό της κανονικής στάθμευσης (με χρονικό όριο 2 δευτερολέπτων), τροποποιώντας την παραπάνω συνάρτηση ανταμοιβής, τα αποτελέσματα δεν ήταν ικανοποιητικά και ο πράκτορας δεν κατάφερε να μάθει να σταθμεύει αξιόπιστα.

#### Διαμόρφωση ανταμοιβής - αλγόριθμοι SAC, TD3, DDPG {.unnumbered}

Η συνάρτηση ανταμοιβής που πέτυχε την καλύτερη επίδοση των πρακτόρων των αλγορίθμων SAC, TD3 και DDPG, στο επίπεδο δυσκολίας 3 - άμεση στάθμευση, δίνεται στην επόμενη σελίδα, υπό μορφή ψευδοκώδικα:

![Συνάρτηση με διαμόρφωση ανταμοιβής - Άμεση στάθμευση.](5-training/figures/rf2-instant.pdf){#fig:training:modeling:reward2-instant width=100%}

Παρατηρούμε αρχικά, πως οι ανταμοιβές σε αυτήν την περίπτωση είναι πολύ πιο συχνές, καθώς η συνάρτηση ανταμοιβής κατευθύνει τον πράκτορα προς τον στόχο. Έτσι, ο πράκτορας τιμωρείται ($-6$) όσο πιο μακριά βρίσκεται από την ελεύθερη θέση στάθμευσης, στον άξονα x και στον άξονα y. Επιπλέον, ο πράκτορας τιμωρείται όταν μένει ακίνητος ($-5$), προκειμένου να τον ενθαρρύνουμε να εξερευνήσει το περιβάλλον, όπως και πριν. Όμως, σε αντίθεση με πριν, πλέον, ο πράκτορας τιμωρείται ακόμα κι όταν κινείται με μικρή ταχύτητα. Αυτή η τιμωρία προέκυψε, καθώς χωρίς αυτήν, ο πράκτορας έμαθε να αποφεύγει την τιμωρία της ακινησίας, κινούμενος πολύ αργά (υποβέλτιστη πολιτική). Μάλιστα, η τιμωρία για την κίνηση με μικρή ταχύτητα είναι μεγαλύτερη ($-5$), όταν ο πράκτορας βρίσκεται μακριά από τον στόχο του και μικρότερη ($-3$), όταν βρίσκεται κοντά σε αυτόν. Η επιλογή αυτή, έγινε με το σκεπτικό πως είναι αναμενόμενο, ο πράκτορας να επιβραδύνει όταν βρίσκεται κοντά στη θέση στάθμευσης, προκειμένου να πραγματοποιήσει τους απαραίτητους ελιγμούς για να παρκάρει. Επομένως, στην περίπτωση αυτή, ο πράκτορας τιμωρείται λιγότερο. Επίσης, όταν ο πράκτορας βρίσκεται κοντά στη θέση στάθμευσης, επιβραβεύεται όσο έχει την κατάλληλη γωνία ($+0.5$), προκειμένου να τον καθοδηγήσουμε να παρκάρει. Ακόμα, όπως και πριν, ο πράκτορας τιμωρείται ($-10$) όταν συγκρούεται με κάποιο αντικείμενο, ώστε να μάθει να αποφεύγει τις συγκρούσεις. Τέλος, ο πράκτορας λαμβάνει μεγάλη επιβράβευση, όταν εισέρχεται στην ελεύθερη θέση στάθμευσης ($+5000$). 

$\\$
$\\$

Οι τιμές των ανταμοιβών είναι ελαφρώς διαφορετικές στους τρεις αλγορίθμους, καθώς έχουν προσαρμοστεί στη συμπεριφορά του κάθε πράκτορα. Επίσης, υπάρχουν δύο κομμάτια της συνάρτησης ανταμοιβής που διαφέρουν στους τρεις αλγορίθμους:

- το πρώτο κομμάτι είναι η επιπλέον επιβράβευση ($+1000$) για την είσοδο του πράκτορα στη θέση στάθμευσης, κινούμενος προς τα εμπρός (γραμμές 4-5) . Αυτό το «bonus», προστέθηκε στους αλγορίθμους που κρίθηκε απαραίτητο, προκειμένου ο πράκτορας να μάθει να παρκάρει τόσο προς τα εμπρός, όσο και με την όπισθεν.
- το δεύτερο κομμάτι, αφορά την επιπλέον επιβράβευση ($+1$) για την κίνηση του πράκτορα προς τα εμπρός (γραμμές 10-11). Παρόμοια με πριν, αυτή η επιβράβευση προστέθηκε σε συγκεκριμένους αλγορίθμους, καθώς χωρίς αυτήν, οι πράκτορες τους έμαθαν να κινούνται μόνο προς τα πίσω.

Είναι πιθανόν, αυτές οι μικρές αλλαγές στη συμπεριφορά των πρακτόρων, να οφείλονται στην εξάρτηση των εκπαιδεύσεων από τους ψευδο-τυχαίους αριθμούς του περιβάλλοντος (βλ. υποενότητα @training:difficulties:instability). Επομένως, οι επιμέρους τροποποίησεις της συνάρτησης ανταμοιβής σε κάθε αλγόριθμο, έχουν μικρή σημασία. Εξάλλου, η γενική δομή της συνάρτησης ανταμοιβής είναι η ίδια, σε όλες τις περιπτώσεις.

Στη συνέχεια, επανεκπαιδεύσαμε τους πράκτορες των αλγορίθμων SAC, TD3 και DDPG, στο επόμενο και τελευταίο επίπεδο δυσκολίας, τροποποιώντας την παραπάνω συνάρτηση ανταμοιβής. Η νέα συνάρτηση ανταμοιβής, για το επίπεδο δυσκολίας 4 - κανονική στάθμευση (με χρονικό όριο 2 δευτερολέπτων), δίνεται στην επόμενη σελίδα, υπό μορφή ψευδοκώδικα:

![Συνάρτηση με διαμόρφωση ανταμοιβής - Κανονική στάθμευση.](5-training/figures/rf2-normal.pdf){#fig:training:modeling:reward2-normal width=100% }

Πλέον, εφαρμόζουμε κλιμακωτή μάθηση και πρέπει να καθοδηγήσουμε τους προηγούμενους πράκτορες, οι οποίοι έχουν μάθει να κινούνται εντός της ελεύθερης θέσης στάθμευσης, να παραμείνουν ακίνητοι εντός αυτής. Για αυτό, στη συνάρτηση ανταμοιβής έχουν προστεθεί οι γραμμμές 5-9. Στις συγκεκριμένες γραμμές, ο πράκτορας επιβραβεύεται όσο βρίσκεται εντός της θέσης στάθμευσης ($+10$), όμως η επιβράβευση αυτή, είναι αντιστρόφως ανάλογη της ταχύτητας του. Επομένως, όσο πιο αργά κινείται ο πράκτορας, τόσο μεγαλύτερη είναι η επιβράβευση. Μάλιστα, όταν η ταχύτητα του πράκτορα μηδενιστεί, τότε προστίθεται κι άλλη επιβράβευση ($+10$). Με αυτόν τον τρόπο, ο πράκτορας σταδιακά μαθαίνει να ακινητοποιείται εντός της θέσης στάθμευσης και όταν το πετύχει αυτό για 2 συνεχόμενα δευτερόλεπτα, τότε λαμβάνει τη μεγάλη επιβράβευση της επιτυχούς στάθμευσης ($+5000$).

Πράγματι, όπως θα δούμε στο επόμενο κεφάλαιο, μέσω αυτής της συνάρτησης ανταμοιβής, οι πράκτορες των αλγορίθμων SAC, TD3 και σε μικρότερο βαθμό, του DDPG, κατάφεραν να μάθουν να σταθμεύουν αξιόπιστα, στο τελικό επίπεδο δυσκολίας.