## Εκπαιδεύσεις με τον αλγόριθμο Q-Learning {#sec:training:q-learning}

### Επισκόπηση εκπαιδεύσεων {#sec:training:q-learning:overview}

Ο αλγόριθμος Q-Learning ήταν ο πρώτος αλγόριθμος που εξετάστηκε και επιλέχθηκε για την απλότητα του, ως μία εισαγωγή στον χώρο της ενισχυτικής μάθησης. Συνολικά, διεξήχθησαν 55 εκπαιδεύσεις του αλγορίθμου, οι οποίες διήρκησαν αθροιστικά 2 ημέρες, 15 ώρες και 45 λεπτά. 

Το πρόβλημα που αντιμετωπίσαμε, είναι το πλήθος καταστάσεων του παιχνιδιού. Συγκεκριμένα, το παιχνίδι που αναπτύξαμε αποτελεί ένα περιβάλλον με συνεχή χώρο καταστάσεων. Όμως, προκειμένου να εφαρμόσουμε τον αλγόριθμο Q-Learning σε αυτό, έπρεπε να διακριτοποιήσουμε τον χώρο του παιχνιδιού. Ωστόσο, με την πρώτη διακριτοποίηση που δοκιμάσαμε, το μέγεθος του πίνακα Q προέκυψε $~6.5$ ΤΒ. Επομένως, προβήκαμε σε διαδοχικές διακριτοποιήσεις, έως ότου καταλήξαμε σε αυτήν που φαίνεται στην *Εικόνα @fig:training:q-learning:discretization*, η οποία δημιουργεί πίνακα Q μεγέθους 36 ΚΒ.

![Τελική διακριτοποίηση χώρου καταστάσεων.](5-training/figures/discretization2.png){#fig:training:q-learning:discretization width=100%} 

Παρατηρούμε αρχικά, πως προκειμένου να μειώσουμε τις διαστάσεις του πίνακα Q, περικόψαμε κάποιες από τις εισόδους και εξόδους του πράκτορα. Συγκεκριμένα, πλέον υπάρχουν 4 αισθητήρες αντί για 8 και 7 ενέργειες αντί για 9, καθώς αφαιρέθηκαν οι ενέργειες `LEFT` και `RIGHT`. Ακόμα, η διακριτοποίηση συνοψίζει την πληροφορία του περιβάλλοντος. Έτσι, οι αισθητήρες πλέον παίρνουν μόνο 2 τιμές: 0 όταν δεν εντοπίζουν κάποιο αντικείμενο και 1 όταν το εντοπίζουν. Αντίστοιχα, οι σχετικές αποστάσεις του πράκτορα από τη θέση στάθμευσης παίρνουν 3 τιμές, για παράδειγμα για τον άξονα x: -1 όταν ο πράκτορας βρίσκεται αριστερά της θέσης, 0 όταν βρίσκεται στον ίδιο οριζόντιο άξονα με τη θέση (με μία απόκλιση $\pm 10$px) και 1 όταν βρίσκεται δεξιά της. Παρόμοια, η ταχύτητα παίρνει μόνο 3 δυνατές τιμές και η γωνία 7. Οι τιμές αυτές κρίθηκαν οι απολύτως αναγκαίες, ώστε να έχει ο πράκτορας επαρκή πληροφορία, για να λύσει το πρόβλημα. Παρόλα αυτά, το πλήθος στοιχείων του πίνακα Q (21168), παραμένει αρκετά μεγαλύτερο, από αυτό που συνηθίζεται. Για παράδειγμα, στην εργασία [@TuningGamingAgents], ο πίνακας Q αποτελείται από μόλις 96 στοιχεία, ενώ στην εργασία των [@Swede], το μέγεθος του πίνακα Q είναι 576. Επομένως, παρά τις διακριτοποιήσεις που κάναμε, το πρόβλημα παραμένει αρκετά μεγάλο, ενώ είναι πιθανή η δημιουργία νέων προβλημάτων, λόγω της έλλειψης ακρίβειας.

Αξίζει να σημειωθεί, πως εξετάστηκε και η τεχνική της διακριτοποίησης με λογαρίθμηση, όσον αφορά την απόσταση του πράκτορα από τον στόχο του. Συγκεκριμένα, αντικαταστήσαμε τα Offset_x και Offset_y με την απόσταση του πράκτορα από την ελεύθερη θέση στάθμευσης (*Distance*). Στη συνέχεια, το distance διακριτοποιήθηκε λογαριθμικά, έτσι ώστε να παρέχει μεγαλύτερη ακρίβεια στον πράκτορα, όταν αυτός βρίσκεται κοντά στη θέση στάθμευσης και λιγότερη ακρίβεια, όταν αυτή δεν είναι απαραίτητη, δηλαδή όταν ο πράκτορας απέχει πολύ από τη θέση στάθμευσης. Ωστόσο, τελικά προτιμήθηκε η χρήση των Offset_x και Offset_y, καθώς προσφέρουν επιπλέον την πληροφορία της κατεύθυνσης της θέσης, ενώ μειώνουν περισσότερο το μέγεθος του πίνακα Q.

Για την αντιμετώπιση του διλήμματος της εξερεύνησης-αξιοποίησης, χρησιμοποιήσαμε την τεχνική decaying $\epsilon$-greedy. Συγκεκριμένα, για τη μείωση της τιμής του $\epsilon$ χρησιμοποιήσαμε την εξίσωση \ref{eq:training:q-learning:epsilon}:
\begin{equation}
\epsilon = \epsilon_{\text{min}} + (\epsilon_{\text{max}} - \epsilon_{\text{min}}) \times e^{(-\text{decay\_rate} \times \text{episode})}
\label{eq:training:q-learning:epsilon}
\end{equation}
όπου $\epsilon_{\text{min}} = 0.0001$, $\epsilon_{\text{max}} = 1$ και $\text{decay\_rate} = 0.0001$. Επιλέξαμε τις τιμές αυτές μετά από διάφορους πειραματισμούς. Σημαντική ήταν η παράμετρος του ρυθμού εξασθένησης του $\epsilon$ (decay_rate), καθώς αυτή επηρεάζει τον ρυθμό σύγκλισης του αλγορίθμου. Ωστόσο, καταλήξαμε στην τιμή 0.0001, καθώς με μικρότερες τιμές, ο αλγόριθμος συνέκλινε σημαντικά αργότερα και χρειαζόταν μεγαλύτερος αριθμός βημάτων εκπαίδευσης, αλλά τελικά ο πράκτορας έφτανε στο ίδιο επίπεδο απόδοσης. Από την άλλη, με μεγαλύτερες τιμές, ο αλγόριθμος συνέκλινε γρηγορότερα, αλλά σε χαμηλότερο επίπεδο απόδοσης. Έτσι, η εξασθένηση του $\epsilon$ φαίνεται στην *Εικόνα @fig:training:q-learning:epsilon*.

![Εξασθένηση του $\epsilon$.](5-training/figures/epsilon.png){#fig:training:q-learning:epsilon width=80%} 

Από την εμπειρία μας, παρατηρήσαμε πως η απόδοση του πράκτορα δεν βελτιωνόταν περαιτέρω κατά την εκπαίδευση, όταν το $\epsilon$ προσέγγιζε το 0. Για αυτό, σταματούσαμε τις εκπαιδεύσεις στα 20000 βήματα.

Επίσης, σημαντικό ρόλο παίζει η παράμετρος του ρυθμού μάθησης $\alpha$ (*learning rate*). Πειραματιστήκαμε με την εξασθένηση κι αυτής της παραμέτρου κατά την εκπαίδευση, ώστε ο πράκτορας να μαθαίνει πιο γρήγορα στην αρχή και να σταθεροποιεί την εκπαίδευσή του στο τέλος. Ωστόσο, καταλήξαμε τελικά στη σταθερή τιμή $\alpha=1$, καθώς το περιβάλλον είναι ντετερμινιστικό κι έτσι, μία συγκεκριμένη ενέργεια σε μία συγκεκριμένη κατάσταση, θα δίνει πάντα την ίδια ανταμοιβή στον πράκτορα.

Κατά τη διάρκεια των εκπαιδεύσεων, δοκιμάστηκαν διάφορες συναρτήσεις ανταμοιβής, όπως και αυτές που είδαμε στην προηγούμενη ενότητα. Όμως, ο πράκτορας δεν κατάφερε να φτάσει με καμία σε ικανοποιητική απόδοση, ενώ συνέκλινε συχνά σε υποβέλτιστες πολιτικές, όπως το να κινείται διαδοχικά εμπρός-πίσω όταν βρίσκεται κοντά στον στόχο του, ώστε να λαμβάνει μικρότερη τιμωρία για την απόσταση του.

Ακόμα, δοκιμάστηκε η μέθοδος του early stopping, με τα επεισόδια να τερματίζονται στην 1^η^ σύγκρουση του πράκτορα με άλλο αντικείμενο. Ωστόσο, αυτή η μέθοδος δεν έφερε καμία βελτίωση στην απόδοση του πράκτορα.

### Καλύτερες εκπαιδεύσεις {#sec:training:q-learning:best-training}

Οι εκπαιδεύσεις με τον αλγόριθμο Q-Learning δεν κατάφεραν να φτάσουν σε ικανοποιητική απόδοση στο συνολικό πρόβλημα, δηλαδή στο επίπεδο δυσκολίας 4. Ωστόσο, σε απλοποιήσεις του προβλήματος, οι πράκτορες κατάφεραν να πετύχουν κάποια, ενδεικτικά αποτελέσματα, τα οποία αποτελούν απόδειξη της σωστής λειτουργίας του αλγορίθμου.

Συγκεκριμένα, στο επίπεδο δυσκολίας 1, όπου τόσο η θέση στάθμευσης, όσο και η αρχική θέση του πράκτορα είναι προκαθορισμένες και η στάθμευση είναι άμεση, ο πράκτορας ανέπτυξε μία ικανή πολιτική, όπως φαίνεται στην *Εικόνα @fig:training:q-learning:best-training1*.

![Καλύτερη εκπαίδευση του Q-Learning στο επίπεδο δυσκολίας 1.](5-training/figures/QL1.png){#fig:training:q-learning:best-training1 width=70%}

Αξιολογώντας τον πράκτορα, παρατηρήσαμε πως παρκάρει επιτυχώς, σε αυτήν τη συγκεκριμένη περίπτωση.

$\\$
$\\$
$\\$
$\\$
$\\$
$\\$

Στο επίπεδο δυσκολίας 2, όπου η θέση στάθμευσης είναι ημι-τυχαία, ο πράκτορας πέτυχε την καλύτερη του απόδοση, με τη συνάρτηση με διαμόρφωση ανταμοιβής (βλ. υποενότητα @sec:training:modeling:reward). Οι μετρικές της εκπαίδευσης φαίνονται στην *Εικόνα @fig:training:q-learning:best-training2*.

![Καλύτερη εκπαίδευση του Q-Learning στο επίπεδο δυσκολίας 2.](5-training/figures/QL2.png){#fig:training:q-learning:best-training2 width=83%}

Παρατηρούμε πως η γραφική της μέσης ανταμοιβής έχει την επιθυμή, θετική κλίση, όμως το ποσοστό επιτυχίας κυμαίνεται σε πολύ χαμηλά επίπεδα, μικρότερα του 10%.

Τέλος, στο επίπεδο 3, όπου η θέση στάθμευσης είναι πλήρως τυχαία, τα αποτελέσματα ήταν ακόμα χειρότερα, όπως φαίνεται και στην *Εικόνα @fig:training:q-learning:best-training3*. 

![Καλύτερη εκπαίδευση του Q-Learning στο επίπεδο δυσκολίας 3.](5-training/figures/QL3.png){#fig:training:q-learning:best-training3 width=83%}

Συγκεκριμένα, βλέπουμε πως το ποσοστό επιτυχίας του πράκτορα είναι σχεδόν μηδαμινό. Πράγματι, κατά την αξιολόγηση του πράκτορα σε 1000 επεισόδια, το ποσοστό επιτυχίας του ήταν 1.1%.

Από τα παραπάνω αποτελέσματα, γίνεται κατανοητό πως η τεχνική της διακριτοποίησης δεν ήταν αρκετή για την επίλυση του προβλήματος. Για αυτό, στη συνεχεία επιλέχθηκε η μέθοδος της προσέγγισης συνάρτησης, δηλαδή η χρήση νευρωνικών δικτύων. Ωστόσο, δεν επιλέξαμε τον αλγόριθμο DQN, αλλά άλλους αλγορίθμους βαθιάς ενισχυτικής μάθησης, καθώς ο DQN δέχεται συνήθως ως είσοδο την εικόνα του παιχνιδιού (*raw pixel data*) κι έτσι, απαιτεί μεγάλη υπολογιστική ισχύ.