## Εκπαιδεύσεις με τον αλγόριθμο PPO {#sec:training:ppo}

### Επισκόπηση εκπαιδεύσεων {#sec:training:ppo:overview}

Ο αλγόριθμος PPO ήταν ο πρώτος αλγόριθμος βαθιάς ενισχυτικής μάθησης που εξετάστηκε κι έτσι, δοκιμάστηκαν σε αυτόν πολλές διαφορετικές τεχνικές. Συνολικά, διεξήχθησαν 107 εκπαιδεύσεις του αλγορίθμου, οι οποίες διήρκησαν αθροιστικά 39 ημέρες, 22 ώρες και 56 λεπτά. Ορισμένες από τις μεθόδους που δοκιμάστηκαν, αλλά δεν είχαν τα επιθυμητά αποτελέσματα, αναλύονται παρακάτω.

Αρχικά, δοκιμάστηκαν διάφορες συναρτήσεις ανταμοιβής, όπως αυτές που περιγράφηκαν στην υποενότητα @sec:training:modeling:reward, με πολλές παραλλαγές των τιμών των ανταμοιβών τους. Μερικές από τις πιο ενδιαφέρουσες ίδεες που εξετάστηκαν, ήταν οι εξής:

- Αλλαγή της τιμωρίας με βάση την απόσταση πράκτορα-στόχου, σε επιβράβευση, με βάση την ίδια απόσταση. Η ίδεα αυτή βασίστηκε στη θεώρηση, πως η θετική ανταμοιβή μπορεί να ενθαρρύνει τον πράκτορα να εξερευνήσει το περιβάλλον, ενώ η συνεχής αρνητική ανταμοιβή ίσως καθιστά πιο δύσκολη την εξερεύνηση, καθώς ο πράκτορας λαμβάνει συνεχώς ποινές.
- Προσθήκη αυξανόμενης τιμωρίας, με βάση τα βήματα του πράκτορα. Η ιδέα αυτή βασίστηκε στη σκέψη, πως ο πράκτορας μπορεί να μάθει να εκτελεί την εργασία του πιο γρήγορα, αν τιμωρείται για κάθε περιττό βήμα που κάνει.
- Αλλαγή της συνάρτησης ανταμοιβής, ώστε αυτή να προσομοιώσει τη διαδικασία στάθμευσης από έναν άνθρωπο. Συγκεκριμένα, ο πράκτορας πρώτα επιβραβευόταν για την σωστή θέση του στον άξονα x, μετά για τη σωστή γωνία και τέλος, για τη σωστή θέση του στον άξονα y.

Σχετικά με τις παραμέτρους του αλγορίθμου, δοκιμάσαμε διαφορετικές τιμές για το ρυθμό μάθησης και το συντελεστή εντροπίας, ενώ σχετικά με το νευρωνικό δίκτυο του αλγορίθμου, δοκιμάσαμε την αλλαγή της συνάρτησης ενεργοποίησης του σε ReLU, καθώς και διαφορετικά πλήθη νευρώνων για τα κρυφά επίπεδα του. Ακόμα, εξετάστηκε η μετατροπή του χώρου ενεργειών σε συνεχή, όπως στην περίπτωση των αλγορίθμων δράστη-κριτή. Ωστόσο, δεν παρατηρήθηκε καμία διαφορά στις επιδόσεις των πρακτόρων.

Επιπλέον, δοκιμάστηκε η μέθοδος εξερεύνησης generalized State Dependent Exploration (gSDE), αντί της προεπιλεγμένης εξερεύνησης του αλγορίθμου (action noise exploration). Η SDE αποτελεί μια μέθοδο εξερεύνησης, η οποία προσθέτει θόρυβο στην ενέργεια του πράκτορα, με βάση την τρέχουσα κατάσταση. Αυτό έχει ως αποτέλεσμα, μία πιο ομαλή και συνεπή εξερεύνηση, σε σχέση με άλλες μεθόδους. Η gSDE είναι μία βελτίωση της SDE, η οποία προσθέτει τη δειγματοληψία των παραμέτρων της συνάρτησης εξερεύνησης ανά συγκεκριμένο αριθμό βημάτων, καθώς και τη χρήση επιλεγμένων χαρακτηριστικών της πολιτικής, ως είσοδο στη συνάρτηση εξερεύνησης [@gsde]. Δοκιμάσαμε η δειγματοληψία να γίνεται κάθε 1, 4, 20 και 100 βήματα, αλλά σε κάθε περίπτωση, τα αποτελέσματα ήταν χειρότερα σε σχέση με την προεπιλεγμένη μέθοδο εξερεύνησης του αλγορίθμου κι έτσι, η μέθοδος gSDE εγκαταλείφθηκε.

Παρόμοια ήταν τα αποτελέσματα, όταν εξετάστηκε η τεχνική του early stopping στην 2^η^ σύγκρουση του πράκτορα. Συγκεκριμένα, αυτή απλώς ώθησε τον πράκτορα να κάνει τον γύρο του χάρτη, αποφεύγοντας έτσι τις συγκρούσεις, αλλά χωρίς να πετυχαίνει το στόχο της στάθμευσης.

Ακόμα, δοκιμάστηκε η αφαίρεση των episode cutoffs, δηλαδή του ανώτατου ορίου των 600 βημάτων για κάθε επεισόδιο. Τα αποτελέσματα ήταν ενδιαφέροντα, αλλά όχι ικανοποιητικά. Συγκεκριμένα, το μέσο μήκος των επεισοδίων κατά την εκπαίδευση ήταν 2500 βήματα, δηλαδή ο πράκτορας χρειαζόταν περίπου τόσα βήματα για να παρκάρει. Ωστόσο, η συνολική ανταμοιβή κάθε επεισοδίου ήταν αρνητική σε μεγάλο βαθμό, καθώς ο πράκτορας προτού παρκάρει, συγκρουόταν επανειλημμένα με άλλα αντικείμενα.

Τέλος, δοκιμάστηκε η τεχνική του FrameSkip, η οποία εφαρμόστηκε με επιτυχία στους αλγορίθμους δράστη-κρίτη, αλλά δεν είχε ικανοποιητικά αποτελέσματα στον αλγόριθμο PPO κι έτσι, δεν χρησιμοποιήθηκε στις καλύτερες εκπαιδεύσεις του.

### Καλύτερες εκπαιδεύσεις {#sec:training:ppo:best-training}

Τελικά, τα καλύτερα αποτελέσματα με τον αλγόριθμο PPO, προέκυψαν με την αρχιτεκτονική του δικτύου και με τη συνάρτηση ανταμοιβής που αναλύθηκαν στην ενότητα @sec:training:modeling, καθώς και με τιμή του συντελεστή εντροπίας ίση με 0.01.

Συγκεκριμένα, στο επίπεδο δυσκολίας 1, μετά από 5Μ steps, το success rate του πράκτορα προσεγγίζει το 100%. Ωστόσο, με τις ίδιες παραμέτρους εκπαίδευσης, τα αποτελέσματα στο επίπεδο δυσκολίας 2 δεν ήταν ικανοποιητικά. Για αυτό, αναγκαστήκαμε να παρεμβάλουμε ένα ενδιάμεσο επίπεδο δυσκολίας, μεταξύ των 1 και 2, στο οποίο η αρχική θέση του πράκτορα θα είναι τυχαία, αλλά ανάλογα με αυτήν, η ελεύθερη θέση στάθμευσης θα είναι είτε η κεντρική θέση της πάνω σειράς θέσεων, είτε της κάτω. Στο επίπεδο αυτό, μετά από 8Μ steps, το success rate του πράκτορα προσεγγίζει το 100%, όπως φαίνεται από την κόκκινη καμπύλη στην *Εικόνα @fig:training:ppo:best-training* (εκπαίδευση 59).

Στη συνέχεια, εφαρμόσαμε Curriculum Learning στον πράκτορα των 20M steps της εκπαίδευσης 59, εκπαιδεύοντας τον στο επίπεδο δυσκολίας 2. Μετά από 53M steps, το success rate συγκλίνει στο 95%, όπως φαίνεται από τη γκρι καμπύλη στην *Εικόνα @fig:training:ppo:best-training* (εκπαίδευση 65Β). Παρατηρούμε μάλιστα, πως το success rate ξεκινάει από την τιμή 50%, το οποίο είναι αναμενόμενο, λόγω της εφαρμογής κλιμακωτής μάθησης. Με άλλα λόγια, ήδη από την αρχή της εκπαίδευσης, ο πράκτορας έχει κάποια χρήσιμη γνώση, για να λύσει το πρόβλημα.

Η ίδια τεχνική εφαρμόστηκε και για το επίπεδο 3, επανεκπαιδεύοντας αυτήν τη φορά τον πράκτορα των 50M steps της εκπαίδευσης 65Β. Μετά από 12Μ steps, το success rate κυμαίνεται γύρω από το 75%, όπως φαίνεται από τη μπλε καμπύλη στην *Εικόνα @fig:training:ppo:best-training* (εκπαίδευση 67Β). Ωστόσο, η εκπαίδευση 67Β δεν συνεχίστηκε περαιτέρω, λόγω χρονικών περιορισμών.

![Καλύτερες εκπαιδεύσεις του αλγορίθμου PPO: με κόκκινο χρώμα στο επίπεδο μεταξύ 1 και 2, με γκρι χρώμα στο επίπεδο 2 και με μπλε χρώμα στο επίπεδο 3.](5-training/figures/PPO-best-training.png){width=100% #fig:training:ppo:best-training}

Παρόλα αυτά, όταν δοκιμάστηκε η εφαρμογή Curriculum Learning για το επίπεδο δυσκολίας 4, τα αποτελέσματα δεν ήταν αντίστοιχα με πριν. Αντίθετα, μετά από 40Μ steps, το success rate συνέκλινε στην τιμή 20%. Επομένως, οι εκπαιδεύσεις με τον αλγόριθμο PPO απέτυχαν να λύσουν το πραγματικό πρόβλημα της αυτόματης στάθμευσης.