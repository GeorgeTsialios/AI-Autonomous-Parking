\begin{algorithm}
\floatname{algorithm}{Q-Learning Algorithm}
\caption{}
\begin{algorithmic}[1]
\Statex \vspace{0.5em}  
 \State Initialize $Q(s, a)$ to 0 for all $\alpha \in A$ in each $s \in S$
    \State Initialize learning rate $\alpha \in (0, 1]$
    \State Initialize discount factor $\gamma \in [0, 1]$
    \State Initialize exploration rate $\epsilon \in [0, 1]$
    \While{not converged}
        \State $s \gets s_{0}$
        \While{$s$ not terminal}
            \State Observe current state $s$
            \If{$explore()$}\State $a \gets$ random action\Else \State $a \gets \arg\max_{a} Q(s, a)$\EndIf

            \State Take action $a$, observe reward $R(s, a)$ and next state $s'$
            \State Update Q-value:
            \[
            Q(s, a) \gets (1-a) \cdot Q(s, a) + \alpha \left[ R(s, a) + \gamma \cdot\max_{a} Q(s', a)\right]
            \]
            \State $s \gets s'$
        \EndWhile
    \EndWhile
\Statex  
\end{algorithmic}
\end{algorithm}
