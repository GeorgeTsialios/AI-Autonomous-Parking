## Ενισχυτική Μάθηση {#sec:theory:reinforcement_learning}

### Γενική επισκόπηση {#sec:theory:reinforcement_learning:overview}

#### Κεντρική ιδέα {.unnumbered}

Η Ενισχυτική Μάθηση (*Reinforcement Learning*) αποτελεί την κατηγορία της μηχανικής μάθησης, στην οποία ένας πράκτορας μαθαίνει από την εμπειρία του, καθώς αλληλεπιδράει με το περιβάλλον του. Ο πράκτορας ενισχυτικής μάθησης επιλέγει ελεύθερα δράσεις και δέχεται ανάδραση για αυτές, υπό μορφή ανταμοιβής. Συγκεκριμένα, ο πράκτορας λαμβάνει θετική ανταμοιβή, όταν οι ενέργειες του οδηγούν σε επιθυμητά αποτελέσματα και αρνητική ανταμοιβή (τιμωρία), όταν οδηγούν σε ανεπιθύμητα αποτελέσματα. Έτσι, μέσω δοκιμών και λαθών (*trial and error*), ο πράκτορας μαθαίνει σταδιακά να παίρνει αποφάσεις που μεγιστοποιούν τις ανταμοιβές του. Η διαδικασία αυτή της εκπαίδευσης διαφέρει σημαντικά σε σχέση με τις δύο προηγούμενες κατηγορίες μηχανική μάθησης, αφού πλέον ο αλγόριθμος εκπαιδεύεται χωρίς κάποιο σταθερό σύνολο δεδομένων εισόδου. Αντίθετα, η εκπαίδευση στην ενισχυτική μάθηση είναι μία δυναμική διαδικασία, στην οποία ο πράκτορας εκτελεί ενέργειες και μεταβάλλει το περιβάλλον του. Επομένως, πρόκειται για μία ενεργή διαδικασία μάθησης, η οποία θυμίζει τον τρόπο με τον οποίο οι ζωντανοί οργανισμοί μαθαίνουν.

Πράγματι, ας αναλογιστούμε το παράδειγμα της εκπαίδευσης ενός σκύλου. Στο σενάριο μας, ο ιδιοκτήτης του σκύλου επιθυμεί να του μάθει την ενέργεια «Κάτσε».  Έτσι, ο ιδιοκτήτης του σκύλου δείχνει με το χέρι του το έδαφος και φωνάζει προς τον σκύλο «Κάτσε». Όσο ο σκύλος στέκεται όρθιος, ο ιδιοκτήτης του δίνει αρνητική ανταμοιβή (πχ του φωνάζει «κακό σκυλί», έχοντας τεντωμένο τον δείκτη του και δείχνοντας προς αυτό). Όταν ο σκύλος κάθεται, ο ιδιοκτήτης του δίνει θετική ανταμοιβή (πχ του χαϊδεύει το κεφάλι ή του δίνει μία λιχουδιά). Έτσι, ο σκύλος μαθαίνει σταδιακά, πως όταν παρατηρεί τον άνθρωπο στην κατάσταση «Κάτσε» (δηλ. χέρι προς το έδαφος και προφορική εντολή), η ενέργεια του να καθίσει στο έδαφος του προσφέρει θετική ανταμοιβή και για αυτό την επιλέγει.

Γίνεται πλέον σαφές, πως στόχος της ενισχυτικής μάθησης είναι η εκπαίδευση του πράκτορα, ώστε να προβαίνει σε ενέργειες που μεγιστοποιούν την ανταμοιβή του. Μάλιστα, μία σημαντική παρατήρηση είναι πως ο πράκτορας πρέπει να μάθει να μεγιστοποιεί τη συνολική ανταμοιβή του, δηλ. να σκέπτεται μακροπρόθεσμα. Συγκεκριμένα, ενδέχεται μία δράση του πράκτορα να οδηγήσει σε αρνητική ανταμοιβή άμεσα, όμως σε βάθος χρόνου να βοηθήσει τον πράκτορα να πετύχει μεγάλη θετική ανταμοιβή. Για παράδειγμα, στο παιχνίδι του σκακιού υπάρχει η στρατηγική της θυσίας (*sacrifice*), στην οποία ο παίκτης επιλέγει να χάσει ένα κομμάτι (πχ την βασίλισσα του), για να κερδίσει στο μέλλον κάτι μεγαλύτερης αξίας (πχ να πετύχει ρουά-ματ στον αντίπαλο βασιλιά).

#### Πλεονεκτήματα {.unnumbered}

Η εκπαίδευση με ενισχυτική μάθηση έχει ορισμένα σημαντικά πλεονεκτήματα σε σχέση με τις πιο παραδοσιακές μεθόδους μηχανικής μάθησης, τα οποία περιγράφονται παρακάτω:

- Δεν χρειάζεται μεγάλα **σύνολα δεδομένων**:  σε πολλά πεδία είναι δύσκολο να συλλεχθούν δεδομένα ή δεν υπάρχουν στον βαθμό που απαιτείται για μία αποδοτική εκπαίδευση επιβλεπόμενης μάθησης. Η ενισχυτική μάθηση αποτελεί μία εναλλακτική λύση, καθώς ο πράκτορας μαθαίνει μόνος του, χωρίς προηγούμενα δεδομένα. 
- Δεν απαιτεί **γνώση ειδικού** στο πεδίο εφαρμογής: στην επιβλεπόμενη μάθηση, τα δεδομένα αποτελούν ζεύγη εισόδου-επιθυμητής εξόδου. Για παράδειγμα, για την εκπαίδευση ενός μοντέλου να παίζει σκάκι, τα δεδομένα θα ήταν της μορφής: κατάσταση σκακιέρας-κίνηση που έκανε ο παίκτης. Έτσι, το μοντέλο θα μάθαινε να παίζει σκάκι με τον τρόπο που παίζουν οι παίκτες στα δεδομένα εκπαίδευσης. Άρα, θα έπρεπε αυτοί οι παίκτες να είναι ειδικοί στο παιχνίδι, προκειμένου να πετύχει το μοντέλο υψηλή απόδοση. Αντίθετα, στην ενισχυτική μάθηση, οι σχεδιαστές ενός συστήματος αρκεί να έχουν βασική γνώση του πεδίου εφαρμογής του, για να εκπαιδεύσουν επιτυχώς έναν πράκτορα. Για παράδειγμα, στην περίπτωση του σκακιού, αρκεί οι σχεδιαστές να γνωρίζουν τους κανόνες του παιχνιδιού, ώστε πχ να επιβραβεύουν τον πράκτορα όταν κερδίζει κομμάτια του αντιπάλου και να τον τιμωρούν όταν χάνει κομμάτια του.
- προσφέρει **καινοτόμες** λύσεις: η επιβλεπόμενη μάθηση ουσιαστικά μιμείται τα δεδομένα εκπαίδευσης. Ναι μεν μπορεί να επιτυχεί υψηλότερη απόδοση από τον άνθρωπο (πχ σκάκι), όμως δεν μπορεί να μάθει μία εντελώς νέα προσέγγιση για την επίλυση του προβλήματος. Αντίθετα, οι αλγόριθμοι ενισχυτικής μάθησης μπορούν να προτείνουν εντελώς νέες και επαναστατικές λύσεις, τις οποίες δεν είχε σκεφτεί ποτέ κάποιος άνθρωπος. Ένα τέτοιο αξιοσημείωτο παράδειγμα συνέβη στη νική του AlphaGo, ενός συστήματος ενισχυτικής μάθησης έναντι του παγκόσμιου πρωταθλητή στο Go, Lee Sedol. Κατά τη διάρκεια του αγώνα, το AlphaGo έκανε μία ασυνήθιστη κίνηση (κίνηση 37) που αρχικά θεωρήθηκε λάθος από τους ειδικούς στο παιχνίδι. Μάλιστα, υπολογίστηκε ότι η πιθανότητα ένας άνθρωπος να προέβαινε σε αυτή την κίνηση στη συγκεκριμένη θέση ήταν ίση με 0.0001%. Ωστόσο, αυτή η κίνηση αποδείχθηκε στη συνέχεια ότι ήταν υψηλά δημιουργική και καθοριστική για τη νίκη του AlphaGo [@wiredMovesAlphaGo].
- Είναι κατάλληλη για περιβάλλοντα **σειριακής λήψης αποφάσεων**: η ενισχυτική μάθηση μαθαίνει στον πράκτορα να μεγιστοποιεί τη συνολική ανταμοιβή σε βάθος χρόνου. Έτσι, είναι κατάλληλη για σενάρια όπου οι αποφάσεις είναι σειριακές και το αποτέλεσμα μίας ενέργειας επηρεάζει τις μελλοντικές αποφάσεις. Αντίθετα, στην επιβλεπόμενη μάθηση, το μοντέλο πραγματοποιεί ανεξάρτητες προβλέψεις σε κάθε βήμα, χωρίς να λαμβάνει υπόψη το πως αυτές θα επηρεάσουν τις μελλοντικές αποφάσεις.

Ωστόσο, η ενισχυτική μάθηση έχει και μειονεκτήματα, τα οποία δυσκολεύουν την επιτυχή εφαρμογή της. Πολλά από αυτά, τα συνάντησα κατά τη διάρκεια της εκπαίδευσης του πράκτορα αυτόματης στάθμευσης. Για αυτό, περιγράφονται στο Κεφάλαιο --------- της εκπαίδευσης, στην Ενότητα --------.

#### Εφαρμογές {.unnumbered}

Η ενισχυτική μάθηση αποτελεί ουσιαστικά την επιστημή της λήψης αποφάσεων κι ως εκ τούτου, έχει εφαρμογές σε πολλά πεδία. Στο πεδίο των χρηματοοικονομικών, η εταιρία Goldman Sachs έχει ξεκινήσει τη χρήση της ενισχυτικής μάθησης στις πλατφόρμες συναλλαγών της για βελτιώσει την απόδοση των επενδυτικών στρατηγικών της [@efinancialcareersGoldmanSachs]. Στον τομέα της βιοτεχνολογίας, η εταιρία Atomwise χρησιμοποιεί την ενισχυτική μάθηση στην πλατφόρμα της AtomNet, που χρησιμοποιείεται για την
ανακάλυψη νέων φαρμάκων [@atomwiseAtomNetTechnology]. Στον χώρο της ρομποτικής, η εταιρία Boston Dynamics έχει ενσωματώσει την ενισχυτική μάθηση στα συστήματα ελέγχου του τετράποδου ρομπότ της, Spot και έχει βελτιώσει την αυτόνομη πλοήγηση του σε πολύπλοκα περιβάλλοντα [@bostondynamicsStartingRight].

Ωστόσο, το επικρατέστερο πεδίο εφαρμογής των αλγορίθμων ενισχυτικής μάθησης είναι τα παιχνίδια. Αυτό οφείλεται στο γεγονός ότι τα παιχνίδια αποτελούν ένα ασφαλές χώρο εκπαίδευσης πρακτόρων και ανάπτυξης αλγορίθμων, προτού αυτοί εφαρμοστούν σε προβλήματα του πραγματικού κόσμου. Μάλιστα, σε περιβάλλοντα προσομοιώσεων δεν υπάρχουν χρονικοί περιορισμοί, οπότε ο πράκτορας μπορεί να εκπαιδεύεται ασταμάτητα για μεγάλα χρονικά διαστήματα, κάτι που συχνά είναι απαραίτητο για την επιτυχία της ενισχυτικής μάθησης. Επίσης, τα παιχνίδια απαιτούν σε σημαντικό βαθμό νοητικές ικανότητες από τον παίκτη, κι έτσι αποτελούν μία καλή πλατφόρμα εκπαίδευσης αλγορίθμων τεχνητής νοημοσύνης.

Οι πρώτες απόπειρες έγιναν το 1959 από τον Arthur Samuel, ο οποίος ανέπτυξε ένα πρόγραμμα που έπαιζε το παιχνίδι της ντάμας [@5389202]. Στη συνέχεια, το 1992, ο Gerald Tesauro ανέπτυξε τον αλγόριθμο TD-Gammon με τη βοήθεια νευρωνικών δικτύων, ο οποίος έπαιζε τάβλι και κατάφερε να φτάσει σε επίπεδο ανάλογο των τριών κορυφαίων παικτών στον κόσμο[@6795979].

Ωστόσο, μέχρι και τη δεκαετία του 2010, υπήρχαν σημαντικοί περιορισμοί στην εφαρμογή της ενισχυτικής μάθησης σε πολύπλοκα προβλήματα ή προβλήματα μεγάλης διαστασιμότητας. Παρόλο που είχαν ήδη αναπτυχθεί αλγόριθμοι επίλυσης
τέτοιων προβλημάτων, η μικρή διαθέσιμη υπολογιστική ισχύ της εποχής δεν επέτρεπε την εκπαίδευση των μοντέλων σε λογικά χρονικά διαστήματα. Το πρόβλημα αυτό υπερκεράστηκε από την τεχνολογική ανάπτυξη στον τομέα του υλικού (*hardware*) με χαρακτηριστικό παράδειγμα την παραλληλοποίηση των υπολογισμών στις μονάδες επεξεργασίας γραφικών (*GPU*), οι οποίες επιταχύνουν σημαντικά τη διαδικασία της εκπαίδευσης. Έτσι, ξεκίνησε η εποχή της βαθιάς ενισχυτικής μάθησης και άρχισαν να φαίνονται για πρώτη φορά οι πραγματικές δυνατότητες των νευρωνικών δικτύων.



Οι δυνατότητες των νευρωνικών δικτύων σε προβλήματα μεγάλης διάστασης ήταν
εκπληκτικές, αρχής γενομένης του αλγορίθμου DQN της DeepMind το 2013 που
υλοποιείται και στην παρούσα εργασία. Ο αλγόριθμος εφαρμόστηκε επιτυχώς στα 49
παιχνίδια της πλατφόρμας Atari2600


η εταιρία Deepmind της Google η
οποία το 2020 παρουσίασε στον κόσμο τον Agent57, ο οποίος πέτυχε
αποτελέσματα εως και 100 φορές καλύτερα από επαγγελματίες ανθρώπινους
παίχτες σε όλα τα παιχνίδια που αντιμετώπισε[2]

Σε μονοπρακτορικό επίπεδο, το ενδιαφέρον έχει μονοπωλήσει η εταιρεία
DeepMind της Google. Εντός του 2020 παρουσίασε τον καλύτερο έως τώρα αλγόριθμο
στα model-free παιχνίδια Atari, τον Agent57. Ανήκει στους αλγορίθμους Q-learning και
με τη βοήθεια ενός μετα-ελεγκτή, ο οποίος προσαρμόζει την εξερεύνηση και τη
μακροπρόθεσμη έναντι της βραχυπρόθεσμης συμπεριφοράς του πράκτορα,
κατόρθωσε να σημειώσει μεγαλύτερες επιδόσεις από επαγγελματίες παίκτες σε
ποσοστό 1000% σε καθένα από τα 57 παιχνίδια που εφαρμόστηκε. Ήταν η πρώτη
φορά στα χρονικά που ένας αλγόριθμος κατάφερε κάτι ανάλογο στο σύνολο των
παιχνιδιών, που διακρίνονται για την πολυπλοκότητα και τη διαφορετικότητά τους

Για τα πολυπρακτορικά προβλήματα τα καλύτερα αποτελέσματα φαίνεται να
προκύπτουν από αλγόριθμους ενισχυτικής μάθησης όπως ο OpenAI Five, της
εταιρίας OpeanAI ο οποίος έχει χρησιμοποιηθεί για να νικήσει ενάντια σε
επαγγελματίες παίχτες στο παιχνίδι Dota 2 , ένα ανταγωνιστικό “esports “ παιχνίδι
αβέβαιης πληροφορίας με ιδιαίτερα πολύπλοκες και συνεχείς state-action χώρους.
Οι πράκτορες του OpenAI Five έμαθαν μέσω self-play που πρόκειται για την πλέον
εδραιωμένη μέθοδο εκμάθησης σε τέτοια προβλήματα μαθαίνοντας από πακέτα
πληροφοριών που αποτελούνταν από 2 εκατομμύρια καρε κάθε 2 δευτερόλεπτα
ενώ εκπαιδεύτηκαν ασταμάτητα για 10 μήνες[6]. 

Η εταιρεία OpenAI έχει αναπτύξει μία παραλλαγή του αλγορίθμου
Proximal Policy Optimization, τον αλγόριθμο OpenAI Five, για την κατασκευή
πρακτόρων στο παιχνίδι Dota2 αλγορίθμου OpenAI Five ήταν εκπληκτικά, αφού κατάφερε να νικήσει το 2019 την
παγκόσμια πρωταθλήτρια ομάδα για το έτος 2018, OG team.

Αξιοσημείωτα
παραδείγματα εφαρμογών αποτελούν η νίκη του αλγορίθμου AlphaGo επί του
καλύτερου επαγγελματία άνθρωπο παίκτη στον κόσμο στο παιχνίδι Go, η νίκη του
αλγορίθμου AlphaStar της ομάδας DeepMind έναντι επαγγελματιών παικτών στο
παιχνίδι StarCraftII και η επιτυχία της ομάδας OpenAI, η οποία ανέπτυξε πράκτορα
στο παιχνίδι Dota-2 που κατατρόπωσε τους παγκόσμιους έως τότε πρωταθλητές.
Τέλος, εντός της χρονιάς του 2020, ξεπεράστηκαν για πρώτη φορά από κάποιον
αλγόριθμο τεχνητής νοημοσύνης, τον Agent57, οι ανθρώπινες επιδόσεις
επαγγελματιών παικτών στο σύνολο των 57 παιχνιδιών Atari 2600. Όλοι αυτοί οι
επιτυχώς εφαρμοσμένοι αλγόριθμοι ανήκουν στο χώρο της βαθιάς ενισχυτικής
μάθησης. 

Σε πλήθος παιχνιδιών η μηχανική μάθηση έχει καταφέρει να φτάσει ή και να
υπερκεράσει τις ανθρώπινες επιδόσεις, δηλαδή η μηχανή σκέφτεται καλύτερα από τον
άνθρωπο.

### Βασικές Έννοιες και Ορολογία {#sec:theory:reinforcement_learning:concepts}
Στη διαδικασία αυτή, ο πράκτορας πρέπει να εξετάσει την ισορροπία μεταξύ της εξερεύνησης (της ανακάλυψης νέων περιοχών) και της εκμετάλλευσης (της χρήσης της υπάρχουσας γνώσης).
μελετησουμε σε μεγαλυτερο βαθος

#### RL CYCLE + ΕΙΚΟΝΑ

Κάθε πρόβλημα ενισχυτικής μάθησης αποτελείται από δύο
βασικές οντότητες, το περιβάλλον και τον πράκτορα, καθώς επίσης και τρία κανάλια
επικοινωνίας αυτών, της ανταμοιβής, των δράσεων και των καταστάσεων.

ενισχυτικής μάθησης, βασιζόμενος στο παραπάνω δόγμα,
δημιουργεί πράκτορες οι οποίοι δύναται να μαθαίνουν διαμέσου της συνεχούς
αλληλεπίδρασης με το περιβάλλον. 
Ο εκάστοτε πράκτορας σε μία ανθρωποκεντρική προσέγγιση, εκπαιδεύεται
με τη βοήθεια οπτικών παρατηρήσεων και της ανταμοιβής που δέχεται από το
περιβάλλον

####  ΠΕΡΙΒΑΛΛΟΝ

Ιδιότητες περιβάλλοντος

- Πλήρως ή μερικώς παρατηρήσιμο
Ένα περιβάλλον μπορεί να είναι πλήρως ή μερικώς παρατηρήσιμο. Πλήρως
παρατηρήσιμο είναι εκείνο, όπου ο πράκτορας έχει πρόσβαση στην πλήρη κατάστασή
του κάθε χρονική στιγμή.

Όταν ο πράκτορας είναι ικανός να έχει όλη τη δυνατή πληροφορία για την
υπάρχουσα κατάσταση του περιβάλλοντος καθ’ όλη τη διάρκεια του πειράματος
τότε το περιβάλλον είναι πλήρως παρατηρήσιμο ειδ’ άλλως είναι μόνο εν μέρη
παρατηρίσιμο

- Στοχαστικό ή αιτιοκρατικό
Αιτιοκρατικό χαρακτηρίζεται το περιβάλλον στο οποίο η επόμενη κατάστασή του
προσδιορίζεται πλήρως από την τρέχουσα κατάστασή του και τη δράση του πράκτορα. an agent can expect the same reward and next state if it repeats
an action in a particular state.
Σε περίπτωση που δεν υπάρχει αυτή η νομοτελειακή συνάφεια μεταξύ καταστάσεων
και δράσεων, το περιβάλλον είναι στοχαστικό. if an agent takes action in a state repeatedly, they cannot be guaranteed to receive
the same reward or the next state

Όταν η πράξη του πράκτορα σε μία ορισμένη κατάσταση οδηγεί με βεβαιότητα και
συνέπεια σε μία επόμενη κατάσταση με τρόπο προφανή και προβλέψιμο τότε το
περιβάλλον ονομάζεται αιτιοκρατικό. Εάν υπάρχει αβεβαιότητα ή τυχαιότητα σε
ό,τι αφορά την επόμενη κατάσταση του περιβάλλοντος δοσμένου ενός ζεύγους
κίνησης – κατάστασης τότε το περιβάλλον είναι στοχαστικό.

- Στατικό ή δυναμικό
Εάν το περιβάλλον μεταβάλλεται ενώ ο πράκτορας σκέπτεται, τότε είναι δυναμικό.
Διαφορετικά, είναι στατικό.

- Διακριτό ή συνεχές
Η διάκριση μεταξύ ενός διακριτού και ενός συνεχούς περιβάλλοντος εξαρτάται
από τον τρόπο που αντιλαμβάνεται το χρόνο και τις καταστάσεις του περιβάλλοντος ο
πράκτορας. Για παράδειγμα, διακριτό περιβάλλον είναι το σκάκι, που αποτελείται από
ένα σύνολο πεπερασμένων διακριτών καταστάσεων. Αντίθετα, συνεχές είναι το
περιβάλλον της πλοήγησης ρομπότ. Η ταχύτητα και η τρέχουσα θέση του οχήματος
λαμβάνουν συνεχείς τιμές σε ένα ορισμένο συνεχές πεδίο τιμών.
Ένα περιβάλλον που αποτελείται από πεπερασμένο αριθμό κινήσεων όπως το σκάκι
ονομάζεται διακριτό ενώ εάν ο αριθμός κινήσεων δεν μπορούν να μετρηθούν είναι
συνεχές, όπως είναι η περίπτωση των αυτοκινούμενων αμαξιών.

- Sparse rewards

- Μονοπρακτορικό ή πολυπρακτορικό
Ανάλογα με το πλήθος των πρακτόρων που συμμετέχουν στο περιβάλλον,
χαρακτηρίζεται μονοπρακτορικό ή πολυπρακτορικό.

 Ο πράκτορας μαθαίνει μέσω της συνεχούς αλληλεπίδρασης με
το περιβάλλον και υιοθετεί κάποια συμπεριφορά ή πολιτική
η πολιτική προσαρμόζεται με βάση κάποια συνάρτηση
ανταμοιβής
Η συνάρτηση
ανταμοιβής έχει ως στόχο την επιβράβευση ή τιμωρία του πράκτορα για τις αποφάσεις
που έλαβε
αναγκαία και ικανή
συνθήκη χαρακτηρισμού ενός προβλήματος ως αντικείμενο ενισχυτικής μάθησης είναι
η αλληλεπίδραση πράκτορα και περιβάλλοντος που εκφράζεται μέσω των καναλιών
παρατήρησης, δράσης και ανταμοιβής
Τα περιβάλλοντα εκπαίδευσης μπορούν να χαρακτηριστούν ως προβλήματα απόφασης Markov (MDP), δηλαδή
ικανοποιούν την ιδιότητα Markov, γεγονός που επιτρέπει την εφαρμογή αλγορίθμων
ενισχυτικής μάθησης σε αυτά. 

MDP ΣΕΛ 25

-Η μαρκοβιανή ιδιότητα:
Η μαρκοβιανή ιδιότητα δηλώνεται ως εξής: «Το μέλλον είναι ανεξάρτητο από το
παρόν δοθέντος του παρόντος» ή με πιο απλά λόγια, αν η τωρινή κατάσταση είναι
γνωστή τότε δεν μας ενδιαφέρει όλη η πληροφορία που έχου

Στο σημείο αυτό είναι αναγκαία η σύνδεση των προβλημάτων ενισχυτικής
μάθησης με τις διαδικασίες απόφασης Markov. Ο πράκτορας της ενισχυτικής μάθησης
συνήθως αγνοεί τόσο το σύνολο των πιθανών καταστάσεων του περιβάλλοντος, όσο
και τη συνάρτηση απόδοσης ανταμοιβής του. Ουσιαστικά, ο πράκτορας αντιμετωπίζει
τις διαδικασίες απόφασης Markov, δίχως πρότερη γνώση των πιθανοτήτων μετάβασης
P(𝑠', 𝑟|𝑠, 𝑎). Αντίθετα, τις μαθαίνει μέσα από την επαναλαμβανόμενη αλληλεπίδραση
με το περιβάλλον, αναλύοντας τα δεδομένα από την παρατήρηση των μεταβάσεων και
κυρίως των ανταμοιβών αυτών

The Markov property
says that the probability of the next state conditioned on the state the agent is currently in,
it's the same as the probability of the next state if all the previous states are imported to the
system. Specifically, with just having the current state, the characterization of the future
would be the same.
The future is independent of the past given the present.
The only thing that needs to be stored is the S, That's what the Markov property means
#### REWARD - ΑΝΤΑΜΟΙΒΗ

Σε κάθε δράση του
πράκτορα, το περιβάλλον του επιστρέφει μία τιμή, την ανταμοιβή. Πρόκειται για έναν
αριθμό, αρνητικό ή θετικό, μικρό ή μεγάλο. Αποκλειστικός σκοπός του πράκτορα είναι
να μεγιστοποιήσει την αθροιστική ανταμοιβή του σε βάθος χρόνου. Επί της ουσίας η
ανταμοιβή δείχνει πόσο καλή ή κακή είναι η κατάσταση που βρίσκεται ο πράκτορας

2 ειδων reward functions: αναλυτική (reward shaping)γενικα καλυτερο αλλα οσο πιοπεριπλοκο τοσο το χειροτερο ή περιεκτική 

στοχος max reward function

#### ΔΡΑΣΗ - ACTION

Δράση πράκτορα
Είναι το σύνολο των πράξεων που μπορεί να πραγματοποιήσει ο πράκτορας. Οι
δράσεις μπορεί να είναι είτε διακριτές είτε συνεχείς, αναλόγως εάν υπάρχει
πεπερασμένος διακριτός αριθμός κινήσεων ή όχι. Συχνά, όταν το πεδίο τιμών των
δράσεων είναι συνεχές, ακολουθείται μία διαδικασία διακριτοποίησης. Με αυτό τον
τρόπο, μειώνεται σε μεγάλο βαθμό η διαστασιμότητα του προβλήματος,
διευκολύνοντας τον πράκτορα να αποφασίσει τη βέλτιστη δράση του σε κάθε
κατάσταση. Φυσικά είναι πιθανή η δημιουργία νέων προβλημάτων λόγω έλλειψης
ακρίβειας, όταν το συνεχές πεδίο μετατρέπεται σε μικρό πλήθος πεπερασμένων
δράσεων. Επομένως, κατά τη διακριτοποίηση πρέπει να ληφθεί υπόψη η ζητούμενη
ακρίβεια

#### ΚΑΤΑΣΤΑΣΗ ΠΕΡΙΒΑΛΛΟΝΤΟΣ (STATE)

Αποτελεί το σύνολο των πληροφοριών που δέχεται ο πράκτορας μία ορισμένη
στιγμή από το περιβάλλον. Επί της ουσίας, κατάσταση είναι η παρατήρηση του
περιβάλλοντος από τη σκοπιά του πράκτορα. Οι παρατηρήσεις μπορεί να λαμβάνονται
είτε με ψηφιακή είτε με αναλογική μορφή - οπότε το περιβάλλον από τη σκοπιά του
πράκτορα είναι διακριτό ή ψηφιακό αντίστοιχα. Παρατηρήσεις σε μορφή εικόνων με
συγκεκριμένο αριθμό εικονοστοιχείων και χρωμάτων, δημιουργούν ένα πεπερασμένο
σύνολο καταστάσεων. Δεν ισχύει όμως το ίδιο, όταν οι παρατηρήσεις είναι σε μορφή
αναλογικών ενδείξεων, με τις εκάστοτε τιμές να ανήκουν σε ένα συνεχές διάστημα
τιμών. Ο αριθμός των διαφορετικών καταστάσεων του πράκτορα εν προκειμένω είναι
άπειρος. Όπως και στην περίπτωση των δράσεων με συνεχές πεδίο τιμών, για τη
μείωση της διαστασιμότητας του προβλήματος, ακολουθείται μια διαδικασία
διακριτοποίησης.


#### ΠΟΛΙΤΙΚΗ
a map from state to action, It could be deterministic or stochastic
Ο πράκτορας κάθε χρονική στιγμή t χρησιμοποιεί ένα σύνολο κανόνων για τη
λήψη απόφασης δράσης, που ονομάζεται πολιτική του πράκτορα [4]. Η πολιτική αυτή
μπορεί να είναι είτε ντετερμινιστική, είτε στοχαστική.
Στην πρώτη περίπτωση, συμβολίζεται ως 𝑎𝑡 = 𝜇(𝑠𝑡) και ο πράκτορας θα πρέπει
όσες φορές και αν κληθεί να αποφασίσει την επόμενη δράση του σε κάθε κατάσταση,
να επιλέγει ακριβώς την ίδια.
Αντίθετα στη δεύτερη περίπτωση, η πολιτική πραγματοποιεί κάποιου είδους
χαρτογράφηση, όπου αντιστοιχίζει καθεμία από τις δυνατές δράσεις κάθε κατάστασης
του πράκτορα, σε πιθανότητες. Η πολιτική συμβολίζεται ως 𝜋𝑡
( 𝑎 ∣ 𝑠 ) και είναι η
πιθανότητα να πράξει το α ενώ βρίσκεται στην κατάσταση s

τα δύο βασικότερα προβλήματα της ενισχυτικής
μάθησης. Το ένα ζήτημα που ταλανίζει την κοινότητα αφορά στον προσδιορισμό του
πλήθους των εμπειριών που θεωρούνται επαρκείς για την εκπαίδευση του πράκτορα.
Το δεύτερο δυσεπίλυτο πρόβλημα σχετίζεται με την εξερεύνηση του περιβάλλοντος
και πιο συγκεκριμένα του χρονικού ορίζοντα στον οποίο ένας πράκτορας θεωρείται ότι
έχει εξερευνήσει πλήρως το μοντέλο του προβλήματος.

Sub optimal policy, local minima

Exploration vs Exploitation

Δεδομένου όμως του
γεγονότος ότι ο πράκτορας ειδικά στα πρώτα αναγνωριστικά του βήματα στο
περιβάλλον, δεν έχει υιοθετήσει κάποια πολιτική δράσης, δημιουργείται η ανάγκη
εξερεύνησής του 

Εάν ο πράκτορας εξερευνήσει πολύ λίγο τον χώρο υπάρχει πάντα ο κίνδυνος να μην
αναπτύξει καλή εικόνα του μοντέλου και όταν αρχίσει να ακολουθεί την πολιτική να
οδηγείται σε λάθος κινήσεις. Εάν ο χρόνος εξερεύνησης είναι πολύ μεγάλος αυτό
αυξάνει το κόστος και τον χρόνο εκτέλεσης του πειράματος. 

ΕΙΚΟΝΑ ΕΞΕΡΕΥΝΗΣΗΣ

Reinforcement learning differs from supervised learning in not needing labelled
input/output pairs be presented, and in not needing sub-optimal actions to be explicitly
corrected. Instead, the focus is on finding a balance between exploration (of uncharted
territory) and exploitation (of current knowledge).

### Κατηγορίες αλγορίθμων {#sec:theory:reinforcement_learning:algorithm_types}

Εικόνα δέντρου OpenAI spinning up

Model Free vs Model Based

 The most successful methods in this domain remain
model-free algorithms (Hessel et al., 2018; Espeholt et al., 2018)

Η διάσπαση των αλγορίθμων στις δύο βασικές αυτές κατηγορίες οφείλεται στην
πρόσβαση ή μη του πράκτορα στο μοντέλο του περιβάλλοντος. Με τον όρο μοντέλο
περιβάλλοντος εννοείται η γνώση των πιθανοτήτων μετάβασης P(𝑠′, 𝑟|𝑠,a). Το μεγάλο
πλεονέκτημα της γνώσης του μοντέλου έγκειται στο γεγονός ότι ο πράκτορας γνωρίζει
όλες τις πιθανές δράσεις του και τα αποτελέσματα αυτών. Επομένως, δύναται να
χρησιμοποιήσει το μοντέλο για την ανάπτυξη μίας βέλτιστης πολιτικής. Δυστυχώς
όμως, στην πλειοψηφία των περιπτώσεων το μοντέλο του περιβάλλοντος είναι
άγνωστο για τον πράκτορα. Τη λύση φέρεται να δίνει η αλληλεπίδραση του πράκτορα
με το περιβάλλον. Με αυτόν τον τρόπο, ο πράκτορας χρησιμοποιεί την εμπειρία του
για την ανάπτυξη της πολιτικής του. Ένα από τα μεγαλύτερα προβλήματα του χώρου
αποτελεί η διαδικασία εξερεύνησης του περιβάλλοντος, δηλαδή πότε κρίνεται ότι η
πολιτική του πράκτορα είναι πλήρης. Το πρόβλημα αυτό ταλανίζει την επιστημονική
κοινότητα της ενισχυτικής μάθησης στα προβλήματα model free και αναλύεται
διεξοδικά παρακάτω. 

Q-learning: A reinforcement learning algorithm that is used to learn a policy for selecting actions in an environment. It is commonly used for tasks such as robotics and game playing.
Deep Q-networks (DQNs): A type of reinforcement learning algorithm that is based on deep learning. It is commonly used for tasks that involve complex environments and large state spaces.
Policy Gradient: A type of reinforcement learning algorithm that is used to learn a policy for selecting actions in an environment. It is commonly used for tasks that involve continuous state spaces.
Actor-Critic: A reinforcement learning algorithm that combines two separate neural networks: an actor and a critic. The actor is used to select actions, and the critic is used to evaluate the actions. It is commonly used for tasks that involve complex environments and large state spaces.

το μεγαλύτερο πλεονέκτημα
που έχουν οι αλγόριθμοι της κατηγορίας βελτιστοποίησης πολιτικής είναι ότι
βελτιστοποιούν άμεσα τον στόχο μας. Αυτό το γεγονός τους καθιστά σταθερούς και
αξιόπιστους

Υπάρχουν ορισμένοι αλγόριθμοι που προσπαθούν να
εκμεταλλευτούν στοιχεία και από τις δύο μεθόδους (Policy Gradient και Q Learning) όπως ο DDPG και ο SAC

RL TAXONOMY 

ON POLICY vs OFF POLICY

### Ο αλγόριθμος Q-learning {#sec:theory:reinforcement_learning:q_learning}

Αν και η απλή μέθοδος q learning μέθοδος είναι ιδιαίτερα ισχυρή, η βασικότερη
αδυναμία του είναι η έλλειψη δυνατότητας γενικοποίησης της γνώσης. Αυτό
υποδεικνύει ότι για καταστάσεις που ο πράκτορας της μέθοδος q-learning δεν έχει
συναντήσει ξανά, δεν μπορεί να αποφασίσει ανεξάρτητα από την ομοιότητα της
νέας αυτής κατάστασης με καταστάσεις που έχει ήδη συναντήσει.
Για την εξάλειψη αυτού του προβλήματος το DQN εισάγει την έννοια των
νευρωνικών δικτύων. Τα νευρωνικά δίκτυα αυτά χρησιμοποιούνται για την
εκτίμηση της συνάρτησης q-value.



𝝐-greedy policy

ΕΙΚΟΝΑ BAELDUNG

Σύμφωνα με αυτή τη συνάρτηση επιλογής δράσης, ένας πράκτορας επιλέγει με
πιθανότητα 1 − 𝜖 την επιτρεπόμενη για την τρέχουσα κατάσταση δράση που του
υπαγορεύει η πολιτική π που έχει αναπτύξει και με πιθανότητα 𝜖 τυχαία και ισοπίθανα
κάποια από όλες τις δυνατές δράσεις του, συμπεριλαμβανομένου εκείνης που του
επιβάλλει η πολιτική του
Η τιμή της πιθανότητας 𝜖 λαμβάνει τιμές στο διάστημα [0,1].

Η πιθανότητα 𝝐
Σύμφωνα με την προηγούμενο τύπο διαπιστώνει κανείς ότι θέτοντας την τιμή της
πιθανότητας μονάδα, ο πράκτορας δρα τελείως τυχαία, ενώ η μηδενική τιμή μετατρέπει
τον πράκτορα σε ‘άπληστο’. Ωστόσο, η τιμή 𝜖 δύναται να μεταβάλλεται με το πέρασμα
του χρόνου. Στα αρχικά βήματα, που η εξερεύνηση του περιβάλλοντος είναι θεμιτή,
ορίζεται ως αρχική τιμή η μονάδα. Αντίθετα, όσο περισσότερα επεισόδια εκτελούνται,
είναι δόκιμη η επιλογή κίνησης, καπηλεύοντας την πολιτική του πράκτορα. Ο ρυθμός
με τον οποίο μειώνεται η τιμή της πιθανότητας είναι συνήθως είτε γραμμικός είτε
εκθετικός και εξαρτάται τόσο από τον αριθμό των καταστάσεων όσο και από τον
αριθμό των δυνατών δράσεων αυτών. Σε κάθε περίπτωση θα πρέπει η τιμή 𝜖, να
ικανοποιεί τις δύο ακόλουθες συνθήκες σύμφωνα με την τεχνική της απληστίας σε
άπειρο χρονικό ορίζοντα:

- 𝜖𝑖 > 0 για όλα τα χρονικά βήματα i και
- 𝜖𝑖
 συγκλίνει προς το μηδέν όσο το χρονικό βήμα i τείνει στο άπειρο
(log𝑖→∞ 𝜖𝑖 = 0)
Θεωρητικά, σε άπειρο χρόνο εκπαίδευσης η πολιτική του πράκτορα θα αντιστοιχεί
στη βέλτιστη. Για αυτό το λόγο δε νοείται τότε εξερεύνηση στο περιβάλλον. Σε
πραγματικά προβλήματα ενισχυτικής μάθησης όμως, όπου ο πράκτορας εκπαιδεύεται
για ένα πεπερασμένο αριθμό βημάτων, η τιμή 𝜖 δεν τείνει ποτέ στο μηδέν. Αντίθετα,
τίθεται ένα κάτω όριο, 0.01 τις περισσότερες φορές, εξασφαλίζοντας με αυτόν τον
τρόπο ότι ο πράκτορας δε θα σταματήσει ποτέ την εξερεύνηση με δεδομένο ότι η
πολιτική του έχει πάντα περιθώρια βελτίωσης.
Ο καθορισμός αρχικής, τελικής τιμής πιθανότητας 𝜖 και ο ρυθμός μείωσης αυτής
αποτελούν ένα από τα πιο διαχρονικά προβλήματα του χώρου, αφού επηρεάζεται
εξολοκλήρου η εκπαιδευτική διαδικασία. 

ο παράγοντας δεν παραμένει σταθερός κατά τη πάροδο
των επεισοδίων αλλά σταδιακά μειώνεται
ο ο
πράκτορας δρα τυχαία και επομένως εξερευνά το περιβάλλον με πιθανότητα
epsilon η οποία στην αρχή των επεισοδίων ισούται με μονάδα. Κάθε κίνηση του
πράκτορα δηλαδή είναι τυχαία και συμβαίνει με μοναδικό σκοπό την εξερεύνηση
του περιβάλλοντος. Υπάρχει όμως ένας ρυθμός μείωσης που εφαρμόζεται στο
έψιλον τέτοιος ώστε μετά από έναν αριθμό επεισοδίων το έψιλον να μειωθεί και να
φτάσει ένα προκαθορισμένο κάτω όριο, συνήθως πολύ μικρό (στις περισσότερες
εφαρμογές χρησιμοποιείται το 0.01).
Δεν υπάρχει καθαρά ορισμένος κανόνας για το πόσο
γρήγορα θα πρέπει να μειώνεται το έψιλον ούτε ποιο πρέπει να είναι το κάτω όριο
του.

Learning rate decay is
empirically observed to help both optimization and generalization


While it’s manageable to create and use a q-table for simple environments, it’s quite difficult with
some real-life environments. The number of actions and states in a real-life environment can be
thousands, making it extremely inefficient to manage q-values in a table.
This is where it’s optimal to use neural networks to predict q-values