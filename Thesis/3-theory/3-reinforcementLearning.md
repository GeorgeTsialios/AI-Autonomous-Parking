## Ενισχυτική Μάθηση {#sec:theory:reinforcement_learning}

### Γενική επισκόπηση {#sec:theory:reinforcement_learning:overview}

#### Κεντρική ιδέα {.unnumbered}

Η Ενισχυτική Μάθηση (*Reinforcement Learning*) αποτελεί την κατηγορία της μηχανικής μάθησης, στην οποία ένας πράκτορας μαθαίνει από την εμπειρία του, καθώς αλληλεπιδράει με το περιβάλλον του. Ο πράκτορας ενισχυτικής μάθησης επιλέγει ελεύθερα ενέργειες και δέχεται ανάδραση για αυτές, υπό μορφή ανταμοιβής. Συγκεκριμένα, ο πράκτορας λαμβάνει θετική ανταμοιβή (επιβράβευση), όταν οι ενέργειες του οδηγούν σε επιθυμητά αποτελέσματα και αρνητική ανταμοιβή (τιμωρία), όταν οδηγούν σε ανεπιθύμητα αποτελέσματα. Έτσι, μέσω δοκιμών και λαθών (*trial and error*), ο πράκτορας μαθαίνει σταδιακά να παίρνει αποφάσεις που μεγιστοποιούν τις ανταμοιβές του. Η διαδικασία αυτή της εκπαίδευσης διαφέρει σημαντικά σε σχέση με τις δύο προηγούμενες κατηγορίες μηχανική μάθησης, αφού πλέον ο αλγόριθμος εκπαιδεύεται χωρίς κάποιο σταθερό σύνολο δεδομένων εισόδου. Αντίθετα, η εκπαίδευση στην ενισχυτική μάθηση είναι μία δυναμική διαδικασία, στην οποία ο πράκτορας εκτελεί ενέργειες και μεταβάλλει το περιβάλλον του. Επομένως, πρόκειται για μία ενεργή διαδικασία μάθησης, η οποία θυμίζει τον τρόπο με τον οποίο οι ζωντανοί οργανισμοί μαθαίνουν.

Πράγματι, ας αναλογιστούμε το παράδειγμα της εκπαίδευσης ενός σκύλου. Στο σενάριο μας, ο ιδιοκτήτης του σκύλου επιθυμεί να του μάθει την ενέργεια «Κάτσε».  Έτσι, ο ιδιοκτήτης του σκύλου δείχνει με το χέρι του το έδαφος και φωνάζει προς τον σκύλο «Κάτσε». Όσο ο σκύλος στέκεται όρθιος, ο ιδιοκτήτης του δίνει αρνητική ανταμοιβή (πχ του φωνάζει «κακό σκυλί», έχοντας τεντωμένο τον δείκτη του και δείχνοντας προς αυτό). Όταν ο σκύλος κάθεται, ο ιδιοκτήτης του δίνει θετική ανταμοιβή (πχ του χαϊδεύει το κεφάλι ή του δίνει μία λιχουδιά). Έτσι, ο σκύλος μαθαίνει σταδιακά, πως όταν παρατηρεί τον άνθρωπο στην κατάσταση «Κάτσε» (δηλ. χέρι προς το έδαφος και προφορική εντολή), η ενέργεια του να καθίσει στο έδαφος του προσφέρει θετική ανταμοιβή και για αυτό την επιλέγει.

Γίνεται πλέον σαφές, πως στόχος της ενισχυτικής μάθησης είναι η εκπαίδευση του πράκτορα, ώστε να προβαίνει σε ενέργειες που μεγιστοποιούν την ανταμοιβή του. Μάλιστα, μία σημαντική παρατήρηση είναι πως ο πράκτορας πρέπει να μάθει να μεγιστοποιεί τη συνολική ανταμοιβή του, δηλ. να σκέπτεται μακροπρόθεσμα. Συγκεκριμένα, ενδέχεται μία ενέργεια του πράκτορα να οδηγήσει σε αρνητική ανταμοιβή άμεσα, όμως σε βάθος χρόνου να βοηθήσει τον πράκτορα να πετύχει μεγάλη θετική ανταμοιβή. Για παράδειγμα, στο παιχνίδι του σκακιού υπάρχει η στρατηγική της θυσίας (*sacrifice*), στην οποία ο παίκτης επιλέγει να χάσει ένα κομμάτι (πχ την βασίλισσα του), για να πετύχει στο μέλλον κάτι μεγαλύτερης αξίας (πχ ρουά-ματ στον αντίπαλο βασιλιά).

#### Πλεονεκτήματα {.unnumbered}

Η εκπαίδευση με ενισχυτική μάθηση έχει ορισμένα σημαντικά πλεονεκτήματα σε σχέση με τις πιο παραδοσιακές μεθόδους μηχανικής μάθησης, τα οποία περιγράφονται παρακάτω:

- Δεν χρειάζεται μεγάλα **σύνολα δεδομένων**:  σε πολλά πεδία είναι δύσκολο να συλλεχθούν δεδομένα ή δεν υπάρχουν στον βαθμό που απαιτείται για μία αποδοτική εκπαίδευση επιβλεπόμενης μάθησης. Η ενισχυτική μάθηση αποτελεί μία εναλλακτική λύση, καθώς ο πράκτορας μαθαίνει μόνος του, χωρίς προηγούμενα δεδομένα. 
- Δεν απαιτεί **γνώση ειδικού** στο πεδίο εφαρμογής: στην επιβλεπόμενη μάθηση, τα δεδομένα αποτελούν ζεύγη εισόδου-επιθυμητής εξόδου. Για παράδειγμα, για την εκπαίδευση ενός μοντέλου να παίζει σκάκι, τα δεδομένα θα ήταν της μορφής: κατάσταση σκακιέρας-κίνηση που έκανε ο παίκτης. Έτσι, το μοντέλο θα μάθαινε να παίζει σκάκι με τον τρόπο που παίζουν οι παίκτες στα δεδομένα εκπαίδευσης. Άρα, θα έπρεπε αυτοί οι παίκτες να είναι ειδικοί στο παιχνίδι, προκειμένου να πετύχει το μοντέλο υψηλή απόδοση. Αντίθετα, στην ενισχυτική μάθηση, οι σχεδιαστές ενός συστήματος αρκεί να έχουν βασική γνώση του πεδίου εφαρμογής του, για να εκπαιδεύσουν επιτυχώς έναν πράκτορα. Για παράδειγμα, στην περίπτωση του σκακιού, αρκεί οι σχεδιαστές να γνωρίζουν τους κανόνες του παιχνιδιού, ώστε πχ να επιβραβεύουν τον πράκτορα όταν κερδίζει κομμάτια του αντιπάλου και να τον τιμωρούν όταν χάνει κομμάτια του.
- προσφέρει **καινοτόμες** λύσεις: η επιβλεπόμενη μάθηση ουσιαστικά μιμείται τα δεδομένα εκπαίδευσης. Ναι μεν μπορεί να επιτυχεί υψηλότερη απόδοση από τον άνθρωπο (πχ σκάκι), όμως δεν μπορεί να μάθει μία εντελώς νέα προσέγγιση για την επίλυση του προβλήματος. Αντίθετα, οι αλγόριθμοι ενισχυτικής μάθησης μπορούν να προτείνουν εντελώς νέες και επαναστατικές λύσεις, τις οποίες δεν είχε σκεφτεί ποτέ κάποιος άνθρωπος. Ένα τέτοιο αξιοσημείωτο παράδειγμα συνέβη στη νική του AlphaGo, ενός συστήματος ενισχυτικής μάθησης έναντι του παγκόσμιου πρωταθλητή στο Go, Lee Sedol. Κατά τη διάρκεια του αγώνα, το AlphaGo έκανε μία ασυνήθιστη κίνηση (κίνηση 37) που αρχικά θεωρήθηκε λάθος από τους ειδικούς στο παιχνίδι. Μάλιστα, υπολογίστηκε ότι η πιθανότητα ένας άνθρωπος να προέβαινε σε αυτή την κίνηση, στη συγκεκριμένη θέση, ήταν ίση με 0.0001%. Ωστόσο, η κίνηση αποδείχθηκε εν τέλει δημιουργική και καθοριστική για τη νίκη του AlphaGo [@wiredMovesAlphaGo].
- Είναι κατάλληλη για περιβάλλοντα **σειριακής λήψης αποφάσεων**: η ενισχυτική μάθηση μαθαίνει στον πράκτορα να μεγιστοποιεί τη συνολική ανταμοιβή σε βάθος χρόνου. Έτσι, είναι κατάλληλη για σενάρια όπου οι αποφάσεις είναι σειριακές και το αποτέλεσμα μίας ενέργειας επηρεάζει τις μελλοντικές αποφάσεις. Αντίθετα, στην επιβλεπόμενη μάθηση, το μοντέλο πραγματοποιεί ανεξάρτητες προβλέψεις σε κάθε βήμα, χωρίς να λαμβάνει υπόψη το πως αυτές θα επηρεάσουν τις μελλοντικές αποφάσεις.

Ωστόσο, η ενισχυτική μάθηση έχει και μειονεκτήματα, τα οποία δυσκολεύουν την επιτυχή εφαρμογή της. Πολλά από αυτά τα προβλήματα προέκυψαν και κατά τη διάρκεια της εκπαίδευσης του πράκτορα αυτόματης στάθμευσης. Για αυτό, περιγράφονται στο Κεφάλαιο --------- της εκπαίδευσης, στην Ενότητα --------.

#### Εφαρμογές {.unnumbered}

Η ενισχυτική μάθηση αποτελεί ουσιαστικά την επιστημή της λήψης αποφάσεων κι ως εκ τούτου, έχει εφαρμογές σε πολλά πεδία. Στο πεδίο των χρηματοοικονομικών, η εταιρία Goldman Sachs έχει ξεκινήσει τη χρήση της ενισχυτικής μάθησης στις πλατφόρμες συναλλαγών της για βελτιώσει την απόδοση των επενδυτικών στρατηγικών της [@efinancialcareersGoldmanSachs]. Στον τομέα της βιοτεχνολογίας, η εταιρία Atomwise χρησιμοποιεί την ενισχυτική μάθηση στην πλατφόρμα της AtomNet, που χρησιμοποιείεται για την
ανακάλυψη νέων φαρμάκων [@atomwiseAtomNetTechnology]. Στον χώρο της ρομποτικής, η εταιρία Boston Dynamics έχει ενσωματώσει την ενισχυτική μάθηση στα συστήματα ελέγχου του τετράποδου ρομπότ της, Spot και έχει βελτιώσει την αυτόνομη πλοήγηση του σε πολύπλοκα περιβάλλοντα [@bostondynamicsStartingRight].

Ωστόσο, το επικρατέστερο πεδίο εφαρμογής των αλγορίθμων ενισχυτικής μάθησης είναι τα παιχνίδια. Αυτό οφείλεται στο γεγονός ότι τα παιχνίδια αποτελούν έναν ασφαλές χώρο εκπαίδευσης πρακτόρων και ανάπτυξης αλγορίθμων, προτού αυτοί εφαρμοστούν σε προβλήματα του πραγματικού κόσμου. Μάλιστα, σε περιβάλλοντα προσομοιώσεων δεν υπάρχουν χρονικοί περιορισμοί, οπότε ο πράκτορας μπορεί να εκπαιδεύεται ασταμάτητα για μεγάλα χρονικά διαστήματα, κάτι που συχνά είναι απαραίτητο για την επιτυχία της ενισχυτικής μάθησης. Επίσης, τα παιχνίδια απαιτούν σε σημαντικό βαθμό νοητικές ικανότητες από τον παίκτη, κι έτσι αποτελούν μία καλή πλατφόρμα εκπαίδευσης αλγορίθμων τεχνητής νοημοσύνης.

Οι πρώτες απόπειρες έγιναν το 1959 από τον Arthur Samuel, ο οποίος ανέπτυξε ένα πρόγραμμα που έπαιζε το παιχνίδι της ντάμας [@5389202]. Στη συνέχεια, το 1992, ο Gerald Tesauro ανέπτυξε τον αλγόριθμο TD-Gammon με τη βοήθεια νευρωνικών δικτύων, ο οποίος έπαιζε τάβλι και κατάφερε να φτάσει σε επίπεδο ανάλογο των τριών κορυφαίων παικτών στον κόσμο[@6795979].

Ωστόσο, μέχρι και τη δεκαετία του 2010, υπήρχαν σημαντικοί περιορισμοί στην εφαρμογή της ενισχυτικής μάθησης σε πολύπλοκα προβλήματα ή προβλήματα μεγάλης διαστασιμότητας. Παρόλο που είχαν ήδη αναπτυχθεί αλγόριθμοι επίλυσης
τέτοιων προβλημάτων, η μικρή διαθέσιμη υπολογιστική ισχύ της εποχής δεν επέτρεπε την εκπαίδευση των μοντέλων σε λογικά χρονικά διαστήματα. Το πρόβλημα αυτό υπερκεράστηκε από την τεχνολογική ανάπτυξη στον τομέα του υλικού (*hardware*) με χαρακτηριστικό παράδειγμα την παραλληλοποίηση των υπολογισμών στις μονάδες επεξεργασίας γραφικών (*GPU*), οι οποίες επιταχύνουν σημαντικά τη διαδικασία της εκπαίδευσης. Έτσι, ξεκίνησε η εποχή της βαθιάς ενισχυτικής μάθησης και άρχισαν να φαίνονται για πρώτη φορά οι πραγματικές δυνατότητες των τεχνητών νευρωνικών δικτύων. Ορισμένες επιτυχίες-ορόσημα της βαθιάς ενισχυτικής μάθησης σε περιβάλλοντα παιχνιδιών αναφέρονται με χρονολογική σειρά παρακάτω:

- **2012**: η εταιρία DeepMind της Google ανέπτυξε το πρώτο σύγχρονο σύστημα βαθιάς ενισχυτικής μάθησης από, ο αλγόριθμος DQN (*Deep Q-Network*). Ο αλγόριθμος αυτός, εκπαιδεύτηκε ξεχωριστά στα 49 παιχνίδια της πλατφόρμας Atari2600, δεχόμενος ως είσοδο μόνο τα pixels της οθόνης και το σκορ του παιχνιδιού και κατάφερε να φτάσει σε επίπεδο συγκρίσιμο με αυτό ενός επαγγελματία δοκιμαστή παιχνιδιών [@Mnih2015HumanlevelCT]. 
- **2016**: η εταιρία DeepMind παρουσίασε τον αλγόριθμο AlphaGo, για το παιχνίδι στρατηγικής Go [@deepmindAlphaGo]. Το Go, είναι ένα παιχνίδι πολύ πιο πολύπλοκο από το σκάκι, έχοντας σημαντικά μεγαλύτερο χώρο καταστάσεων κι έτσι, αποτελούσε πρόκληση για την τεχνητή νοημοσύνη. Οι παραδοσιακοί αλγόριθμοι μηχανικής μάθησης δυσκολεύονταν να αξιολογήσουν όλες τις πιθανές κινήσεις, να αναπτύξουν ανθρώπινη δημιουργικότητα και συνολικά, να ανταγωνιστούν τους ανθρώπους. Όμως, ο αλγόριθμος AlphaGo κατάφερε να νικήσει σε μία σειρά παιχνιδιών τον θρυλικό παγκόσμιο πρωταθλητή στο Go, Lee Sedol. Αυτή η νίκη αποτέλεσε απόδειξη πως τα συστήματα βαθιάς ενισχυτικής μάθησης μπορούν να μάθουν να λύνουν τα πιο δύσκολα προβλήματα σε υψηλά περίπλοκα περιβάλλοντα.
- **2017**: η εταιρία OpenAI ανέπτυξε μία παραλλαγή του αλγορίθμου Proximal Policy Optimization, τον αλγόριθμο OpenAI Five για την εκπαίδευση πρακτόρων στο παιχνίδι Dota2, ένα πολυπρακτορικό παιχνίδι (παίζεται από 2 ομάδες των 5 ατόμων), αβέβαιης πληροφορίας και με ιδιαίτερα πολύπλοκες καταστάσεις και ενέργειες. Οι πράκτορες του OpenAI Five εκπαιδεύτηκαν για 10 μήνες και μέσω self-play, δηλαδή παίζοντας μεταξύ τους, και το 2019 κατάφεραν να νικήσουν τους τρέχοντες παγκόσμιους πρωταθλητές στο παιχνίδι [@OpenAIFive]. 
- **2020**: η εταιρία DeepMind παρουσίασε τον Agent57, μία βελτιωμένη έκδοση του αλγορίθμου DQN, ο οποίος χρησιμοποιεί έναν μετα-ελεγκτή για την προσαρμογή της εξερεύνησης και τη ρύθμιση της μακροπρόθεσμης έναντι της βραχυπρόθεσμης συμπεριφοράς του πράκτορα [@deepmindAgent57Outperforming]. Ο Agent57 εκπαιδεύτηκε στα 57 παιχνίδια της πλατφόρμας Atari2600 και κατάφερε να ξεπεράσει τις επιδόσεις επαγγελματιών παικτών σε κάθε ένα από αυτά. Το σημαντικό σε αυτή την επιτυχία, είναι πως ήταν η πρώτη φορά που ένας αλγόριθμος κατάφερε κάτι ανάλογο στο σύνολο των 57 παιχνιδιών, τα οποία διακρίνονται για την πολυπλοκότητα και τη διαφορετικότητά τους.

### Βασικές Έννοιες και Ορολογία {#sec:theory:reinforcement_learning:concepts}

Έχοντας ήδη περιγράψει την γενικότερη ιδέα της ενισχυτικής μάθησης, στην ενότητα αυτή θα μελετήσουμε το πεδίο σε μεγαλύτερο βάθος, παρουσιάζοντας τις βασικές έννοιες και την ορολογία που χρησιμοποιείται. Αυτό θα βοηθήσει στην καλύτερη κατανόηση των βασικών αρχών της εκπαίδευσης πρακτόρων, τη μεθοδολογία που ακολουθείται, καθώς και τους στόχους αλλά και τα προβλήματα που προκύπτουν κατά την εκπαίδευση.

#### Κύκλος Ενισχυτικής Μάθησης {.unnumbered}

Ο κύκλος της ενισχυτικής μάθησης περιλαμβάνει την εξής διαδικασία: ο πράκτορας επιλέγει μία ενέργεια και την εκτελεί. Έτσι, μεταβάλει το περιβάλλον, δηλ. αυτό μεταβαίνει σε μία νέα κατάσταση. Έπειτα, ο πράκτορας δέχεται ως είσοδο τη νέα κατάσταση του περιβάλλοντος καθώς και την ανταμοιβή που προέκυψε από την ενέργειά του. Με βάση αυτές τις πληροφορίες, ο πράκτορας επιλέγει την επόμενη ενέργεια που θα εκτελέσει. Ο κύκλος αυτός φαίνεται και παραστατικά στην *Εικόνα @fig:theory:reinforcement_learning:rl_cycle*.

![Κύκλος Ενισχυτικής Μάθησης [@mediumReinforcementLearning].](3-theory/figures/RLCycle.png){#fig:theory:reinforcement_learning:rl_cycle width=70%}

Μία επανάληψη της παραπάνω διαδικασίας ονομάζεται και βήμα (*step*) της εκπαίδευσης. Συνηθίζεται η εκπαίδευση ενός πράκτορα να χαρακτηρίζεται από το πλήθος των βημάτων στα οποία ο πράκτορας έχει εκπαιδευτεί. Στην πράξη, χρειάζονται μερικά εκατομμύρια βήματα εκπαίδευσης προκειμένου ο πράκτορας να φτάσει σε επιθυμητή απόδοση.

Επομένως, γίνεται πλέον κατανοητό πως κάθε πρόβλημα ενισχυτικής μάθησης αποτελείται από δύο βασικές οντότητες: το *περιβάλλον* και τον *πράκτορα*, καθώς επίσης και τρία κανάλια επικοινωνίας αυτών: της *ανταμοιβής*, των *καταστάσεων* και των *ενεργειών*. Ο όρος *πράκτορας* έχει ήδη αναλυθεί στην υποενότητα @sec:theory:artificial_intelligence:agents_models, ενώ οι υπόλοιποι όροι περιγράφονται λεπτομερώς στις επόμενες υποενότητες.

####  Περιβάλλον {.unnumbered}

Το περιβάλλον αποτελεί τον κόσμο, στον οποίο ο πράκτορας εκτελεί ενέργειες. Ένα περιβάλλον ενισχυτικής μάθησης πρέπει να ικανοποιεί την Μαρκοβιανή ιδιότητα. Προκειμένου να γίνει κατανοητή αυτή η ιδιότητα, πρέπει πρώτα να γίνει αναφορά στις Διαδικασίες Αποφάσεων Μαρκόβ.

**Διαδικασίες Αποφάσεων Μαρκόβ**

Οι διαδικασίες αποφάσεων Μαρκόβ (Markov Decision Processes - MDPs) αποτελούν ένα μαθηματικό πλαίσιο που χρησιμοποιείται για την περιγραφή ενός περιβάλλοντος σε προβλήματα επίτευξης ενός στόχου. Παρέχει μία μοντελοποίηση της λήψης αποφάσεων, σε καταστάσεις όπου τα αποτελέσματα είναι μερικώς τυχαία και μερικώς υπό τον έλεγχο του λήπτη αποφάσεων (δηλ. του πράκτορα). Αποτελούν επέκταση των αλυσίδων Μαρκόβ, με τη διαφορά ότι προσθέτουν ενέργειες και ανταμοιβές. Έτσι, τα MDPs χρησιμοποιούνται για να περιγράψουν ένα περιβάλλον στο οποίο θέλουμε να εφαρμόσουμε ενισχυτική μάθηση.

Περιγράφονται από την εξής διαδικασία: Ο πράκτορας αλληλεπιδρά με το περιβάλλον σε μία ακολουθία χρονικών στιγμών $t=0,1,2,...$. Σε κάθε χρονική στιγμή $t$, το περιβάλλον βρίσκεται σε μία κατάσταση $s_t \in S$, όπου $S$ είναι το σύνολο των καταστάσεων του περιβάλλοντος. Η κατάσταση αυτή δίνεται ως είσοδος στον πράκτορα, ο οποίος στη συνέχεια επιλέγει μία ενέργεια $a_t \in A$, όπου $A$ είναι το σύνολο των ενεργειών που μπορεί να εκτελέσει. Το περιβάλλον ανταποκρίνεται στην ενέργεια του πράκτορα, μεταβαίνοντας σε μία νέα κατάσταση $s_{t+1}$ και επιστρέφοντας στον πράκτορα μία ανταμοιβή $r_{t+1} \in R$, όπου $R$ είναι το σύνολο των πιθανών ανταμοιβών. Με τον τρόπο αυτό, μία διαδικασία απόφασης Μαρκόβ ορίζεται ως μία λίστα 4 στοιχείων $(S, A, P, R)$, όπου:

- $S$ είναι το σύνολο των καταστάσεων του περιβάλλοντος (καλείται και χώρος καταστάσεων),
- $A$ είναι το σύνολο των ενεργειών που μπορεί να εκτελέσει ο πράκτορας (καλείται και χώρος ενεργειών),
- $P$ είναι η συνάρτηση πιθανότητας μετάβασης, όπου $P(s_{t+1}|s_{t}, a_{t})$ είναι η πιθανότητα να μεταβεί το περιβάλλον στην κατάσταση $s_{t+1}$ μετά την εκτέλεση της ενέργειας $a_{t}$ στην κατάσταση $s_{t}$,
- $R$ είναι η συνάρτηση ανταμοιβής, ενώ το $R_{a_{t}}(s_{t}, s_{t+1})$ είναι η ανταμοιβή που λαμβάνει ο πράκτορας όταν μεταβεί από την κατάσταση $s_{t}$ στην κατάσταση $s_{t+1}$ μετά την εκτέλεση της ενέργειας $a_{t}$.

Ένα παράδειγμα μίας Διαδικασίας Απόφασης Μαρκόβ φαίνεται με τη χρήση ενός γράφου στην *Εικόνα @fig:theory:reinforcement_learning:mdp_example*. Οι πράσινοι κόμβοι αντιστοιχούν στις καταστάσεις, οι πορτοκαλί κόμβοι στις ενέργειες, και τα βάρη των ακμών στις πιθανότητες μετάβασης. Επίσης, τα πορτοκαλί βέλη αντιστοιχούν στις ανταμοιβές που λαμβάνει ο πράκτορας μετά την εκτέλεση της ενέργειας.

![Παράδειγμα Διαδικασίας Απόφασης Μαρκόβ [@wikipediaMarkovDecision]](3-theory/figures/Markov_Decision_Process.png){#fig:theory:reinforcement_learning:mdp_example width=70%}

**Μαρκοβιανή Ιδιότητα**

Η μαρκοβιανή ιδιότητα αναφέρεται στην ιδιότητα «αμνησίας» μίας στοχαστικής διαδικασίας, δηλαδή στο χαρακτηριστικό ότι η μελλοντική εξέλιξή της είναι ανεξάρτητη από το παρελθόν της. Συγκεκριμένα, δηλώνει ότι η επόμενη κατάσταση $s_{t+1}$ εξαρτάται μόνο από την τρέχουσα κατάσταση $s_{t}$ και την ενέργεια $a_{t}$ που επιλέγει ο πράκτορας. Δοθέντος αυτών των δύο, είναι ανεξάρτητη από όλες τις προηγούμενες καταστάσεις και ενέργειες. Αυτό περιγράφεται μαθηματικά από την εξίσωση \ref{eq:theory:reinforcement_learning:markov}:
\begin{equation}
P(s_{t+1}|s_{t}, a_{t}, s_{t-1}, a_{t-1}, ..., s_{0}, a_{0}) = P(s_{t+1}|s_{t}, a_{t})
\label{eq:theory:reinforcement_learning:markov}
\end{equation}

Η μαρκοβιανή ιδιότητα είναι κρίσιμη για την επιτυχή εφαρμογή της ενισχυτικής μάθησης, καθώς εξασφαλίζει ότι ο πράκτορας μπορεί να λάβει αποφάσεις με βάση μόνο την τρέχουσα κατάσταση του περιβάλλοντος, χωρίς να χρειάζεται να αποθηκεύσει όλες τις προηγούμενες καταστάσεις.

Πέραν όμως της μαρκοβιανής ιδιότητας, ένα περιβάλλον ενισχυτικής μάθησης περιγράφεται κι από άλλες ιδιότητες, οι οποίες οφείλονται στη διαφορετική φύση του κάθε προβλήματος και επηρεάζουν τη σημαντικά τη σχεδίαση του πράκτορα. Οι ιδιότητες αυτές είναι οι εξής:

**Πλήρως ή μερικώς παρατηρήσιμο** 

Το περιβάλλον είναι πλήρως παρατηρήσιμο όταν ο πράκτορας έχει πρόσβαση στην πλήρη κατάστασή του κάθε χρονική στιγμή. Σε αντίθεση, όταν ο πράκτορας έχει πρόσβαση μόνο σε μέρος της κατάστασης του περιβάλλοντος, τότε το περιβάλλον είναι μερικώς παρατηρήσιμο. Ένα παράδειγμα ενός πλήρους παρατηρήσιμου περιβάλλοντος είναι το παιχνίδι του σκακιού, όπου οι παίκτες γνωρίζουν τη θέση όλων των πιονιών και των πιονιών του αντιπάλου. Αντίθετα, ένα παράδειγμα μερικώς παρατηρήσιμου περιβάλλοντος είναι το παιχνίδι του πόκερ, όπου οι παίκτες δεν γνωρίζουν τις κάρτες των αντιπάλων τους.

**Αιτιοκρατικό ή στοχαστικό** 

Αιτιοκρατικό χαρακτηρίζεται το περιβάλλον στο οποίο η επόμενη κατάστασή του προσδιορίζεται με ακρίβεια από την τρέχουσα κατάστασή του και την ενέργεια του πράκτορα. Στην περίπτωση αυτιοκρατικού περιβάλλοντος, όταν ο πράκτορας βρίσκεται σε συγκεκριμένη κατάσταση και εκτελεί μία συγκεκριμένη ενέργεια, θα προκύπτει πάντα η ίδια ανταμοιβή και η ίδια επόμενη κατάσταση. Για παράδειγμα, παιχνίδια όπως το σκάκι είναι αιτιοκρατικά, καθώς η κίνηση ενός πιονιού σε συγκεκριμένη θέση θα οδηγήσει πάντα σε μία συγκεκριμένη κατάσταση του παιχνιδιού.

Όταν δεν υπάρχει αυτή η νομοτελειακή σχέση μεταξύ καταστάσεων και ενεργειών, αλλά υπάρχει και ένας βαθμός τυχαιότητας, το περιβάλλον είναι στοχαστικό. Τότε, ακόμα και όταν ο πράκτορας βρίσκεται σε συγκεκριμένη κατάσταση και εκτελεί μία συγκεκριμένη ενέργεια, δεν μπορεί να είναι βέβαιος για την ανταμοιβή που θα λάβει ή την επόμενη κατάσταση του περιβάλλοντος. Ένα παράδειγμα στοχαστικού περιβάλλοντος είναι το παιχνίδι του πόκερ, όπου οι κάρτες που θα αποκαλυφθούν στην επόμενη φάση του παιχνιδιού είναι τυχαίες.

**Διακριτό ή συνεχές**

Ένα περιβάλλον μπορεί να είναι διακριτό ή συνεχές ανάλογα με το πλήθος των δυνατών καταστάσεων του. Όταν το πλήθος αυτό είναι πεπερασμένο, τότε το περιβάλλον είναι διακριτό. Για παράδειγμα, η τρίλιζα είναι ένα παιχνίδι με διακριτό περιβάλλον, καθώς οι δυνατές καταστάσεις του παιχνιδιού είναι πεπερασμένες και μάλιστα, λίγες.

Αντίθετα, όταν το πλήθος των δυνατών καταστάσεων είναι άπειρο, τότε το περιβάλλον είναι συνεχές. Ένα παράδειγμα συνεχούς περιβάλλοντος είναι το περιβάλλον της πλοήγησης ενός ρομπότ, όπου η θέση, η ταχύτητα κι ο προσανατολισμός του ρομπότ μπορούν να λάβουν οποιαδήποτε τιμή σε ένα συνεχές πεδίο τιμών.

**Μονοπρακτορικό ή πολυπρακτορικό**

Ανάλογα με το πλήθος των πρακτόρων που συμμετέχουν στο περιβάλλον, αυτό χαρακτηρίζεται ως μονοπρακτορικό ή πολυπρακτορικό. Τα παζλ αποτελούν παραδείγματα μονοπρακτορικού περιβάλλοντος, ενώ αντίθετα αθλήματα όπως το ποδόσφαιρο είναι πολυπρακτορικά, καθώς συμμετέχουν πολλοί πράκτορες, οι οποίοι συνεργάζονται ή ανταγωνίζονται μεταξύ τους.

**Επεισοδιακό ή ακολουθιακό**

Το περιβάλλον μπορεί να είναι επεισοδιακό ή ακολουθιακό ανάλογα με τον τρόπο με τον οποίο ο πράκτορας αλληλεπιδρά με αυτό. Σε επεισοδιακά περιβάλλοντα, η αλληλεπίδραση μεταξύ πράκτορα χωρίζεται σε επεισόδια, τα οποία τερματίζονται μετά από την επίτευξη του επιθυμητού στόχου ή μετά από ένα συγκεκριμένο αριθμό βημάτων . Για παράδειγμα το σκάκι μπορεί να θεωρηθεί επεισοδιακό περιβάλλον, καθώς κάθε παιχνίδι αποτελεί ένα επεισόδιο, το οποίο τελειώνει είτε μέσω ρουά ματ ή όταν τελειώσει ο χρόνος ενός παίκτη.

Αντίθετα, σε ακολουθιακά περιβάλλοντα, η αλληλεπίδραση μεταξύ πράκτορα και περιβάλλοντος δεν έχει φυσικό τέλος και ο πράκτορας συνεχίζει να αλληλεπιδρά με το περιβάλλον για αόριστο χρονικό διάστημα. Ένα παράδειγμα ακολουθιακού περιβάλλοντος είναι η πλοήγηση ενός ρομπότ σε έναν άγνωστο χώρο.

**Με αραίες ή πυκνές ανταμοιβές**

Η διάκριση εδώ αφορά τον ρυθμό με τον οποίο ο πράκτορας λαμβάνει ανταμοιβές από το περιβάλλον. Σε περιβάλλοντα με αραίες ανταμοιβές, ο πράκτορας λαμβάνει ανταμοιβή σπάνια, μόνο όταν επιτύχει έναν συγκεκριμένο στόχο. Για παράδειγμα στην τρίλιζα, ο πράκτορας επιβραβεύεται μόνο όταν καταφέρνει να κερδίσει το παιχνίδι.

Αντίθετα, σε περιβάλλοντα με πυκνές ανταμοιβές, ο πράκτορας λαμβάνει ανταμοιβή πιο συχνά, προσφέροντας του έτσι πιο άμεση ανατροφοδότηση για την ποιότητα των ενεργειών του. Ένα παράδειγμα πυκνών ανταμοιβών είναι το κλασικό περιβάλλον εκπαίδευσης CartPole, οπου ο πράκτορας προσπαθεί να διατηρήσει σε ισορροπία σε ένα όρθιο κοντάρι. Στο περιβάλλον αυτό, όσο το κοντάρι παραμένει σε ισορροπία, ο πράκτορας λαμβάνει συνεχώς ανταμοιβές.

#### Ανταμοιβή {.unnumbered}

Σε κάθε ενέργεια του πράκτορα, το περιβάλλον του επιστρέφει μία τιμή, την ανταμοιβή. Πρόκειται για έναν αριθμό, ο οποίος μπορεί να είναι είτε αρνητικός (ο πράκτορας «τιμωρείται») ή θετικός (ο πράκτορας «επιβραβεύεται»). Ουσιαστικά, η ανταμοιβή δείχνει πόσο καλή ή κακή είναι η κατάσταση που βρίσκεται ο πράκτορας. Σκοπός του πράκτορα είναι να μεγιστοποιήσει την αθροιστική ανταμοιβή του σε βάθος χρόνου.

Η συνάρτηση η οποία υπολογίζει σε κάθε βήμα του πράκτορα την ανταμοιβή που λαμβάνει, ονομάζεται Συνάρτηση Ανταμοιβής (*Reward Function*). Η συνάρτηση ανταμοιβής μπορεί να είναι απλή, με τη λογική της επιβράβευσης ή τιμωρίας στο τέλος του παιχνιδιού, ή πιο πολύπλοκη, με περισσότερες επιβραβεύσεις και τιμωρίες, ώστε να οδηγήσει τον πράκτορα στην επιθυμητή συμπεριφορά. Για παράδειγμα, ας θεωρήσουμε το παιχνίδι της πλοήγησης σε έναν λαβύρινθο, όπου στόχος του πράκτορα είναι να βρει την έξοδο του. Μία απλή συνάρτηση ανταμοιβής θα μπορούσε να είναι η ανταμοιβή +1 όταν βρει την έξοδο και -1 εφόσον τελειώσει ο διαθέσιμος χρόνος του πράκτορα. Σε αυτήν την περίπτωση, η συνάρτηση ανταμοιβής είναι αραιή (*Sparse Rewards*). Αντίθετα, μία πιο πολύπλοκη συνάρτηση ανταμοιβής θα μπορούσε να είναι η ανταμοιβή +1000 όταν βρει την έξοδο, +10 όταν πλησιάζει σε αυτήν,  -10 όταν απομακρύνεται από αυτήν και -1000 εφόσον τελειώσει ο διαθέσιμος χρόνος του πράκτορα. Η δεύτερη, αναλυτικότερη προσέγγιση ονομάζεται Διαμόρφωση Ανταμοιβής (*Reward Shaping*) και συνήθως οδηγεί σε καλύτερα ή ταχύτερα αποτελέσματα, καθώς παρέχει περισσότερη ανάδραση στον πράκτορα, καθοδηγώντας τον προς τον τελικό στόχο. Ωστόσο, χρειάζεται περισσότερη προσοχή από τον σχεδιαστή του συστήματος, προκειμένου να αποφευχθούν μη επιθυμητές συμπεριφορές του πράκτορα. Για αυτό, κάποιες φορές είναι προτιμότερο να διατηρηθεί απλή η συνάρτηση ανταμοιβής, ακόμα κι αν αυτό σημαίνει μεγαλύτερο χρόνο εκπαίδευσης.

Η ορθή σχεδίαση μίας συνάρτησης ανταμοιβής είναι είναι καθοριστική για την επιτυχία του πράκτορα στην εκπαίδευση. Ωστόσο, η διαδικασία αυτή αποδεικνύεται στην πράξη δύσκολη, καθώς περιέχει αρκετές προκλήσεις και απαιτεί προσεκτική ανάλυση του προβλήματος. Περισσότερες λεπτομέρειες σχετικά με τα προβλήματα που μπορεί να προκύψουν από τη σχεδίαση της συνάρτησης ανταμοιβής και τις πρακτικές που χρησιμοποιούνται για την αντιμετώπισή τους, δίνονται στο Κεφάλαιο 5, στις *Ενότητες ---------* και *----------* αντίστοιχα.

#### Κατάσταση {.unnumbered}

Η κατάσταση (*state*) αποτελεί το σύνολο των πληροφοριών που δέχεται ο πράκτορας από το περιβάλλον, σε μία ορισμένη χρονική
στιγμή. Ουσιαστικά, πρόκειται για την κωδικοποίηση της οπτικής αναπαράστασης του περιβάλλοντος, σε μορφή πληροφοριών που μπορούν να δοθούν ως είσοδο στον πράκτορα. Για παράδειγμα, όταν οι άνθρωποι παίζουμε ένα παιχνίδι, το οπτικό κανάλι είναι αυτό που λαμβάνει τις περισσότερες πληροφορίες για την κατάσταση του παιχνιδιού. Σε ένα επιτραπέζιο παιχνίδι όπως η τρίλιζα, οι άνθρωποι βλέπουν την εικόνα του ταμπλό και με βάση αυτήν παίρνουν αποφάσεις. Όμως ένας πράκτορας χρειάζεται δεδομένα σε μορφή αριθμών ως είσοδο για το νευρωνικό του δίκτυο. Συνεπώς, η κατάσταση πρέπει να κωδικοποιηθεί σε ένα διάνυσμα αριθμών. Για την περίπτωση της τρίλιζας θα μπορούσαμε να είχαμε ένα διάνυσμα 9 θέσεων, όπου κάθε μία συμβολίζει ένα τετράγωνο του ταμπλό και μπορεί να πάρει 3 διακριτές τιμές: 1 αν στη θέση υπάρχει Ο, -1 αν υπάρχει Χ και 0 αν είναι άδεια. Έτσι, το διάνυσμα [0, 0, 0, 0, -1, 0, 0, 0, 0] αντιστοιχεί σε ένα ταμπλό με Χ στη μεσαία θέση.

Στο παραπάνω παράδειγμα, το κάθε στοιχείο του διανύσματος κατάστασης μπορούσε να πάρει συγκεκριμένες, διακριτές τιμές. Έτσι, ο χώρος καταστάσεων που δημιουργείται είναι διακριτός (*Discrete state space*). Υπάρχουν όμως περιπτώσεις, στις οποίες το κάθε στοιχείο του διανύσματος κατάστασης μπορεί να πάρει οποιαδήποτε τιμή σε ένα συνεχές διάστημα. Σε αυτές τις περιπτώσεις, ο χώρος καταστάσεων είναι συνεχής (*Continuous state space*) και ο αριθμός των διαφορετικών καταστάσεων του πράκτορα είναι άπειρος. Για παράδειγμα, στην πλοήγηση ενός ρομπότ σε έναν άγνωστο χώρο, η κατάσταση του ρομπότ μπορεί να περιγραφεί από τη θέση του στο χώρο, την ταχύτητά του και τον προσανατολισμό του. Κάθε μία από αυτές τις παραμέτρους μπορεί να πάρει οποιαδήποτε τιμή σε ένα συνεχές διάστημα κι έτσι ο χώρος καταστάσεων είναι συνεχής. Ο χαρακτηρισμός του χώρου καταστάσεων ως διακριτός ή συνεχής είναι καθοριστικής σημασίας, διότι παίζει σημαντικό ρόλο στην επιλογή του αλγορίθμου εκπαίδευσης του πράκτορα.

Γενικά, είναι προτιμότερο οι πληροφορίες που δίνονται στον πράκτορα να περιορίζονται μόνο στις χρήσιμες σε αυτόν για την επίτευξη του στόχου του. Αυτό έχει ως αποτέλεσμα τη μείωση της διαστασιμότητας του προβλήματος, κάτι που επιταχύνει την εκπαίδευση του πράκτορα. Ωστόσο, η περιορισμένη πληροφορία μπορεί να οδηγήσει σε ανεπαρκή εκπαίδευση του πράκτορα, καθώς αυτός δεν έχει την πλήρη εικόνα του περιβάλλοντος και μπορεί να χάσει σημαντικές πληροφορίες. Επομένως, η σχεδίαση της κατάστασης πρέπει να γίνει με προσοχή, ώστε να εξασφαλιστεί η ισορροπία μεταξύ της πληροφορίας και της διαστασιμότητας του προβλήματος.

#### Ενέργεια {.unnumbered}

Ο όρος «ενέργεια» αντιπροσωπεύει μία πράξη που μπορεί να πραγματοποιήσει ο πράκτορας στο περιβάλλον του. Σε κάθε χρονική στιγμή t, ο πράκτορας πρέπει να επιλέξει μία ενέργεια από το σύνολο των διαθέσιμων ενεργειών του. Οι ενέργειες μπορεί να είναι διακριτές ή συνεχείς. Όπως και πριν, στο παιχνίδι της τρίλιζας, ο αριθμός των δυνατών κινήσεων του πράκτορα είναι πεπερασμένος και άρα ο χώρος ενεργειών είναι διακριτός (*Discrete action space*). Αντίθετα, στην πλοήγηση ενός ρομπότ σε έναν άγνωστο χώρο, δεν αρκεί το ρομπότ να επιλέξει να κινηθεί πχ γρήγορα προς τα δεξιά, αλλά απαιτείται μεγαλύτερη ακρίβεια στην κίνηση του. Έτσι, το ρομπότ επιλέγει συγκεκριμένη ταχύτητα και γωνία κίνησης, με αποτέλεσμα ο χώρος ενεργειών είναι συνεχής (*Continuous action space*). Η επιλογή των δυνατών ενεργειών του πράκτορα από τον σχεδιαστή του συστήματος είναι σημαντική, επειδή επηρεάζει την πολυπλοκότητα του προβλήματος και την επιλογή του αλγορίθμου εκπαίδευσης. 

#### Πολιτική {.unnumbered}

Η πολιτική του πράκτορα περιγράφει τον τρόπο με τον οποίο ο πράκτορας επιλέγει την επόμενη ενέργειά του σε κάθε κατάσταση. Από τεχνικής άποψης, η πολιτική του πράκτορα αποτελεί απλώς μία αντιστοιχίση της κάθε κατάστασης σε μία ενέργεια ή, αν θέλουμε να την παρομοιάσουμε με τον τρόπο που λειτουργούν οι άνθρωποι, πρόκειται για το σκεπτικό το οποίο χρησιμοποιεί ο πράκτορας για τη λήψη αποφάσεων.
Η πολιτική αυτή μπορεί να είναι ντετερμινιστική ή στοχαστική. Στην πρώτη περίπτωση, συμβολίζεται ως $a_{t+1}=\pi(s_{t})$, δηλ. η πολιτική αποτελεί την συνάρτηση που δέχεται ως είσοδο την κατάσταση του περιβάλλοντος και επιστρέφει την ενέργεια του πράκτορα. Τότε, όταν ο πράκτορας βρίσκεται στην ίδια κατάσταση, θα επιλέγει πάντα την ίδια ενέργεια. Στη δεύτερη περίπτωση, η πολιτική αντιστοιχίζει κάθε μία από τις δυνατές ενέργειες κάθε κατάστασης του πράκτορα, σε πιθανότητες. Συμβολίζεται ως $\pi(a_{i}|s_{t})$ και αντιπροσωπεύει την πιθανότητα να επιλέξει την ενέργεια $a_{i}$ ενώ βρίσκεται στην κατάσταση $s_{t}$. Εξυπακούεται ότι το άθροισμα των πιθανοτήτων όλων των πιθανών ενεργειών σε μία κατάσταση ισούται με τη μονάδα. Προκειμένου να γινεί καλύτερα κατανοητός ο τρόπος λειτουργίας μίας στοχαστικής πολιτικής, ας θεωρησούμε το παράδειγμα του κλασικού arcade παιχνιδιού «Snake» (φιδάκι). Στο παιχνίδι αυτό, ο πράκτορας έχει 4 δυνατές ενέργειες: πάνω, κάτω, αριστερά και δεξιά. Σε μία συγκεκριμένη κατάσταση, η στοχαστική πολιτική θα αντιστοιχίσει σε κάθε μία από τις 4 δυνατές κινήσεις του πράκτορα, μία πιθανότητα. Για παράδειγμα, αν οι πιθανότητες είναι: P(πάνω) = 0.7, P(κάτω) = 0.2, P(αριστερά) = 0.05 και P(δεξιά) = 0.05, τότε ο πράκτορας θα κινηθεί πάνω με πιθανότητα 0.7, κάτω με πιθανότητα 0.2 και αριστερά και δεξιά με πιθανότητα 0.05. Γενικά, οι στοχαστικές πολιτικές είναι πιο ευέλικτες από τις ντετερμινιστικές, καθώς επιτρέπουν στον πράκτορα να εξερευνήσει το περιβάλλον του και να ανακαλύψει νέες στρατηγικές. Ωστόσο, η επιλογή της στοχαστικής πολιτικής απαιτεί προσεκτική ανάλυση, καθώς μπορεί να οδηγήσει σε ανεπιθύμητες συμπεριφορές του πράκτορα.

Πλέον, γίνεται κατανοητό ότι η πολιτική που αναπτύσσει ο πράκτορας είναι ο παράγοντας που εν τέλει, καθορίζει την επιτυχία ή αποτυχία της εκπαίδευσης του. Το ζητούμενο είναι ο πράκτορας να αναπτύξει τη βέλτιστη πολιτική, η οποία θα τον οδηγήσει στην επίτευξη του στόχου του με τον πιο αποδοτικό τρόπο. Ωστόσο, η ανάπτυξη της βέλτιστης πολιτικής αποτελεί στην πράξη μία πρόκληση, καθώς απαιτεί την εξερεύνηση του περιβάλλοντος και την ανακάλυψη των στρατηγικών που θα οδηγήσουν στην επίτευξη του στόχου. 

Έχοντας εξηγήσει την έννοια της πολιτικής, είμαστε πλέον σε θέση να παρουσιάσουμε το βασικότερο ίσως, πρόβλημα της ενισχυτικής μάθησης, το δίλημμα της εξερεύνησης έναντι της αξιοποίησης.

#### Δίλημμα Εξερεύνησης - Αξιοποίησης {.unnumbered}

Το δίλημμα της εξερεύνησης έναντι της αξιοποίησης (*exploration vs exploitation dilemma*) αποτελεί ένα από τα πιο δυσεπίλητα προβλήματα στο πεδίο της ενισχυτικής μάθησης. Το δίλημμα αναφέρεται στην ισορροπία μεταξύ της εξερεύνησης, δηλ. της ανακάλυψης νέων περιοχών και της εκμετάλλευσης, δηλ. της χρήσης της υπάρχουσας γνώσης. Έτσι, προκύπτει το ερώτημα: πρέπει ο πράκτορας να συνεχίσει να εφαρμόζει τις ενέργειες που γνωρίζει ότι λειτουργούν (*αξιοποίηση*) ή να δοκιμάσει νέες ενέργειες, προκειμένου να ανακαλύψει νέες στρατηγικές που ίσως είναι ακόμα πιο αποδοτικές (*εξερεύνηση*); Για παράδειγμα, πρέπει κάποιος να παραγγέλνει πάντα το ίδιο, γνωστό πιάτο στο αγαπημένο του εστιατόριο ή να δοκιμάσει κάτι καινούριο και διαφορετικό, με την ελπίδα να ανακαλύψει κάτι καλύτερο; Το παράδειγμα αυτό παρουσιάζεται και παραστατικά, στην *Εικόνα @fig:theory:reinforcement_learning:ExplorationVsExploitation*.

![Το δίλημμα Εξερεύνησης - Αξιοποίησης [@mediumEpsilonGreedyAlgorithm].](3-theory/figures/test2.png){#fig:theory:reinforcement_learning:ExplorationVsExploitation width=80%}

Προκειμένου να κατανοήσουμε σε βάθος το δίλημμα αυτό και τις επιπλοκές που έχει στην εκπαίδευση του πράκτορα, θα ξεκινήσουμε εξηγώντας τη διαδικασία που θα έπρεπε, ιδανικά, να ακολουθηθεί, ώστε ο πράκτορας να αναπτύξει την βέλτιστη πολιτική. Στη συνέχεια, θα αναδείξουμε τα προβλήματα που ανακύπτουν, τα οποία δυσκολεύουν την ομαλή εξέλιξη της παραπάνω διαδικασίας.

Στα πρώτα στάδια της εκπαίδευσης, επιθυμούμε ο πράκτορας να εξερευνήσει το περιβάλλον. Με τον όρο « εξερεύνηση», εννοούμε ο πράκτορας να δοκιμάσει τυχαίες ενέργειες σε διαφορετικές κατάστασεις, προκειμένου να αποκτήσει εμπειρία και να καταλάβει ποιές καταστάσεις είναι καλές και ποιές όχι. Δεν μας ενδιαφέρει ακόμα η απόδοση του πράκτορα, αλλά η ανάπτυξη μίας ισχυρής βάσης γνώσης. Είναι σημαντικό άλλωστε να έχουμε κατά νου πως οι πράκτορες τεχνητής νοημοσύνης δεν έχουν όλες τις προηγούμενες γνώσεις κι εμπειρίες που έμεις, οι άνθρωποι, έχουμε αναπτύξει, ήδη από μικρή ηλικιά. Για παράδειγμα, στο περιβάλλον εκπαίδευσης αυτής της διπλωματικής εργασίας, στόχος του πράκτορα είναι η στάθμευση ενός αυτοκινητού σε μία ελεύθερη θέση. Εάν ο χειριστής του αυτοκινήτου ήταν άνθρωπος, αμέσως μόλις έβλεπε το περιβάλλον εκπαίδευσης (τον χώρο πάρκινγκ), θα είχε ήδη αρκετές χρήσιμες γνώσεις, όπως το πως κινείται ένα αυτοκίνητο, το ότι ο στόχος του είναι να παρκάρει το αυτοκίνητο στην ελεύθερη θέση ή το ότι η σύγκρουση με άλλα, σταθμευμένα αυτοκίνητα είναι -το λιγότερο- ανεπιθύμητη. Ωστόσο, ο πράκτορας δεν γνωρίζει ακόμα τίποτα από αυτά και πρέπει να τα ανακαλύψει μόνος του, εξερευνώντας το περιβάλλλον και μαθαίνοντας από τις ανταμοιβές που παίρνει. Έτσι, προς το παρόν, επιθυμούμε παραδείγματος χάριν, ο πράκτορας να συγκρούεται με άλλα αυτοκίνητα, ώστε να καταλάβει ότι αυτό δεν είναι επιθυμητό και να το αποφεύγει στα επόμενα στάδια της εκπαίδευσης του.

Μετά από ένα επαρκές διάστημα εξερεύνησης, ο πράκτορας πρέπει να περάσει στο στάδιο της αξιοποίησης. Στο στάδιο αυτό, ο πράκτορας δεν επιλέγει πια τυχαίες ενέργειες, αλλά εκμεταλλεύεται (*αξιοποιεί*) τη γνώση που έχει ήδη αποκτήσει, ώστε να επιλέγει σε κάθε κατάσταση την καλύτερη ενέργεια. Η μετάβαση από το ένα στάδιο στο άλλο πρέπει να γίνει σταδιακά, δηλ. επιθυμούμε ο πράκτορας να ελαττώσει βαθμιαία την τυχαιότητα των κινήσεων του, έως ότου τη μηδενίσει και βασίζεται εξ ολοκλήρου στην πολιτική του για τη λήψη αποφάσεων. Μόνο με αυτόν τον τρόπο, θα καταφέρει ο πράκτορας να ανακαλύψει τη βέλτιστη συμπεριφορά. Για παράδειγμα, στο περιβάλλον στάθμευσης αυτοκινήτων, επιθυμούμε ο πράκτορας να καταλάβει νωρίς στην εκπαίδευση, ότι το να πλησιάζει την ελεύθερη θέση είναι καλό. Έτσι, όσο βασίζεται όλο και περισσότερο στην πολιτική του για τη λήψη αποφάσεων, τόσο θα επιλέγει ενέργειες που θα τον κινούν πιο κοντά στην ελεύθερη θέση. Ταυτόχρονα όμως, διατηρεί έναν βαθμό τυχαιότητας, ο οποίος αν και μειωμένος, τον ωθεί να εξακολουθεί να εξερευνεί νέες καταστάσεις και στρατηγικές. Έτσι, θα ανακαλύψει κάποτε τη μεγάλη επιβράβευση της στάθμευσης, θα εξερευνήσει διαφορετικούς τρόπους για να φτάνει σε αυτήν και στο τέλος της εκπαίδευσης, όταν θα αξιοποιεί αποκλειστικά την πολιτική του, θα επιλέγει πάντα τον βέλτιστο τρόπο στάθμευσης.

Με τη λογική της σταδιακής μείωσης της εξερεύνησης έναντι της αξιοποίησης, ο πράκτορας ωθείται να εξερευνήσει «προς τη σωστή κατεύθυνση». Αυτό σημαίνει πως δεν θέλουμε ο πράκτορας να σπαταλάει μεγάλο χρόνο εκπαίδευσης εξερευνώντας μη χρήσιμες καταστάσεις, όπως διαφορετικούς τρόπους να συγκρούεται με άλλα αυτοκίνητα, αλλά θέλουμε να εξερευνεί καταστάσεις χρήσιμες, που τον φέρνουν όλο πιο κοντά στον τελικό του στόχο.

Επομένως, για τη σωστή εκτέλεση της παραπάνω διαδικασίας, είναι κρίσιμη η ισορροπία μεταξύ της εξερεύνησης και της εκμετάλλευσης, δηλ. ο ρυθμός με τον οποίο ο πράκτορας μειώνει την τυχαιότητα των ενεργειών του. Εάν ο πράκτορας εξερευνήσει πολύ λίγο τον χώρο, τότε υπάρχει ο κίνδυνος να μην αναπτύξει καλή εικόνα του μοντέλου και όταν αρχίσει να ακολουθεί την πολιτική να οδηγείται σε λάθος κινήσεις. Αντίθετα, εάν ο χρόνος εξερεύνησης είναι πολύ μεγάλος, αυτό αυξάνει το κόστος και τον χρόνο εκτέλεσης του πειράματος. Επομένως, η επίτευξη της ισορροπίας αυτής αποτελεί μία από τις μεγαλύτερες προκλήσεις στον τομέα της ενισχυτικής μάθησης.

Είναι σημαντικό να σημειωθεί ότι η επιτυχία του πράκτορα στο στάδιο της αξιοποίησης εξαρτάται από την ποιότητα της εμπειρίας που έχει αποκτήσει κατά τη διάρκεια της εξερεύνησης. Εάν η εμπειρία αυτή είναι ανεπαρκής ή ανεπιθύμητη, τότε ο πράκτορας θα αντιμετωπίσει δυσκολίες στην επίτευξη του στόχου του.

Επομένως, για την επιτυχή εκτέλεση της ανωτέρω διαδικασίας, είναι κρίσιμη η ισορροπία μεταξύ εξερεύνησης και αξιοποίησης, δηλ. η ρύθμιση του ρυθμού μείωσης της τυχαιότητας των ενεργριών του πράκτορα. Η ισορροπία αυτή είναι στην πράξη πολύ δύσκολο να επιτευχθεί, καθώς δίνεται περισσότερη βαρύτητα στο ένα από τα δύο στάδια. 

Συγκεκριμένα, εάν ο πράκτορας εξερευνήσει πολύ λίγο τον χώρο, τότε δεν θα αποκτήσει επαρκείς εμπειρίες και δεν θα ανακαλύψει τον βέλτιστο τρόπο για την επίτευξη του στόχου του - ή μπορεί και να μην ανακαλύψει καν τον στόχο του. Για να γίνει καλύτερα κατανοητό αυτό, ας επιστρέψουμε στο παράδειγμα της αυτόματης στάθμευσης κι ας εξετάσουμε μία τέτοια συμπεριφορά που συνέβη πολλές φορές στις εκπαιδεύσεις των πρακτόρων. Η επιλογή τυχαίων ενεργειών στα πρώτα στάδια της εκπαίδευσης, έχει ως αποτέλεσμα ο πράκτορας να συγκρούεται συχνά με άλλα αυτοκίνητα. Αν η εξερεύνηση σταματήσει πρόωρα, τώρα ο πράκτορας θα αρχίσει να εκμεταλλεύεται τη γνώση που έμαθε. Η γνώση αυτή, είναι μόνο ότι οι συγκρούσεις είναι ανεπιθύμητες, κι έτσι θα τις αποφεύγει. Ωστόσο, ο πράκτορας δεν πρόλαβε να μάθει πως η στάθμευση οδηγεί σε μεγάλη επιβράβευση κι έτσι, δεν επιχειρεί ποτέ να παρκάρει, αλλά κάνει απλώς κύκλους γύρω από την πίστα του παιχνιδιού. Τότε, λέμε πως ο πράκτορας έχει υιοθετήσει μία υποβέλτιστη πολιτική (*suboptimal policy*), ή έχει παγιδευτεί σε τοπικό μέγιστο (*local maximum*). Η δεύτερη έκφραση αναφέρεται στη συνάρτηση ανταμοιβής, καθώς γνωρίζουμε πως στόχος του πράκτορα είναι να τη μεγιστοποιεί, δηλ. να παίρνει ως ανταμοιβή το ολικό μέγιστο της. Όμως, όταν η εξερεύνηση του πράκτορα είναι ανεπαρκής, αυτός παίρνει ένα τοπικό μέγιστο της συνάρτησης ανταμοιβής, το οποίο είναι μικρότερο από το ολικό. 

Από την άλλη, εάν ο χρόνος εξερεύνησης είναι πολύ μεγάλος, αυξάνεται ο χρόνος εκτέλεσης του πειράματος, επειδή ο πράκτορας αφιερώνει σημαντικό χρόνο σε μη χρήσιμες καταστάσεις. Ακόμα, υπάρχει ο κίνδυνος η απόδοση του πράκτορα να είναι ιδιαίτερα ασταθής, καθώς συνεχίζει να δοκιμάζει νέες ενέργειες αντί να βελτιώνει και να εξελίσσει τις γνωστές του στρατηγικές. Έτσι, μπορεί να μην καταφέρει να αναπτύξει μία σταθερή πολιτική, η οποία να τον οδηγεί στον στόχο του.

Μέχρι και σήμερα, δεν υπάρχει κάποια γενικά αποδεκτή λύση στο πρόβλημα της εξερεύνησης έναντι της αξιοποίησης. Έχουν προταθεί οριμένες τεχνικές για την επίτευξη της ζητούμενης ισορροπίας, όπως ο αλγόριθμος $\epsilon$-greedy και η κανονικοποίηση της εντροπίας (*entropy regularization*), τις οποίες θα εξετάσουμε σε επόμενες ενότητες, όμως καμία δεν εγγυάται την επίτευξη της βέλτιστης πολιτικής. 

### Κατηγορίες αλγορίθμων {#sec:theory:reinforcement_learning:algorithm_types}

Έχοντας κατανοήσει τις βασικές αρχές της ενισχυτικής μάθησης, μπορούμε πλέον να μελετήσουμε τους επικρατέστερους αλγορίθμους που χρησιμοποιούνται για την εκπαίδευση πρακτόρων. Στην Ενότητα αυτή, θα προβούμε σε μία επισκόπηση των διαφορετικών κατηγοριών αλγορίθμων και σε μεταγενέστερες ενότητες, θα εξετάσουμε τους αλγορίθμους που χρησιμοποιήθηκαν στα πλαίσια αυτής της εργασίας.

Μία συνοπτική ταξινόμηση των δημοφιλέστερων αλγορίθμων ενισχυτικής μάθησης παρουσιάζεται στην *Εικόνα @fig:theory:reinforcement_learning:rl_taxonomy*.

![Ταξινόμηση αλγορίθμων ενισχυτικής μάθησης [@openaiPartKinds].](3-theory/figures/rl_algorithms2.png){#fig:theory:reinforcement_learning:rl_taxonomy width=95%}

Παρατηρούμε πως μία πρώτη διάκριση των αλγορίθμων ενισχυτικής μάθησης γίνεται σε αλγορίθμους *Model-Free* και *Model-Based*. 

#### Model-Free vs Model-Based {.unnumbered}

Η διάσπαση των αλγορίθμων στις δύο βασικές αυτές κατηγορίες εξαρτάται από την πρόσβαση ή μη του πράκτορα στο μοντέλο του περιβάλλοντος. Με τον όρο «μοντέλο του περιβάλλοντος» εννοούμε μία συνάρτηση που προβλέπει τις μεταβάσεις καταστάσεων και τις ανταμοιβές. Οι αλγόριθμοι model based απαιτούν την ύπαρξη ενός μοντέλου του περιβάλλοντος, ενώ οι model free όχι.

**Model Based**

Το μεγάλο πλεονέκτημα της γνώσης του μοντέλου έγκειται στο γεγονός ότι ο πράκτορας γνωρίζει όλες τις πιθανές ενέργειες του και τα αποτελέσματα αυτών. Επομένως, έχει τη δυνατότητα να σχεδιάσει την επόμενη κίνηση του, επιλέγοντας την καλύτερη από τις διαθέσιμες ενέργειες. Με τον τρόπο αυτό, οι αλγόριθμοι model based χρησιμοποιούν το μοντέλο για την ανάπτυξη μίας βέλτιστης πολιτικής. Όταν αυτή η προσέγγιση λειτουργεί, μπορεί να οδηγήσει σε σημαντική βελτίωση της αποδοτικότητας σε σχέση με αλγορίθμους που δεν διαθέτουν μοντέλο. 

Ωστόσο, το κύριο πρόβλημα αυτών των αλγορίθμων έγκειται στη δημιουργία του μοντέλου. Συγκεκριμένα, το μοντέλο αυτό είτε προυπάρχει και είναι διαθέσιμο στον πράκτορα, είτε πρέπει να δημιουργηθεί από τον ίδιο. Στην πλειοψηφία των προβλημάτων ενισχυτικής μάθησης, το μοντέλο του περιβάλλοντος είναι άγνωστο και πρέπει να αναπτυχθεί από τον πράκτορα κατά την εκπαίδευση του, μέσω των εμπειριών του. Αυτό παρουσιάζει αρκετές προκλήσεις, καθώς η πολυπλοκότητα του πραγματικού κόσμου καθιστά δύσκολη τη δημιουργία ενός ακριβούς μοντέλου. Κάθε απόκλιση μεταξύ του μοντέλου και του πραγματικού κόσμου (*model bias*) μπορεί να οδηγήσει σε μειωμένη απόδοση του πράκτορα, κατά την εφαρμογή του στον πραγματικό κόσμο. Συνολικά, η εκμάθηση του μοντέλου είναι δύσκολη και μπορεί να αποτύχει, ακόμα και αν αφιερωθεί πολύς χρόνος και υπολογιστική ισχύς στην ανάπτυξή του. 

**Model Free**

Οι αλγόριθμοι model free δεν απαιτούν γνώση ενός μοντέλου του περιβάλλοντος, αλλά στηρίζονται αποκλειστικά στην αλληλεπίδραση του πράκτορα με το περιβάλλον. Επομένως, ο πράκτορας χρησιμοποιεί την εμπειρία του για την ανάπτυξη της πολιτικής του. Το μειονέκτημα αυτής της προσέγγισης είναι ότι συνήθως απαιτείται περισσότερος χρόνος εκπαίδευσης, προεκιμένου ο πράκτορας να αναπτύξει μία αποδοτική πολιτική. Ωστόσο, η απουσία ανάπτυξης ενός μοντέλου του περιβάλλοντος, καθιστά τους αλγορίθμους model free πιο εύκολους στην υλοποίηση και τη ρύθμιση. Επιπλέον, οι αλγορίθμοι αυτοί αποδεικνύονται πιο σταθεροί και αξιόπιστοι, καθώς δεν επηρεάζονται από τυχόν σφάλματα και αποκλίσεις του μοντέλου. Έτσι, οι μέθοδοι χωρίς μοντέλο είναι πιο δημοφιλείς κι έχουν αναπτυχθεί και δοκιμαστεί περισσότερο από τις μεθόδους με μοντέλο.

Για τους λόγους που αναλύθηκαν παραπάνω, όλοι οι αλγόριθμοι που υλοποιήθηκαν στα πλαίσια αυτής της εργασίας ανήκουν στην κατηγορία των model free αλγορίθμων. Ως εκ τούτου, στη συνέχεια θα αναλύσουμε μόνο τις υποκατηγορίες των αλγορίθμων model free.

#### Κατηγορίες Model Free Αλγορίθμων {.unnumbered}

Υπάρχουν δύο βασικές προσεγγίσεις στους model free αλγορίθμους: οι αλγόριθμοι Εκτίμησης Αξίας (*Value Estimation*) και οι αλγόριθμοι Βελτιστοποίησης Πολιτικής (*Policy Optimization*). Υπάρχει όμως κι ένας τρίτος τύπος model free αλγορίθμων, οι αλγόριθμοι Δράστη-Κριτή (*Actor-Critic*), οι οποίοι συνδυάζουν στοιχεία από τις δύο προηγούμενες προσεγγίσεις. Κάθε ένα από αυτούς τους τύπους έχει τα δικά του πλεονεκτήματα και μειονεκτήματα, και είναι κατάλληλος για διαφορετικούς τύπους προβλημάτων. Οι 3 αυτοί τύποι φαίνονται στο κάτω αριστερά τμήμα της *Εικόνας @fig:theory:reinforcement_learning:rl_taxonomy* [^Q] και εξετάζονται στη συνέχεια.

[^Q]: Οι αλγόριθμοι Εκτίμησης Αξίας παρουσιάζονται στην εικόνα ως Q-Learning αλγόριθμοι, ενώ οι αλγόριθμοι που βρίσκονται μεταξύ των δύο βασικών προσεγγίσεων αποτελούν τους αλγορίθμους Δράστη-Κριτή.

**Αλγόριθμοι Εκτίμησης Αξίας**

Οι αλγόριθμοι Εκτίμησης Αξίας (*Value Estimation*) χωρίζονται σε δύο υποκατηγορίες, τους αλγορίθμους Εκτίμησης Αξίας Κατάστασης (*State Value Estimation*) και τους αλγορίθμους Εκτίμησης Αξίας Ζεύγους Κατάστασης-Ενέργειας (*State-Action Value Estimation*). Η λογική πίσω από αυτούς τους αλγορίθμους όμως παραμένει η ίδια και στις 2 περιπτώσεις και αναλύεται στη συνέχεια.

Οι αλγόριθμοι Εκτίμησης Αξίας Καταστάσεων επικεντρώνονται στην εκμάθηση της αξίας των καταστάσεων. Η βασική ιδέα είναι η ανάπτυξη μίας συνάρτησης αξίας (*value function*), η οποία συμβολίζεται ως $V_{\pi}(s)$ και θα εκτιμάει την ποιότητα μίας κατάστασης. Ο όρος «αξία» ή «ποιότητα» μίας κατάστασης αναφέρεται στην αναμενόμενη αθροιστική ανταμοιβή που θα λάβει ο πράκτορας, ξεκινώντας από την κατάσταση αυτή και ακολουθώντας έπειτα την πολιτική που έχει αναπτύξει. 

Κατά παρόμοιο τρόπο, οι αλγόριθμοι Εκτίμησης Αξίας Ζευγών Κατάστασης-Ενέργειας εστιάζουν στην εκμάθηση της αξίας των ζευγών κατάστασης-ενέργειας. Η συνάρτηση αξίας σε αυτή την περίπτωση συμβολίζεται ως $Q_{\theta}(s, a)$ και εκτιμά την ποιότητα της ενέργειας $a$ στην κατάσταση $s$, δηλ. την αναμενόμενη αθροιστική ανταμοιβή που θα λάβει ο πράκτορας, εάν επιλέξει την ενέργεια $a$ στην κατάσταση $s$ και ακολουθήσει έπειτα την πολιτική που έχει αναπτύξει. Οι ενημερώσεις της συνάρτησης αξίας γίνονται *off-policy*, το οποίο σημαίνει ότι κάθε ενημέρωση μπορεί να χρησιμοποιήσει δεδομένα που συλλέχθηκαν σε οποιοδήποτε σημείο της εκπαίδευσης, ανεξάρτητα από το πώς εξερευνούσε τότε ο πράκτορας το περιβάλλον. Χαρακτηριστικό παράδειγμα αυτής της προσέγγισης είναι ο αλγόριθμος Q-Learning και για αυτό, οι συγκεκριμένοι αλγόριθμοι συχνά αναφέρονται και ως Q-Learning αλγόριθμοι.

Τα μεγαλύτερο πλεονέκτημα των αλγορίθμων Εκτίμησης Αξίας είναι η απλότητά τους. Ακόμα, είναι ιδιαίτερα αποτελεσματικοί σε περιβάλλοντα με διακριτούς χώρους καταστάσεων και ενεργειών. Στα περιβάλλοντα αυτά, είναι σημαντικά πιο αποδοτικοί στον χρόνο εκπαίδευσης σε σχέση με τις άλλες κατηγορίες αλγορίθμων, επειδή μπορούν να επαναχρησιμοποιήσουν τα ίδια δεδομένα πολλές φορές. Όμως, οι αλγόριθμοι Εκτίμησης Αξίας βελτιστοποιούν μόνο έμμεσα την απόδοση του πράκτορα, μέσω της ενημέρωσης της συνάρτησης αξίας. Επίσης, παρουσιάζουν δυσκολία στην αντιμετώπιση μεγάλων ή συνεχών χώρων καταστάσεων και ενεργειών, αφού απαιτούν την αποθήκευση μίας τιμής για κάθε ζεύγος κατάστασης-ενέργειας. Έτσι, συνολικά, τείνουν να είναι λιγότερο σταθεροί από τους αλγορίθμους των άλλων δύο κατηγοριών.

**Αλγόριθμοι Βελτιστοποίησης Πολιτικής**

Οι αλγόριθμοι Βελτιστοποίησης Πολιτικής (*Policy Optimization*) μαθαίνουν απευθείας μία πολιτική $\pi_{\theta}$, που μεγιστοποιεί την αναμενόμενη αθροιστική ανταμοιβή, χωρίς να απαιτούν την ανάπτυξη συνάρτησης αξιών. Αντίθετα, επικεντρώνονται στη ρύθμιση των παραμέτρων $\theta$ της πολιτικής, προκειμένου να βελτιστοποιήσουν την απόδοση τους. Αυτή η ρύθμιση πραγματοποιείται *on-policy*, δηλαδή κάθε ενημέρωση χρησιμοποιεί μόνο δεδομένα που συλλέχθηκαν ενώ ο πράκτορας ενεργούσε σύμφωνα με την πιο πρόσφατη έκδοση της πολιτικής του. Όπως αναφέρθηκε και νωρίτερα, η πολιτική μπορεί να είναι ντετερμινιστική ή στοχαστική. Μία ντετερμινιστική πολιτική συμβολίζεται ως $a_{t+1}=\pi_{\theta}(s_{t})$ και υποδηλώνει ότι σε συγκεκριμένη κατάσταση, θα επιλέγεται κάθε φορά η ίδια ενέργεια. Αντίθετα, μία στοχαστική πολιτική συμβολίζεται ως $\pi_{\theta}(a_{i}|s_{t})$ και εισάγει ένα βαθμό τυχαιότητας στη λήψη αποφάσεων από τον πράκτορα, αφού η επιλεγμένη ενέργεια προκύπτει από μία κατανομή πιθανοτήτων. Παραδείγματα αλγορίθμων αυτής της κατηγορίας αποτελούν οι κλασικοί αλγόριθμοι REINFORCE και PPO.

Το μεγαλύτερο πλεονέκτημα που έχουν οι αλγόριθμοι της κατηγορίας βελτιστοποίησης πολιτικής είναι ότι βελτιστοποιούν άμεσα την πολιτική τους, δηλ. τον στόχο της εκπαίδευσης. Αυτό το γεγονός τους καθιστά σταθερούς και αξιόπιστους. Ακόμα, αξιοσημείωτη είναι η απόδοση τους σε περιβάλλοντα με συνεχείς χώρους καταστάσεων ή και ενεργειών. Ωστόσο, συχνά απαιτούν μεγάλο χρόνο εκπαίδευσης για την επίτευξη σταθερής μάθησης. Επιπλέον, απαιτούν πολύ προσεκτική ρύθμιση των παραμέτρων τους, επειδή είναι επιρρεπείς σε τοπικά ελάχιστα.

**Αλγόριθμοι Δράστη - Κριτή**

Οι αλγόριθμοι Δράστη-Κριτή (*Actor-Critic*) συνδυάζουν στοιχεία από τις δύο προηγούμενες κατηγορίες. Έτσι, προσπαθούν να εκμεταλλευτούν τα πλεονεκτήματα αλλά και να αποφύγουν τις αδυναμίες της κάθε προσέγγισης. Αποτελούνται από δύο ξεχωριστά νευρωνικά δίκτυα: ένα δίκτυο-δράστη και ένα δίκτυο-κριτή. Ο δράστης αποφασίζει ποια ενέργεια να πάρει ο πράκτορας και έπειτα ο κριτής αξιολογεί την επιλεγμένη ενέργεια με την εκτίμηση της συνάρτησης αξίας. Στη συνέχεια, ο κριτής παρέχει στον δράστη ανατροφοδότηση για να βελτιώσει την πολιτική του. Μάλιστα, ο κριτής βοηθά στη μείωση της διακύμανσης στις ενημερώσεις της πολιτικής, οδηγώντας σε πιο σταθερή μάθηση. Παραδείγματα αλγορίθμων αυτής της κατηγορίας αποτελούν οι SAC και TD3.

Το μεγαλύτερο πλεονέκτημα των αλγορίθμων Δράστη-Κριτή είναι πως αξιοποιούν τόσο τις τεχνικές της εκτίμησης αξίας, όσο και της βελτιστοποίησης πολιτικής. Έτσι, η διαδικασία αυτή επιτυγχάνει συνήθως μία καλή ισορροπία μεταξύ εξερεύνησης και αξιοποίησης, οδηγώντας σε πιο σταθερή και αποτελεσματική μάθηση. Χρησιμοποιείται ευρύτερα σε προβλήματα που περιλαμβάνουν πολύπλοκα περιβάλλοντα και μεγάλους χώρους καταστάσεων. Ωστόσο, οι αλγόριθμοι αυτοί είναι πιο πολύπλοκοι στην υλοποίηση και τη ρύθμιση, λόγω της αλληλεπίδρασης μεταξύ του δράστη και του κριτή. Μπορούν ακόμα να υποφέρουν από αστάθεια ή και απόκλιση, εάν ο κριτής παρέχει κακές εκτιμήσεις.
$\\$

Τέλος, αξίζει να γίνει ειδική μνεία σε μία άλλη, λιγότερο γνωστή απόπειρα ταξινομήσης του μεγάλου πλήθους των αλγορίθμων ενισχυτικής μάθησης από τον [@githubGitHubBennylpRLTaxonomy], η οποία παρουσιάζεται στην *Εικόνα @fig:theory:reinforcement_learning:rl_taxonomy2*. 

![Εκτενέστερη ταξινόμηση αλγορίθμων ενισχυτικής μάθησης [@githubGitHubBennylpRLTaxonomy].](3-theory/figures/rl-taxonomy.svg){#fig:theory:reinforcement_learning:rl_taxonomy2 width=100%}

Στην εικόνα αυτή, τα βέλη που συνδέουν δύο αλγορίθμους υποδηλώνουν ότι ο ένας αλγόριθμος αποτελεί βελτίωση του άλλου. Συγκεκριμένα, οι συνεχείς γραμμές υποδηλώνουν ισχυρή σύνδεση μεταξύ των δύο αλγορίθμων, ενώ οι διακεκομμένες γραμμές υποδηλώνουν πιο ασθενή σύνδεση. Έτσι, καταλαβαίνουμε για παράδειγμα, πως οι αλγόριθμοι PPO, TD3 που χρησιμοποιηθηκαν στην παρούσα εργασία, αποτελούν βελτιώσεις των αλγορίθμων TRPO και DDPG αντίστοιχα. Επιπλέον, στο κάτω μέρος της εικόνας παρουσιάζεται το χρονολόγιο της δημοσίευσης των αλγορίθμων, προκειμένου να γίνει κατανοητή η εξέλιξη των αλγορίθμων στο χρόνο. Μάλιστα, στον σύνδεσμο που υπάρχει στη βιβλιογραφία, υπάρχει ένα πλήρες αποθετήριο, όπου για κάθε αλγόριθμο μπορεί κάνεις να βρει την αντίστοιχη δημοσίευση, καθώς και άλλες χρήσιμες πληροφορίες. Για αυτό, προτρέπω τους ενδιαφερόμενους αναγνώστες να επισκεφτούν και να χρησιμοποιήσουν το συγκεκριμένο αποθετήριο, για να περιηγηθούν γρήγορα και εύκολα στον χώρο των αλγορίθμων ενισχυτικής μάθησης.

### Ο αλγόριθμος $Q$-learning {#sec:theory:reinforcement_learning:q_learning}

Ο αλγόριθμος $Q$-Learning προτάθηκε από τον Chris Watkins το 1989, στη διδακτορική του διατριβή [@Watkins1989]. Είναι ένας από τους πιο διαδεδομένους αλγορίθμους ενισχυτικής μάθησης και αποτέλεσε τη βάση για πολλές εξελίξεις στον τομέα. Ανήκει στην κατηγορία των model free αλγορίθμων και πιο συγκεκριμένα, στην κατηγορία των αλγορίθμων εκτίμησης αξίας ζευγών κατάστασης-ενέργειας.

#### Μεθοδολογία αλγορίθμου {.unnumbered}

Όπως κι οι υπόλοιποι αλγόριθμοι της κατηγορίας αξίας ζευγών κατάστασης-ενέργειας, ο $Q$-Learning επικεντρώνεται στην εκμάθηση της αξίας των ζευγών κατάστασης-ενέργειας. Η συνάρτηση αξίας που αναπτύσσει συμβολίζεται ως $Q(s, a)$, όπου το «$Q$» αντιπροσωπεύει την ποιότητα (*Quality*) της ενέργειας $a$ στην κατάσταση $s$, δηλ. πόσο χρήσιμη είναι η συγκεκριμένη ενέργεια στη συγκεκριμένη κατάσταση, στο να μεγιστοποιήσει τις μελλοντικές ανταμοιβές. Συνήθως, χρησιμοποιείται ένας πίνακας (*$Q$ Table*) για την αποθήκευση των τιμών $Q$ της συνάρτησης $Q(s, a)$, όπου κάθε γραμμή αντιστοιχεί σε μία κατάσταση και κάθε στήλη σε μία ενέργεια, οπως φαίνεται στην *Εικόνα @fig:theory:reinforcement_learning:q_table*.

![Πίνακας $Q$ για την αποθήκευση των τιμών της συνάρτησης $Q(s, a)$ [@baeldungEpsilonGreedyQlearning].](3-theory/figures/Q-Table.png){#fig:theory:reinforcement_learning:q_table width=100%}

Το πιο σημαντικό κομμάτι του αλγορίθμου $Q$-Learning είναι η κατασκευή της συνάρτησης $Q(s, a)$. Η κατασκευή αυτή γίνεται μέσω της επαναληπτικής ενημέρωσης των τιμών $Q$.  

Αρχικά, ας δούμε πως προκύπτει μία τιμή $Q$ για ένα ζεύγος κατάστασης-ενέργειας. Η τιμή $Q$ για το ζεύγος $(s, a)$ υπολογίζεται κατά τη διάρκεια της εκπαίδευσης από την εξίσωση \ref{eq:theory:reinforcement_learning:bellman} (*εξίσωση Bellman*):
\begin{equation}
Q(s, a) = R(s, a) + \gamma \cdot \max_{a} Q(s', a)
\label{eq:theory:reinforcement_learning:bellman}
\end{equation}

Η εξίσωση αυτή, δηλώνει ότι η αξία του ζεύγους $(s, a)$ είναι ίση με την ανταμοιβή που προκύπτει από την εκτέλεση της ενέργειας $a$ στην κατάσταση $s$ (συμβολίζεται ως $R(s, a)$), συν την αναμενόμενη ανταμοιβή που προκύπτει εκτελώντας την καλύτερη ενέργεια $a$ σε όλες τις επόμενες καταστάσεις $s'$ (συμβολίζεται ως $\max_{a} Q(s', a)$), πολλαπλασιασμένη επί τον συντελεστή $\gamma$ . Ο συντελεστής αυτός ονομάζεται παράγοντας έκπτωσης (*discount factor*), παίρνει τιμές στο διάστημα [0, 1] και χρησιμοποιείται για τον καθορισμό της σημασίας των μελλοντικών ανταμοιβών. Συγκεκριμένα, οι μελλοντικές ανταμοιβές είναι λιγότερο πολύτιμες από τις τρέχουσες ανταμοιβές, και για αυτό ο παράγοντας $\gamma$ τις μετριάζει. Όταν παίρνει τιμές κοντά στο 0, τότε ο πράκτορας συμπεριφέρεται πιο κοντόφθαλμα, ενώ όσο η τιμή του παράγοντα πλησιάζει το 1, τόσο ο πράκτορας προσπαθεί να μεγιστοποιήσει το μακροπρόθεσμο κέρδος του.

Ωστόσο, όπως αναφέραμε, η ενημέρωση των τιμών $Q$ γίνεται επαναληπτικά κατά τη διάρκεια της εκπαίδευσης. Έτσι, στην αρχή της εκπαίδευσης, οι τιμές $Q$ όλων των ζευγών κατάστασης-ενέργειας αρχικοποιούνται σε μία τιμή (συνήθως 0). Έπειτα, όταν ο πράκτορας επισκέπτεται μία κατάσταση $s$, εκτελεί μία ενέργεια $a$ και λαμβάνει μία ανταμοιβή $R(s, a)$. Τότε, η τιμή $Q$ του ζεύγους $(s, a)$ ενημερώνεται σύμφωνα με την εξίσωση \ref{eq:theory:reinforcement_learning:TD_update_rule}:
\begin{equation}
Q^{new}(s, a) = (1-\alpha) \cdot Q^{old}(s, a) + \alpha \left[ R(s, a) + \gamma \cdot \max_{a} Q(s', a) \right]
\label{eq:theory:reinforcement_learning:TD_update_rule}
\end{equation}

Επειδή αυτή η εξίσωση χρησιμοποιεί τη διαφορά χρησιμότητας μεταξύ διαδοχικών καταστάσεων (κι επομένως, διαδοχικών χρόνων), ονομάζεται και Εξίσωση Ενημέρωσης Χρονικών Διαφορών (*Temporal Difference Update Rule*). Παρατηρούμε ότι η εξίσωση TD Update Rule χρησιμοποιεί την εξίσωση Bellman. Συγκεκριμένα, η νέα τιμή $Q$ του ζεύγους $(s, a)$  συμβολίζεται ως $Q^{new}(s, a)$ και προκύπτει από:

- τον όρο $(1-\alpha) \cdot Q^{old}(s, a)$, που αντιπροσωπεύει την παλιά τιμή $Q$ του ζεύγους $(s, a)$, δηλ. αυτήν που ήδη υπήρχε στον πίνακα $Q$, πολλαπλασιασμένη με τον συντελεστή $(1-\alpha)$ και
- τον όρο $\alpha \left[ R(s, a) + \gamma \cdot \max_{a} Q(s', a) \right]$, που αντιπροσωπεύει την τιμή $Q$ του ζεύγους $(s, a)$ που μόλις υπολογίστηκε από την εξίσωση Bellman, πολλαπλασιασμένη με τον συντελεστή $\alpha$.

Ο συντελεστής $\alpha$ ονομάζεται ρυθμός μάθησης (*learning rate*) και καθορίζει σε ποιό βαθμό, οι νεότερες, πιο πρόσφατες πληροφορίες αντικαθιστούν τις παλιές, δηλ. καθορίζει τον ρυθμό με τον οποίο ο πράκτορας μαθαίνει. Ο ρυθμός μάθησης παίρνει κι αυτός τιμές στο διάστημα [0, 1] και αποτελεί μία ακόμα κρίσιμη υπερπαράμετρο που πρέπει να ρυθμιστεί με προσοχή, καθώς επηρεάζει σημαντικά τη σύγκλιση του αλγορίθμου. Εάν πάρει την τιμή 0, τότε ο πίνακας $Q$ δεν ενημερώνεται καθόλου (δηλ. ο πράκτορας δεν μαθαίνει τίποτα καινούργιο), ενώ εάν πάρει την τιμή 1, τότε ο πίνακας $Q$ ενημερώνεται πλήρως από την τιμή που προκύπτει από την εξίσωση Bellman, δηλ. ο πράκτορας λαμβάνει υπόψη μόνο τις πιο πρόσφατες πληροφορίες. Συνήθως, οι τιμές του συντελεστή αυτού μειώνονται κατά τη διάρκεια της εκπαίδευσης, ώστε να επιτευχθεί μία καλή ισορροπία μεταξύ της εκμάθησης και της σταθερότητας του αλγορίθμου.

Τέλος, αφού ο πράκτορας κατασκευάσει τη συνάρτηση $Q(s, a)$, μέσα από πολλές ενημερώσεις των τιμών $Q$, τότε μπορεί να ενεργεί βέλτιστα μέσα από τον υπολογισμό $a = \arg\max_{a} Q(s, a)$, δηλ. επιλέγοντας απλώς την ενέργεια με τη μεγαλύτερη τιμή $Q$ σε κάθε κατάσταση.

#### Η τεχνική $\varepsilon$-greedy {.unnumbered}

Ένα σημαντικό κομμάτι της παραπάνω διαδικασίας είναι το πως ο πράκτορας επιλέγει την ενέργεια που θα εκτελέσει σε κάθε κατάσταση, όταν βρίσκεται ακόμα στο στάδιο της εκπαίδευσης. Πρόκειται για το δίλημμα εξερεύνησης-αξιοποίησης, το οποίο αναλύθηκε στην υποενότητα @sec:theory:reinforcement_learning:concepts. Ο αλγόριθμος $Q$-Learning αντιμετωπίζει το πρόβλημα αυτό, χρησιμοποιώντας την τεχνική $\varepsilon$-greedy. 

Η τεχνική $\varepsilon$-greedy είναι μία διαδεδομένη μέθοδος για την εξισορρόπηση της εξερεύνησης και της αξιοποίησης. Είναι εύκολη στην υλοποίηση, αλλά ταυτόχρονα αποδίδει εξίσου καλά με άλλες, πιο πολύπλοκες μεθόδους. Η τεχνική αυτή παρουσιάζεται υπό μορφή διαγράμματος ροής (*flowchart*) στην *Εικόνα @fig:theory:reinforcement_learning:epsilon_greedy*.

![Η τεχνική $\varepsilon$-greedy [@baeldungEpsilonGreedyQlearning].](3-theory/figures/epsilon-greedy.png){#fig:theory:reinforcement_learning:epsilon_greedy width=50%}

Σημαντικό ρόλο στην τεχνική αυτή παίζει η παράμετρος $\varepsilon$, η οποία λαμβάνει τιμές στο διάστημα [0,1] και συμβολίζει την πιθανότητα ο πράκτορας να εξερευνήσει το περιβάλλον. Συγκεκριμένα, κατά τη διάρκεια της εκπαίδευσης, όταν ο πράκτορας βρίσκεται σε μία κατάσταση $s$, επιλέγει με πιθανότητα $\varepsilon$, τυχαία μία από τις διαθέσιμες ενέργειες (εξερεύνηση), ενώ με πιθανότητα $1-\varepsilon$, επιλέγει την ενέργεια που έχει τη μεγαλύτερη τιμή $Q(s, a)$ στον πίνακα εκείνη τη στιγμή (αξιοποίηση) . 

Συνήθως, επιλέγεται η παράμετρος $\varepsilon$ μειώνεται σταδιακά κατά την εκπαίδευση κι έτσι, η τεχνική ονομάζεται και ως φθίνουσα (*decaying*) $\varepsilon$-greedy. Με τον τρόπο αυτό δίνεται μεγαλύτερη έμφαση στην εξερεύνηση στα πρώτα στάδια της εκπαίδευσης και στην αξιοποίηση στα τελευταία. Με άλλα λόγια, ο πράκτορας δρα σε μεγάλο βαθμό τυχαία στην αρχή της εκπαίδευσης, όπου η τιμή του $\varepsilon$ είναι υψηλή, ενώ όταν η τιμή του $\varepsilon$ φτάσει το 0, ο πράκτορας γίνεται «άπληστος» (*greedy*), δηλ. επιλέγει πάντα την βέλτιστη ενέργεια. Ο ρυθμός με τον οποίο μειώνεται η τιμή της πιθανότητας είναι συνήθως είτε γραμμικός είτε εκθετικός και εξαρτάται από τον αριθμό των καταστάσεων και των ενεργειών.

Θεωρητικά, σε άπειρο χρόνο εκπαίδευσης του αλγορίθμου $Q$-Learning, η πολιτική του πράκτορα θα αντιστοιχεί στη βέλτιστη [@QlearningConvergence]. Έτσι, δεν θα είχε τότε, νόημα η εξερεύνηση στο περιβάλλον. Ωστόσο, σε πραγματικά προβλήματα ενισχυτικής μάθησης , ο πράκτορας εκπαιδεύεται για πεπερασμένο αριθμό βημάτων, κι επομένως η τιμή $\varepsilon$ δεν τείνει ποτέ στο μηδέν. Αντίθετα, τίθεται ένα κατώτατο όριο, για παράδειγμα η τιμή 0.005. Με αυτόν τον τρόπο, εξασφαλίζεται ότι ο πράκτορας δεν θα σταματήσει ποτέ, πλήρως, την εξερεύνηση, μιας και η πολιτική του θα έχει πάντα περιθώρια βελτίωσης.

Δεν υπάρχει συγκεκριμένος κανόνας για τον καθορισμό του ρυθμού μείωσης του $\varepsilon$, ούτε για το κατώτατο όριο του. Αντίθετα, ορίζονται από τον σχεδιαστή του συστήματος, μετά από πειραματισμό και δοκιμές. Είναι σημαντική η προσεκτική ρύθμιση αυτών των παραμέτρων, διότι επηρεάζουν σημαντικά τη σύγκλιση του αλγορίθμου. 

Η μεθοδολογία του αλγορίθμου $Q$-Learning δίνεται παράκατω υπό μορφή ψευδοκώδικα (*pseudocode*).

![](3-theory/figures/Q-Learning-pseudocode.pdf){}
Συνολικά, η διαδικασία που ακολουθεί ο αλγόριθμος Q-Learning για την κατασκευή της συνάρτησης Q, θα γίνει καλύτερα κατανοητή με ένα παράδειγμα. Για αυτό, θα θεωρησούμε το παιχνίδι Frozen Lake της βιβλιοθήκης OpenAI Gymnasium[^Gym], που αποτελεί ένα κλασικό περιβάλλον εφαρμογής του αλγορίθμου Q-Learning. 

[^Gym]: Η βιβλιοθήκη OpenAI Gymnasium αποτελεί μία από τις πιο δημοφιλείς βιβλιοθήκες για την εφαρμογή αλγορίθμων ενισχυτικής μάθησης σε περιβάλλοντα προσομοίωσης. Περιλαμβάνει μία μεγάλη ποικιλία από περιβάλλοντα, όπως το Frozen Lake, το CartPole, παιχνίδια Atari κ.ά. Επιπλέον, παρέχει μία εύχρηστη διεπαφή (API) για την αλληλεπίδραση με τα περιβάλλοντα, καθώς και πολλές χρήσιμες συναρτήσεις για την εκπαίδευση των αλγορίθμων.

#### Παράδειγμα: Το παιχνίδι Frozen Lake {.unnumbered}

Στο παιχνίδι της παγωμένης λίμνης, η πίστα είναι ενα πλέγμα 4x4 και στόχος του πράκτορα είναι να φτάσει από την αρχή της πίστας (τετράγωνο πάνω αριστερά) στον στόχο του (τετράγωνο κάτω δεξιά), αποφεύγοντας τις τρύπες. Αν ο πράκτορας πέσει σε μία τρύπα, τότε το επεισόδιο τερματίζει και ο πράκτορας χάνει. Ο πράκτορας μπορεί να κινηθεί προς τα πάνω, κάτω, αριστερά και δεξιά. Ένα στιγμιότυπο του παιχνιδιού φαίνεται στην *Εικόνα @fig:theory:reinforcement_learning:frozen_lake*.

![Το παιχνίδι Frozen Lake της βιβλιοθήκης OpenAI Gymnasium.](3-theory/figures/FrozenLake.png){#fig:theory:reinforcement_learning:frozen_lake width=50%}

Μοντελοποιώντας το παιχνίδι σε πρόβλημα ενισχυτικής μάθησης, μπορούμε να θεωρήσουμε ως κατάσταση του περιβάλλοντος τη θέση του πράκτορα εντός του πλέγματος. Έτσι, υπάρχουν 16 δυνατές καταστάσεις. Επίσης, όπως αναφέραμε προηγουμένως, ο πράκτορας έχει 4 δυνατές ενέργειες. Η συνάρτηση ανταμοιβής μπορεί να εκφραστεί ως εξής: 

- +1 εάν ο πράκτορας φτάσει το στόχο
- -1 εάν ο πράκτορας πέσει σε τρύπα
- 0 σε όλες τις άλλες περιπτώσεις

Στην αρχή της εκπαίδευσης, οι τιμές του πίνακα $Q$ αρχικοποιούνται στο 0. Ο πράκτορας επιλέγει ενέργειες χρησιμοποιώντας την τεχνική $\varepsilon$-greedy και ενημερώνει σε κάθε βήμα την αντίστοιχη τιμή $Q$ του ζεύγους κατάστασης-ενέργειας, χρησιμοποιώντας την εξίσωση TD Update Rule. Μετά από 50000 επεισόδια, παρατηρείται πως το ποσοστό επιτυχίας του πράκτορα προσεγγίζει το 100%. Τότε, η εκπαίδευση σταματάει και ξεκινάει η αξιολόγηση του πράκτορα, όπου τίθεται η τιμή 0 στην παράμετρο $\varepsilon$ , ώστε να επιλέγει ο πράκτορας πάντα την βέλτιστη ενέργεια, σύμφωνα με την πολιτική του. Στην *Εικόνα @fig:theory:reinforcement_learning:frozen_lake_results* παρουσιάζεται η πολιτική που ανέπτυξε ο πράκτορας (αριστερά), καθώς και η τελική μορφή του πίνακα $Q$ (δεξιά).

![Αποτελέσματα εκπαίδευσης στο παιχνίδι Frozen Lake [@otagoCOSC470].](3-theory/figures/Q-Table-FrozenLake.png){#fig:theory:reinforcement_learning:frozen_lake_results width=90%}

Τα βέλη στο πλέγμα αριστερά αντιστοιχούν στην ενέργεια που επιλέγει στην αντίστοιχη κατάσταση ο πράκτορας. Όπως αποδεικνύεται κι από τον πίνακα δεξιά, η ενέργεια που επιλέγεται κάθε φορά είναι αυτή με τη μεγαλύτερη τιμή $Q$. Ακόμα, αξίζει να σημειωθεί πως οι μόνες καταστάσεις με μηδενικές τιμές $Q$ είναι οι τερματικές καταστάσεις, αφού σε αυτές ο πράκτορας δεν εκτελεί καμία ενέργεια, καθώς το παιχνίδι τελειώνει. Τελικά, η διαδρομή που σχηματίζεται από τις επιλεγμένες ενέργειες, οδηγεί τον πράκτορα από την αφετηρία στον στόχο, κι έτσι η εκπαίδευση θεωρείται επιτυχής. 

#### Αδυναμίες και πιθανές λύσεις {.unnumbered}

Ο αλγόριθμος $Q$-Learning αποτελεί έναν από τους θεμελιώδεις και πιο γνωστούς αλγορίθμους στον χώρο της ενισχυτικής μάθησης. Παρόλα αυτά, η χρήση του τα τελευταία χρόνια έχει μειωθεί σημαντικά, κάτι το οποίο οφείλεται στις σημαντικές αδυναμίες που εμφανίζει. Ωστόσο, πριν αναλύσουμε αυτές, ας επισημάνουμε πρώτα τα πλεονεκτήματά του αλγορίθμου, τα οποία τον κατέστησαν τόσο δημοφιλή:

- **Εύκολη υλοποίηση**: Η απλή φύση του αλγορίθμου καθιστά την υλοποίησή του αρκετά εύκολη, χωρίς να χρειάζεται η χρήση κάποιας βιβλιοθήκης μηχανικής μάθησης.
- **Μικρές απαιτήσεις σε υπολογιστικούς πόρους**: μπορεί να εκτελεστεί σε συστήματα με μικρή υπολογιστική ισχύ, σε αντίθεση με τους πιο μοντέρνους αλγορίθμους, που συνηθώς απαιτούν ειδικό πόρους όπως εξελιγμένες μονάδες επεξεργασίας γραφικών υπολογισμών (GPU).	
- Κατάλληλος για προβλήματα με **μακροπρόθεσμα αποτελέσματα**:  Χάρη στην ενημέρωση των τιμών $Q$ με την εξίσωση TD Update Rule, ο αλγόριθμος είναι ικανός να αντιμετωπίσει προβλήματα με μακροπρόθεσμα αποτελέσματα, τα οποία είναι ιδιαίτερα απαιτητικά.

Στη συνέχεια, ας εξετάσουμε τις σημαντικότερες αδυναμίες του αλγορίθμου $Q$-Learning:

- Ακατάλληλος για **μεγάλους χώρους καταστάσεων και ενεργειών**: Σε προβλήματα με συνεχή χώρο καταστάσεων ή και ενεργειών, ο αλγόριθμος Q-Learning είναι πρακτικά, ανεφάρμοστος. Αυτό συμβαίνει, διότι ο αλγόριθμος απαιτεί την αποθήκευση των τιμών $Q$ σε έναν πίνακα. Σε περιβάλλοντα όμως με μεγάλους χώρους καταστάσεων και ενεργειών, το πλήθος των διαθέσιμων ζευγών κατάστασης-ενέργειας είναι τεράστιο, με αποτέλεσμα ο πίνακας $Q$ να γίνεται υπερβολικά μεγάλος και να απαιτεί μεγάλα ποσά μνήμης. Τα προβλήματα αυτά, που εμφανίζονται σε χώρους μεγάλων καταστάσεων είναι γνωστά και με το όνομα *κατάρα της διάστασης* (*curse of dimensionality*), ένας όρος που δόθηκε από τον Richard E. Bellman. 
- **Έλλειψη γενίκευσης**: Η απλή μέθοδος Q-Learning δεν είναι ικανή να γενικεύσει τη γνώση που αποκτά. Με άλλα λόγια, ο πράκτορας δεν μπορεί να πάρει αποφάσεις για καταστάσεις που δεν έχει συναντήσει ξανά, ανεξάρτητα από την ομοιότητα της νέας αυτής κατάστασης με άλλες που έχει ήδη συναντήσει. Επομένως, σε προβλήματα με μεγάλο χώρο καταστάσεων, ακόμα και αν είχαμε τους απαραίτητους υπολογιστικούς πόρους για την αποθήκευση του πίνακα $Q$, ο χρόνος που θα χρειαζόταν για να εξερευνήσει ο πράκτορας κάθε ένα ζεύγος κατάστασης-ενέργειας θα ήταν απαγορευτικά μεγάλος. Τελικά, η αδυναμία γενίκευσης της γνώσης αποτελεί άλλον έναν παράγοντα για τον οποίο δεν είναι εφικτή η αποτελεσματική λειτουργία του αλγορίθμου σε πραγματικά προβλήματα.

Γίνεται πλέον κατανοητό, ότι η χρήση του αλγορίθμου $Q$-Learning σε πραγματικά προβλήματα είναι περιορισμένη, λόγω των παραπάνω αδυναμιών. Ωστόσο, υπάρχουν κάποιες προτάσεις για την αντιμετώπιση των προβλημάτων του αλγορίθμου. Οι κυριότερες από αυτές -και οι οποίες δοκιμάστηκαν στο περιβάλλον αυτόματης στάθμευσης- είναι οι εξής:

- **Διακριτοποίηση**: Μία τεχνική για τη μείωση του χώρου καταστάσεων και ενεργειών είναι η διακριτοποίηση των πιθανών τιμών. Για παράδειγμα, στο περιβάλλον αυτόματης στάθμευσης, αν η ταχύτητα του αυτοκινητού παίρνει συνεχείς τιμές στο διάστημα [0, 100], μπορεί να κβαντοποιηθεί στις τιμές [0, 1, 2, 3, 4], όπου το 0 αντιστοιχεί στην τιμή 0-20, το 1 στην τιμή 21-40 κ.ο.κ. Αν πραγματοποιηθεί αυτή η διαδικασία σε κάθε ένα χαρακτηριστικό του χώρου καταστάσεων, τότε το πλήθος των στοιχείων του πίνακα $Q$ μειώνεται σημαντικά. Έτσι, μπορεί να γίνει εφικτή η εφαρμογή του αλγορίθμου $Q$-Learning. Βέβαια, αν η διακριτοποίηση γίνει σε υπερβολικό βάθμο, μπορεί να προκαλέσει την έλλειψη ακρίβειας στην αναπαράσταση των καταστάσεων και να οδηγήσει σε χαμηλές επιδόσεις του πράκτορα.

- **Προσέγγιση συνάρτησης**: Μία άλλη τεχνική για την αντιμετώπιση του προβλήματος της μεγάλης διάστασης του πίνακα $Q$ είναι η χρήση συναρτήσεων προσέγγισης. Συγκεκριμένα, μπορούν να χρησιμοποιηθούν νευρωνικά δίκτυα για να προσεγγίζουν τη συνάρτηση $Q(s, a)$ και να εκτιμούν τις τιμές $Q$, χωρίς να χρειάζεται αυτές να αποθηκεύονται σε πίνακα. Έτσι, τα νευρωνικά δίκτυα είναι ικανά να γενικεύουν τη γνώση που αποκτούν, δηλ. να παίρνουν αποφάσεις για καταστάσεις που δεν έχουν συναντήσει ξανά αυτούσιες, κάτι που ο απλός πίνακας $Q$ δεν μπορεί να κάνει. Η χρήση νευρωνικών δικτύων στον αλγόριθμο $Q$-Learning οδηγεί στη δημιουργία του αλγορίθμου Deep Q-Network (DQN). Ο αλγόριθμος αυτός αποτελεί μία από τις πιο δημοφιλείς εκδοχές του αλγορίθμου $Q$-Learning, καθώς είναι ικανός να αντιμετωπίσει προβλήματα με μεγάλους, συνεχείς χώρους καταστάσεων.

Η μέθοδος της προσέγγισης συνάρτησης αποτελεί σήμερα, την πιο συνηθισμένη λύση στο πρόβλημα της διαστασιμότητας. Με τον τρόπο αυτό, εισερχόμαστε στο πεδίο της Βαθίας Ενισχυτικής Μάθησης, το οποίο περιγράφεται στην επόμενη ενότητα.