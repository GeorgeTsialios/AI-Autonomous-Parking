## Βαθιά Ενισχυτική Μάθηση {#sec:theory:deep_reinforcement_learning}

### Ορισμός και Χαρακτηριστικά {#sec:theory:deep_reinforcement_learning:definition}

Τα προβλήματα της σύγχρονης εποχής, χαρακτηρίζονται από υψηλή πολυπλοκότητα και μεγάλο πλήθος διαφορετικών καταστάσεων και
ενεργειών. Οι μεγάλες διαστάσεις των προβλημάτων αυτών, έχουν θέσει σημαντικούς περιορισμούς στην εφαρμογή των αλγορίθμων της ενισχυτικής μάθησης. Συγκεκριμένα, σε τέτοιες περιπτώσεις, η εκπαίδευση και η ικανοποιητική εξερεύνηση του περιβάλλοντος καθίσταται χρονοβόρα, κοστοβόρα και εν τέλει, απαγορευτική από παραδοσιακούς αλγορίθμους ενισχυτικής μάθησης.

Η λύση δίνεται μέσω της μίμησης ενός βιολογικού μηχανισμού, των νευρωνικών δικτύων, και της ενσωμάτωσης τους στους αλγορίθμους ενισχυτικής μάθησης. Έτσι, δημιουργήθηκε ένα νέο επιστημονικό πεδίο, αυτό της Βαθιάς Ενισχυτικής Μάθησης. Επομένως, η Βαθιά Ενισχυτική Μάθηση (*Deep Reinforcement Learning*) αποτελεί απλά την ενοποίηση της Βαθιάς Μάθησης (*Deep Learning* - χρήση βαθιών νευρωνικών δικτύων) με την Ενισχυτική Μάθηση (*Reinforcement Learning*). 

Μία σύγκριση της κλασικής ενισχυτικής μάθησης με τη βαθιά ενισχυτική μάθηση παρουσιάζεται στην *Εικόνα @fig:theory:deep_reinforcement_learning:DQN*, για την περίπτωση του αλγορίθμου $Q$-Learning. 

![Κλασική Ενισχυτική Μάθηση εναντίον Βαθιάς Ενισχυτικής Μάθησης [@baeldungQLearningDeep].](3-theory/figures/QLearningVsDQN.png){#fig:theory:deep_reinforcement_learning:DQN width=100%}

Παρατηρώντας την *Εικόνα @fig:theory:deep_reinforcement_learning:DQN*, γίνεται σαφής η λογική της βαθιάς ενισχυτικής μάθησης καθώς και τα πλεονεκτήματα που προκύπτουν από τη χρήση νευρωνικών δικτύων. Συγκεκριμένα, στον κλασικό αλγόριθμο Q-Learning, όλες οι τιμές $Q$ των ζευγών κατάστασης ενέργειας αποθηκεύονται σε έναν πίνακα. Ο πράκτορας ανατρέχει στον πίνακα αυτό, για να επιλέξει σε κάθε κατάσταση, την ενέργεια με τη μεγαλύτερη αξία. Αντίθετα, ο αλγόριθμος βαθιάς ενισχυτικής μάθησης DQN, αντικαθιστά τον προηγούμενο πίνακα με ένα βαθύ νευρωνικό δίκτυο. Το δίκτυο αυτό, εκτιμά σε κάθε κατάσταση, τις τιμές $Q$ για όλες τις δυνατές ενέργειες, ώστε να επιλέξει ο πράκτορας τη βέλτιστη ενέργεια. Επομένως, αποφεύγεται η αποθήκευση των τιμών $Q$, ενώ επιτυγχάνεται η γενίκευση σε άγνωστες καταστάσεις.

Τα τεχνητά νευρωνικά δίκτυα αποτελούν το αντικείμενο της επόμενης υποενότητας, όπου αρχικά συγκρίνονται με τα βιολογικά νευρωνικά δίκτυα, ενώ στη συνέχεια παρουσιάζονται οι βασικές αρχές λειτουργίας τους.

### Τεχνητά Νευρωνικά Δίκτυα {#sec:theory:deep_reinforcement_learning:ANNs}

Τα Τεχνητά Νευρωνικά Δίκτυα (*Artificial Neural Networks - ANNs*) είναι υπολογιστικά μοντέλα εμπνευσμένα από τη δομή και τη λειτουργία των βιολογικών νευρωνικών δικτύων που βρίσκονται στον ανθρώπινο εγκέφαλο. Αποτελούνται από συνδεδεμένα επίπεδα κόμβων (ή *νευρώνων*), που συνεργάζονται για να επεξεργαστούν και να μάθουν από τα δεδομένα.

#### Σύγκριση με Βιολογικά Νευρωνικά Δίκτυα {.unnumbered}

Όπως προαναφέρθηκε, τα τεχνητά νευρωνικά δικτύα είναι σχεδιασμένα, ώστε να μιμούνται τη λειτουργία των βιολογικών νευρωνικών δικτύων. Προκειμένου να κατανοήσουμε πώς λειτουργούν τα τεχνητά νευρωνικά δίκτυα, ας εξετάσουμε πρώτα πώς λειτουργούν τα βιολογικά νευρωνικά δίκτυα. Ο ανθρώπινος εγκέφαλος αποτελείται από δισεκατομμύρια κύτταρα που ονομάζονται νευρώνες, οι οποίοι επικοινωνούν μεταξύ τους, μέσω ηλεκτρικών και χημικών σημάτων. Η *Εικόνα @fig:theory:deep_reinforcement_learning:BiologicalVsArtificialNN* δείχνει μια σύγκριση μεταξύ ενός βιολογικού νευρώνα και ενός τεχνητού νευρώνα, επισημαίνοντας τα δομικά τους στοιχεία.

![Σύγκριση βιολογικού και τεχνητού νευρώνα [@karpathy2016neural].](3-theory/figures/BiologicalVsArtificialNN.png){#fig:theory:deep_reinforcement_learning:BiologicalVsArtificialNN width=65%}

$\\$

Από το πάνω μέρος της εικόνας *@fig:theory:deep_reinforcement_learning:BiologicalVsArtificialNN*, παρατηρούμε πως ένας βιολογικός νευρώνας αποτελείται από τέσσερις κύριες μονάδες: τους δενδρίτες, το σώμα, τους άξονες και τις συνάψεις. Οι δενδρίτες λαμβάνουν τα σήματα εισόδου και τα μεταφέρουν στο σώμα, όπου επεξεργάζονται. Έπειτα, οι άξονες μεταφέρουν το επεξεργασμένο σήμα σε άλλους νευρώνες μέσω συνάψεων. Οι συνάψεις λειτουργούν ως σύνδεσμοι ανάμεσα στους νευρώνες.

Στον τεχνητό νευρώνα, παρατηρούμε πως οι είσοδοι $x_1, x_2, ..., x_n$ παίζουν το ρόλο των δενδριτών, ενώ τα βάρη $w_1, w_2, ..., w_n$ αντιστοιχούν στις συνάψεις. Ο υπολογισμός του σταθμισμένου αθροίσματος $z = \sum_{i=1}^{n} w_i x_i$ αντιστοιχεί στην επεξεργασία του σήματος στο σώμα του νευρώνα. Τέλος, η έξοδος $y$ αντιστοιχεί στο σήμα που μεταφέρεται μέσω των αξόνων. Οι έννοιες αυτές αναλύονται λεπτομερώς στη συνέχεια.

$\\$
$\\$
$\\$

#### Αρχές Λειτουργίας {.unnumbered}

**Νευρώνας**

Ας ξεκινήσουμε εξετάζοντας τη λειτουργία ενός μεμονωμένου νευρώνα, σε ένα τεχνητό νευρωνικό δίκτυο. Κάθε νευρώνας λαμβάνει εισόδους από άλλους νευρώνες, οι οποίες συμβολίζονται ως $x_i$. Κάθε είσοδος πολλαπλασιάζεται με έναν συντελεστή, που ονομάζεται **βάρος** (*weight*) και συμβολίζεται ως $w_i$. Αξίζει να σημειωθεί εδώ, πως τα βάρη παίζουν κρίσιμο ρόλο στη λειτουργία του νευρωνικού δικτύου. Συγκεκριμένα, μεγάλα βάρη υποδηλώνουν ότι οι συγκεκριμένες μεταβλητές είναι πιο σημαντικές, για την τελική απόφαση του δικτύου. Επομένως, τα βάρη αυτά, πρέπει να προσαρμοστούν κατά την εκπαίδευση του δικτύου, ώστε να αντικατοπτρίζουν την πραγματική σημασία των μεταβλητών.

Στη συνέχεια, υπολογίζεται το σταθμισμένο άθροισμα ολων των εισόδων $z = \sum_{i=1}^{n} w_i x_i$. Στο άθροισμα αυτό, προστίθεται κι ένας άλλος παράγοντας, ο οποίος ονομάζεται πόλωση (***bias***) και συμβολίζεται ως  $b$. Ο παράγοντας αυτός, αποτελεί μία σταθερά και χρησιμοποιείται για την οριζόντια μετατόπιση της εξόδου του νευρώνα. Με αυτόν τον τρόπο, το bias βελτιώνει την προσαρμογή του μοντέλου στα δεδομένα και την ακρίβεια του.
 
Τέλος, το προηγούμενο αποτέλεσμα περνάει από μια συνάρτηση ενεργοποίησης, για να προκύψει η τελική έξοδος του νευρώνα. Η **συνάρτηση ενεργοποίησης** (*activation function*) είναι μία μαθηματική συνάρτηση, η οποία συμβολίζεται ως $f$ και εφαρμόζεται στην έξοδο του νευρώνα, πριν όμως αυτή περάσει στο επόμενο επίπεδο του δικτύου. Η συνάρτηση ενεργοποίησης αποφασίζει αν η έξοδος είναι αρκετά σημαντική, για να περάσει στο επόμενο επίπεδο. Συγκεκριμένα, αν η έξοδος υπερβαίνει ένα καθορισμένο κατώφλι, τότε θα περάσει και λέμε ότι ο νευρώνας «ενεργοποιείται». Οι συναρτήσεις ενεργοποίησης αποτελούν ένα πολύ σημαντικό στοιχείο των νευρωνικών δικτύων, για δύο λόγους. Πρώτον, εισάγουν μη γραμμικότητα (*non linearity*) στο δίκτυο, η οποία είναι εξαιρετικά σημαντική, καθώς χάρη σε αυτήν, καθίσταται δυνατή η επίλυση πολύπλοκων προβλημάτων από τα νευρωνικά δίκτυα. Δεύτερον, ελέγχουν το εύρος των εξόδων των νευρώνων, το οποίο μπορεί να είναι κρίσιμο για τη σταθεροποίηση και την επιτάχυνση της διαδικασίας μάθησης. Υπάρχουν διάφορες συναρτήσεις ενεργοποίησης, με διαφορετικά χαρακτηριστικά η κάθε μία. Η επιλογή της συνάρτησης ενεργοποίησης εξαρτάται από παράγοντες όπως οι απαιτήσεις μάθησης του δικτύου και οι ιδιαιτερότητες του εκάστοτε προβλήματος. Στην πράξη, η επιλογή γίνεται συνήθως έπειτα από δοκιμές και πειραματισμούς, καθώς διαφορετικές συναρτήσεις ενεργοποίησης μπορεί να επιδρούν διαφορετικά στην απόδοση του δικτύου. Δύο από τις πιο διάσημες συναρτήσεις ενεργοποίησης -και οι οποίες χρησιμοποιήθηκαν στο πρόβλημα της αυτόματης στάθμευσης- παρουσιάζονται παρακάτω:

- Συνάρτηση **Tanh**: Η συνάρτηση υπερβολικής εφαπτομένης (*Hyperbolic Tangent - Tanh*) ορίζεται από την εξίσωση \ref{eq:theory:deep_reinforcement_learning:Tanh} και σχεδιάζεται στην *Εικόνα @fig:theory:deep_reinforcement_learning:Tanh*.
\begin{equation}
f(x) = \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}, \text{όπου x είναι η έξοδος του νευρώνα}
\label{eq:theory:deep_reinforcement_learning:Tanh}
\end{equation}

![Γραφική παράσταση της συνάρτησης Tanh.](3-theory/figures/Tanh.pdf){#fig:theory:deep_reinforcement_learning:Tanh width=75%}

Παρατηρούμε πως το εύρος εξόδου της συνάρτησης Tanh είναι $[-1, 1]$. Έτσι, η έξοδος της Tanh είναι συμμετρική, με κέντρο το 0. Αυτό συμβάλει στην επιτάχυνση της σύγκλισης του αλγορίθμου κατά την εκπαίδευση. Ακόμα, χάρη σε αυτήν την ίδιότητα της, όταν τα δεδομένα εισόδου είναι κανονικοποιημένα ώστε να έχουν μέση τιμή 0, η εκπαίδευση μπορεί να γίνει πιο αποδοτική. Το βασικό μειονέκτημα της Tanh είναι το πρόβλημα της εξαφάνισης της κλίσης (*vanishing gradients*). Το πρόβλημα αυτό, εμφανίζεται όταν η είσοδος της συνάρτησης γίνεται πολύ μικρή ή πολύ μεγάλη. Τότε, η κλίση της συνάρτησης προσεγγίζει το 0. Αυτό οδηγεί σε πολύ μικρές αλλαγές στα βάρη κατά την εκπαίδευση, κι επομένως η μάθηση του δικτύου γίνεται πολύ αργή ή σταματά.

- Συνάρτηση **ReLU**: Η συνάρτηση Ανορθωμένης Γραμμικής Μονάδας (*Rectified Linear Unit - ReLU*) ορίζεται από την εξίσωση \ref{eq:theory:deep_reinforcement_learning:ReLU} και σχεδιάζεται στην *Εικόνα @fig:theory:deep_reinforcement_learning:ReLU*.
\begin{equation}
f(x) = \max(0, x), \text{όπου x είναι η έξοδος του νευρώνα}
\label{eq:theory:deep_reinforcement_learning:ReLU}
\end{equation}

![Γραφική παράσταση της συνάρτησης ReLU.](3-theory/figures/ReLU.pdf){#fig:theory:deep_reinforcement_learning:ReLU width=75%}

Παρατηρούμε πως το εύρος εξόδου της συνάρτησης ReLU είναι $[0, \infty)$. Άρα, δεν υπάρχει ανώτατο όριο για την έξοδο της συνάρτησης κι επομένως, η κανονικοποίηση των δεδομένων εισόδου, είναι ξανά μία καλή πρακτική. Η συνάρτηση ReLU αποτελεί την πιο ευρέως χρησιμοποιούμενη συνάρτηση ενεργοποίησης. Είναι δημοφιλής για τη μικρή απαιτούμενη υπολογιστική ισχύ της, για την επιτάχυνση της σύγκλισης και την αποφυγή του προβλήματος της εξαφάνισης της κλίσης. Ωστόσο, έχει κάποια μειονεκτήματα, με κυριότερο την τάση νεκρών νευρώνων (*dying ReLU problem*), όπου κάποιοι νευρώνες σταματούν να ανταποκρίνονται σε οποιαδήποτε είσοδο και παραμένουν ανενεργοί.

**Δίκτυο Νευρώνων**

Έχοντας μελετήσει τη λειτουργία ενός μεμονωμένου νευρώνα, ας εξετάσουμε τη συνεργασία πολλών μαζί, για τη δημιουργία ενός τεχνητού νευρωνικού δικτύου, όπως αυτό που φαίνεται στην *Εικόνα @fig:theory:deep_reinforcement_learning:NeuralNetwork*.

![Παράδειγμα τεχνητού νευρωνικού δικτύου [@comsolDeepNeural].](3-theory/figures/DeepNeuralNetwork.png){#fig:theory:deep_reinforcement_learning:NeuralNetwork width=90%}

Παρατηρούμε πως τα νευρωνικά δίκτυα αποτελούνται από πολλά επίπεδα (ή στρώματα) νευρώνων, τα οποία συνδέονται μεταξύ τους. Το πρώτο επίπεδο, ονομάζεται **επίπεδο εισόδου** (*input layer*) και λαμβάνει τα δεδομένα εισόδου του δικτύου. Τα δεδομένα αυτά, στη συνέχεια μεταφέρονται μέσα από μία σειρά επιπέδων που ονομάζονται **ενδιάμεσα επίπεδα** (*hidden layers*) και είναι υπεύθυνα για την επεξεργασία τους και την εξαγωγή χαρακτηριστικών από αυτά. Τέλος, τα δεδομένα φτάνουν στο τελευταίο επίπεδο, το οποίο ονομάζεται **επίπεδο εξόδου** (*output layer*) και παράγει την έξοδο του δικτύου. 

Ανάλογα με την πολυπλοκότητα του προβλήματος, ένα νευρωνικό δίκτυο μπορεί να αποτελείται από ένα ή περισσότερα ενδιάμεσα επίπεδα. Όταν το δίκτυο αποτελείται από δύο ή περισσότερα ενδιάμεσα επίπεδα, τότε ονομάζεται βαθύ νευρωνικό δίκτυο (*deep neural network*). Αντίστοιχα, το κάθε επίπεδο μπορεί να αποτελείται από λίγους έως και εκατομμύρια νευρώνες.

Ακόμα, ένα νευρωνικό δίκτυο μπορεί να διαφέρει στον τρόπο με τον οποίο συνδέονται οι νευρώνες του. Η πιο διαδεδομένη **τοπολογία**, είναι αυτή των πλήρως συνδεδεμένων επιπέδων (*fully connected layers*).  Σε αυτήν, ο κάθε νευρώνας ενός επιπέδου συνδέεται με όλους τους νευρώνες του επόμενου επιπέδου. 

#### Εκπαίδευση {.unnumbered}

Τα τεχνητά νευρωνικά δίκτυα μαθαίνουν μέσω της προσαρμογής των βαρών και των πολώσεων τους, κατά τη διάρκεια της εκπαίδευσης. Ας εξετάσουμε τη διαδικασία αυτή βήμα προς βήμα, αναλύοντας παράλληλα ορισμένες σημαντικές έννοιες και παραμέτρους.

Στο *1^ο^ βήμα* της εκπαίδευσης γίνεται η αρχικοποίηση των βαρών, δηλαδή η ανάθεση τυχαίων τιμών σε αυτά.

Στο *2^ο^ βήμα* της εκπαίδευσης λαμβάνει χώρα η εμπρόσθια διάδοση, κατά την οποία τα δεδομένα εισόδου περνάνε μέσα από το δίκτυο, για να παραχθεί η έξοδος του.

Στο *3^ο^ βήμα* της εκπαίδευσης, υπολογίζεται το σφάλμα της εξόδου, δηλαδή η διαφορά της εξόδου του δικτύου από την επιθυμητή τιμή της εξόδου[^desired-output]. Ο υπολογισμός γίνεται από τη **συνάρτηση σφάλματος** (*loss function*) και στόχος είναι, μέσα από την εκπαίδευση, να ελαχιστοποιηθεί αυτό το σφάλμα. Μερικές από τις πιο διάσημες συναρτήσεις σφάλματος είναι η συνάρτηση Μέσης Τετραγωνικής Απόκλισης (*Mean Squared Error - MSE*), η συνάρτηση Μέσου Απόλυτου Σφάλματος (*Mean Absolute Error - MAE*) και η συνάρτηση σφάλματος Εντροπίας (*Cross-Entropy*). Η επιλογή της συνάρτησης σφάλματος εξαρτάται από τον τύπο του προβλήματος.

[^desired-output]: σε προβλήματα επιβλεπόμενης μάθησης, η επιθυμητή τιμή της εξόδου είναι γνωστή για τα δεδομένα εκπαίδευσης. Σε προβλήματα ενισχυτικής μάθησης, ο υπολογισμός του σφάλματος γίνεται με βάση την ανταμοιβή που λαμβάνει το δίκτυο από το περιβάλλον. Περισσότερες λεπτομέρειες στην παράγραφο «*Σύνδεση με την Ενισχυτική Μάθηση*».

Στο *4^ο^ βήμα* της εκπαίδευσης λαμβάνει χώρα η **οπισθοδρόμικη διάδοση** του σφάλματος (*backward propagation* ή *backpropagation*). Κατά τη συγκεκριμένη διαδικασία, υπολογίζεται η κλίση (δηλαδή η μερική παράγωγος) της συνάρτησης σφάλματος ως προς κάθε βάρος του δικτύου. Ο υπολογισμός αυτός γίνεται προς τα πίσω, δηλαδή από το επίπεδο εξόδου προς το επίπεδο εισόδου και αντικατοπτρίζει την επίδραση του κάθε βάρους, στο σφάλμα της εξόδου. Αξίζει να σημειωθεί πως, από τεχνικής άποψης,  η οπισθοδρόμηση αποτελεί απλώς τη μέθοδο για τον αποτελεσματικό υπολογισμό της κλίσης της συνάρτησης σφάλματος και δεν έχει σχέση με το πως χρησιμοποιείται αυτή στη συνέχεια. Ωστόσο, συχνά, αυτός ο όρος χρησιμοποιείται ελαστικά, για να περιγράψει τη συνολική διαδικασία μάθησης. 

Στο *5^ο^ βήμα* της εκπαίδευσης, γίνεται η ενημέρωση των βαρών του δικτύου. Η ενημέρωση των βαρών γίνεται σύμφωνα με τις υπολογισμένες μερικές παραγώγους, προς την κατεύθυνση που μειώνει το σφάλμα και πραγματοποιείται από έναν **αλγόριθμο βελτιστοποίησης** (*optimization algorithm*).  Οι πιο συνηθισμένοι αλγόριθμοι βελτιστοποίησης είναι:

- Ο αλγόριθμος **Κατάβασης Πλαγιάς** (*Gradient Descent*): είναι ο πιο απλός αλγόριθμος βελτιστοποίησης και παρουσιάζει προβλήματα όπως η αργή σύγκλιση και η πιθανότητα να παγιδευτεί σε τοπικά ακρότατα. Αποτέλεσε τη βάση για την ανάπτυξη πιο προηγμένων αλγορίθμων, ενώ υπάρχουν και πολλές παραλλαγές του, όπως ο αλγόριθμος Στοχαστικής Κατάβασης Πλαγιάς (*Stochastic Gradient Descent - SGD*) και ο αλγόριθμος Κατάβασης Πλαγιάς Τεμαχισμένου Ρυθμού (*Mini-batch Gradient Descent*).

- Ο αλγόριθμος **Ορμής** (*Momentum*): αποτελεί επέκταση του SGD, προσθέτοντας έναν όρο «ορμής», δηλαδή ένα τμήμα της προηγούμενης ενημέρωσης στην τρέχουσα ενημέρωση. Βοηθάει στην επιτάχυνση της σύγκλισης και τη μείωση των ταλαντώσεων.

- Ο αλγόριθμος **Διάδοσης Μέσης Τετραγωνικής Απόκλισης** (*Root Mean Square Propagation - RMSprop*): προσαρμόζει τον ρυθμό μάθησης για κάθε παράμετρο ξεχωριστά και κανονικοποιεί το βήμα της ενημέρωσης. Αυτό βοηθάει στην αντιμέτωπιση του προβλήματος της εξαφάνισης της κλίσης (*vanishing gradients*).

- Ο αλγόριθμος **Αδάμ** (*Adam*): συνδυάζει τα πλεονεκτήματα των αλγορίθμων Momentum και RMSprop. Προσαρμόζει το ρυθμό μάθησης για κάθε παράμετρο ξεχωριστά, βασιζόμενος σε προηγούμενες κλίσεις. Είναι πολύ αποδοτικός, καθώς επιτυγχάνει ταχεία σύγκλιση και απαιτεί μικρή ποσότητα μνήμης. Στην πράξη, οι επιδόσεις του Adam συχνά ξεπερνούν τους υπόλοιπους αλγορίθμους βελτιστοποίησης και για αυτό, αποτελεί μία από τις πιο δημοφιλείς επιλογές σήμερα. 

Μία σημαντική παράμετρος των αλγορίθμων βελτιστοποίησης είναι ο **ρυθμός μάθησης** (*learning rate*), ο οποίος καθορίζει το πόσο γρήγορα τα βάρη του δικτύου προσαρμόζονται κατά τη διάρκεια της εκπαίδευσης. Η προσαρμογή του ρυθμού μάθησης είναι κρίσιμη για να επιτευχθεί ισορροπία μεταξύ της ταχύτητας σύγκλισης και της αποφυγής της υπερπήδησης (*overshooting*) της βέλτιστης λύσης.

Επίσης, αξίζει να σημειωθεί, πως στα πλαίσια αυτής της εργασίας, έγινε η επιλογή του αλγορίθμου βελτιστοποίησης Adam, χάρη στην ικανότητα του να επιτυγχάνει καλά αποτελέσματα, γρήγορα.

Τέλος, τα βήματα 2-5 επαναλαμβάνονται για κάθε τεμάχιο (*batch*) των δεδομένων εκπαίδευσης. Ένα πλήρες πέρασμα όλων των δεδομένων εκπαίδευσης ονομάζεται εποχή (*epoch*). Μπορεί να χρειαστούν πολλές εποχές εκπαίδευσης, ώστε να επιτευχθεί η επιθυμητή απόδοση του δικτύου.

#### Υπερ-παράμετροι {.unnumbered}

Οι παράμετροι του νευρωνικού δίκτυου που ορίζονται πριν την εκπαίδευση, από τον σχεδιαστή του, ονομάζονται υπερ-παράμετροι (*hyperparameters*). Οι παράμετροι αυτοί, έχουν πολύ μεγάλη επίδραση στην τελική αποτελεσματικότητα του δικτύου κι επομένως, επιβάλλεται η προσεκτική επιλογή και ρύθμιση τους. Δεν υπάρχουν συγκεκριμένοι κανόνες που να ορίζουν την επιλογή των υπερ-παραμέτρων, κι έτσι αυτή πρέπει να γίνει μέσω δοκιμών και κατάλληλων ρυθμίσεων. Αξίζει να σημειωθεί, πως οι αλλαγές των υπερ-παραμέτρων δεν μπορούν να λάβουν χώρα κατά τη διάρκεια της εκπαίδευσης. Έτσι, αν κριθεί απαραίτητη η αλλαγή κάποιας παραμέτρου του δικτύου, η εκπαίδευση πρέπει να επαναληφθεί από την αρχή. Μερικές από τις βασικότερες υπερ-παραμέτρους ενός νευρωνικού δικτύου είναι:

- Η **αρχιτεκτονική** του δικτύου. Ο όρος «αρχιτεκτονική» ή «δομή» του δικτύου περιλαμβάνει τον αριθμό των επιπέδων, το πλήθος των νευρώνων σε κάθε επίπεδο καθώς και τη συνδεσμολογία μεταξύ τους. Συνήθως, η αύξηση της πολυπλοκότητας του δικτύου, δηλαδή του πλήθους των επιπέδων και των νευρώνων, βελτιώνει την ικανότητα του να μαθαίνει σύνθετα μοτίβα δεδομένων και αυξάνει έτσι την απόδοση του. Ωστόσο, αυξάνοντας την πολυπλοκότητα του δικτύου, αυξάνεται και ο κίνδυνος της υπερ-προσαρμογής του στα δεδομένα εκπαίδευσης (*overfitting*), με αποτέλεσμα να μην γενικεύει καλά σε νέα δεδομένα [@eitcaDoesIncreasing]. 
- Η **συνάρτηση ενεργοποίησης**:  Η επιλογή της συνάρτησης ενεργοποίησης επηρεάζει την ικανότητα του δικτύου να μάθει και την ταχύτητα της μάθησης.
- O **ρυθμός μάθησης**:  Η επιλογή ενός υψηλού ρυθμού μάθησης μπορεί να οδηγήσει σε αστάθεια τη διαδικασία εκπαίδευσης, ενώ από την άλλη, ένας χαμηλός ρυθμός μάθησης μπορεί να καθυστερήσει τη σύγκλιση του δικτύου.
- O **αλγόριθμος βελτιστοποίησης**: Η επιλογή του αλγορίθμου βελτιστοποίησης επηρεάζει την ταχύτητα σύγκλισης και την σταθερότητα της εκπαίδευσης.

Προφανώς, η λίστα αυτή δεν είναι εξαντλητική. Υπάρχουν πολλές ακόμα υπερ-παράμετροι στο εσωτερικό των δικτύων αυτών και κάποιος μπορεί να αναλύσει τη λειτουργία τους, σε πολύ μεγαλύτερο βάθος και λεπτομέρεια. Ωστόσο, οι παραπάνω παράμετροι αποτελούν αφενός μία καλή αφετηρία για έναν αρχάριο στο αντικείμενο και αφετέρου, θεωρούμε πως με αυτές αξίζει πρώτα, να πειραματιστεί κάποιος.
 
#### Κατηγορίες {.unnumbered}

Υπάρχουν διαφορετικοί τύποι νευρωνικών δικτύων, οι οποίοι διακρίνονται από την αρχιτεκτονική τους και από τον τύπο προβλημάτων που είναι σχεδιασμένοι να επιλύσουν. Οι πιο κοινοί τύποι νευρωνικών δικτύων σήμερα είναι οι εξής:

- Τα **Νευρωνικά Δίκτυα Πρόσθιας Διάδοσης** (*Feedforward Neural Networks*): Αποτελούν την πιο βασική μορφή νευρωνικών δικτύων. Όπως γίνεται φανερό και από το όνομα τους, χαρακτηριστικό αυτών των δικτύων είναι πως τα δεδομένα εισόδου διαδίδονται μόνο προς την κατεύθυνση της έξοδου, χωρίς να υπάρχουν κύκλοι ή πλάγιες συνδέσεις. Μάλιστα, όταν τα δίκτυα αυτά, αποτελούνται από πολλά επίπεδα, πλήρως συνδεδεμένα μεταξύ τους -δηλαδή στην πλειοψηφία των περιπτώσεων-, τότε ονομάζονται και **Πολυεπίπεδα Νευρωνικά Δίκτυα** (*Multilayer Perceptrons - MLPs*). Παρά την απλή δομή τους, τα ενδιάμεσα επίπεδα των δικτύων αυτών ενδέχεται να είναι πολύπλοκα κι έτσι, τα μοντέλα αυτά χρησιμοποιούνται σε διάφορες εργασίες, όπως προβλήματα ταξινόμησης και παλινδρόμησης.

- Τα **Συνελικτικά Νευρωνικά Δίκτυα** (*Convolutional Neural Networks - CNNs*): Τα δίκτυα αυτά χρησιμοποιούν την πράξη της συνέλιξης για την επεξεργασία δεδομένων. Είναι ιδιαίτερα αποτελεσματικά στην αναγνώριση προτύπων ή εικόνων και την ανίχνευση αντικειμένων. Έτσι, καθίστανται εξαιρετικά χρήσιμα σε εφαρμογές υπολογιστικής όρασης. Τα CNNs αποτελούνται από τρία βασικά είδη επιπέδων: τα επίπεδα συνέλιξης, υπεύθυνα για την εξαγωγή χαρακτηριστικών από την είσοδο, τα επίπεδα υποδειγματοληψίας, υπεύθυνα για τη μείωση της διάστασης των προηγούμενων χαρακτηριστικών και τα πλήρως συνδεδεμένα επίπεδα, υπεύθυνα για την τελική ταξινόμηση των δεδομένων.

- Τα **Ανατροφοδοτούμενα Νευρωνικά Δίκτυα** (*Recurrent Neural Networks - RNNs*): Τα δίκτυα αυτά διακρίνονται από τους κύκλους ανάδρασης τους, επιτρέποντας έτσι την αποθήκευση πληροφορίας εντός του δικτύου. Με τον τρόπο αυτό, τα RNNs διατηρούν μία μνήμη των προηγούμενων εισόδων και μπορούν να χειριστούν επιτυχώς ακολουθιακά δεδομένα, δηλαδή περιπτώσεις όπου η εισερχόμενη πληροφορία είναι διαδοχικής φύσεως. Για παράδειγμα, χρησιμοποιούνται ευρέως σε εφαρμογές όπως η αναγνώριση ομιλίας, η πρόβλεψη χρονοσειρών και η επεξεργασία φυσικής γλώσσας.

- Τα **Παραγωγικά Δίκτυα Αντιπαλότητας** (*Generative Adversarial Networks - GANs*): Η βασική διαφοροποίηση των δικτύων αυτών, είναι πως ουσιαστικά, αποτελούνται από δύο ξεχωριστά νευρωνικά δίκτυα. Το πρώτο δίκτυο ονομάζεται «παραγωγός» (*generator*) και είναι υπεύθυνο για τη δημιουργία νέων εικόνων ή κειμένων με βάση ένα σύνολο δεδομένων εκπαίδευσης. Το δεύτερο δίκτυο ονομάζεται «κρίτης» και στόχος του είναι να κρίνει το έργο του παραγωγού, αποφασίζοντας εάν φαίνεται πραγματικό ή ψεύτικο. Η εκπαίδευση ολοκληρώνεται όταν ο κριτής δεν μπορεί να διακρίνει μεταξύ των δεδομένων εκπαίδευσης και των έργων του παραγωγού. Έτσι, τα GANs χρησιμοποιούνται για την παραγωγή νέου περιεχομένου, όπως κείμενο, εικόνες και βίντεο. 

Τέλος, αξίζει να σημειώθει πως οι έννοιες που αναλύθηκαν στις προηγούμενες παραγράφους περιγράφουν τη λειτουργία των πολυεπίπεδων νευρωνικών δικτύων (*MLPs*). Οι υπόλοιποι τύποι νευρωνικών δικτύων βασίζονται στις ίδιες αρχές και τρόπο λειτουργίας, αλλά, όπως είδαμε, έχουν και επιπλέον, ειδικές ιδιότητες και μηχανισμούς. Επιλέχθηκε να αναλυθεί πιο συγκεκριμένα η κατηγορία των MLPs, επειδή αποτελούν όχι μόνο τη βάση για την κατανόηση των γενικών αρχών των νευρωνικών δικτύων, αλλά και τον τύπο που χρησιμοποιήθηκε στα πλαίσια αυτής της εργασίας.

#### Σύνδεση με την Ενισχυτική Μάθηση {.unnumbered}

Έχοντας μελετήσει τις θεμελιώδεις αρχές και τον τρόπο λειτουργίας των τεχνητών νευρωνικών δικτύων, ας εξετάσουμε την χρήση τους σε προβλήματα ενισχυτικής μάθησης. Η κεντρική ιδέα είναι πως ο πράκτορας εκφράζεται ως ένα βαθύ νευρωνικό δίκτυο, το οποίο δέχεται ως είσοδο την κατάσταση του περιβάλλοντος και επιστρέφει την επιλεγμένη ενέργεια. Κατά τη διάρκεια της εκπαίδευσης, αναπροσαρμόζονται τα βάρη του νευρωνικού δικτύου, με στόχο την επίτευξη της μέγιστης συνολικής ανταμοιβής.

Ωστόσο, υπάρχουν μερικές διαφοροποιήσεις όσον αφορά την έξοδο του δικτύου και τον τρόπο εκπαίδευσής του, με βάση την κατηγορία του αλγορίθμου ενισχυτικής μάθησης που χρησιμοποιείται. Επομένως, ας εξετάσουμε πιο αναλυτικά τις διαφορές αυτές, για τις 3 κατηγορίες model free αλγορίθμων, που αναλύσαμε στην υποενότητα @sec:theory:reinforcement_learning:algorithm_types.

**Αλγόριθμοι Εκτίμησης Αξίας**

Οι πιο χρησιμοποιούμενοι αλγόριθμοι αυτής της κατηγορίας είναι οι οι αλγόριθμοι Εκτίμησης Αξίας Ζευγών Κατάστασης-Ενέργειας (αλγόριθμοι Q-Learning), όπως για παράδειγμα, ο αλγόριθμος DQN. Οι αλγόριθμοι αυτοί εφαρμόζονται σε περιβάλλοντα με διακριτό χώρο ενεργειών και χρησιμοποιούν ένα *δίκτυο αξίας* για την εκτίμηση των τιμών Q των ζευγών κατάστασης-ενέργειας.

- Έξοδος Δικτύου: η έξοδος του νευρωνικού δικτύου αποτελείται από τις τιμές Q για όλες τις διαθέσιμες ενέργειες στην τρέχουσα κατάσταση του περιβάλλοντος. Η επιλογή ενέργειας γίνεται με βάση κάποια τεχνική όπως η $\epsilon$-greedy, για την αντιμέτωπιση του διλήμματος εξερεύνησης-εκμετάλλευσης, όπως έχει αναφερθεί στην υποενότητα {@sec:theory:reinforcement_learning:q_learning}.
- Εκπαίδευση Δικτύου: η εκπαίδευση του δίκτυου γίνεται με βάση την ανταμοιβή που λαμβάνει ο πράκτορας από το περιβάλλον. Συγκεκριμένα, η συνάρτηση σφάλματος που χρησιμοποιείται, ονομάζεται Σφάλμα Χρονικών Διαφορών (*Temporal Difference Error - TD Error*) και είναι η διαφορά μεταξύ της εκτιμώμενης τιμής Q του δικτύου και της πραγματικής τιμής Q. Η πραγματική τιμή Q υπολογίζεται από την εξίσωση Bellman (\ref{eq:theory:reinforcement_learning:bellman}), χρησιμοποιώντας την τρέχουσα ανταμοιβή του πράκτορα από το περιβάλλον.

**Αλγόριθμοι Βελτιστοποίησης Πολιτικής**

Σε αυτούς τους αλγορίθμους χρησιμοποιείται ένα *δίκτυο πολιτικής*, δηλαδή
το νευρωνικό δίκτυο αντικατοπτρίζει απευθείας την πολιτική του πράκτορα.

- Έξοδος Δικτύου: 
    - Διακριτός Χώρος Ενεργειών: η έξοδος του δικτύου είναι μία κατανομή πιθανοτήτων στις διαθέσιμες ενέργειες π.χ. P(action~1~) = 0.5, P(action~2~) = 0.4, P(action~3~) = 0.1. Η επιλογή ενέργειας εξαρτάται από τον χαρακτηρισμό της πολιτικής ως ντετερμινιστική ή στοχαστική. Εάν η πολιτική είναι ντετερμινιστική, τότε θα επιλεγεί η ενέργεια με τη μεγαλύτερη πιθανότητα. Αντίθετα, εάν η πολιτική είναι στοχαστική, τότε η επιλογή της ενέργειας θα γίνει μέσω δειγματοληψίας από την κατανομή πιθανοτήτων.
    - Συνεχής χώρος Ενεργειών: η έξοδος του δικτύου, για κάθε ενέργεια, αποτελείται από 2 παραμέτρους μίας Γκαουσιανής κατανομής: τη μέση τιμή της ενέργειας (*μ*) και την τυπική απόκλιση της (*σ*). Για παράδειγμα, αν οι ενέργειες του πράκτορα ήταν η ταχύτητα και η γωνία ενός αυτοκινήτου, τότε η έξοδος του δικτύου θα ήταν της μορφής: μ(velocity), σ(velocity), μ(angle), σ(angle). Η επιλογή της συγκεκριμένης τιμής κάθε ενέργειας, εξαρτάται ξανά από την πολιτική του δικτύου. Αν η πολιτική είναι ντετερμινιστική, τότε θα επιλεγεί απλώς η μέση τιμή της ενέργειας. Αντίθετα, αν η πολιτική είναι στοχαστική, τότε η επιλογή της τιμής θα γίνει μέσω δειγματοληψίας από την κατανομή που περιγράφεται από τη μέση τιμή και την τυπική απόκλιση.
- Εκπαίδευση Δικτύου: η εκπαίδευση του δίκτυου γίνεται με βάση την ανταμοιβή που λαμβάνει ο πράκτορας από το περιβάλλον. Συγκεκριμένα, σε κάθε βήμα $t$ γίνεται μία εκτίμηση για την αθροιστική ανταμοιβή $G_{t}$ που θα λάβει ο πράκτορας: \begin{equation}
G_{t} = R_{t} + \gamma R_{t+1} + \gamma^{2} R_{t+2} + \ldots
\label{eq:theory:deep_reinforcement_learning:G}
\end{equation}όπου $R_{t}$ είναι η τρέχουσα ανταμοιβή και $\gamma$ ο παράγοντας έκπτωσης. Στόχος είναι η μεγιστοποίηση της συνάρτησης $G_{t}$ κι έτσι, υπολογίζεται η κλίση της ως προς τις παραμέτρους $\theta$ της πολιτικής. Η κλίση αυτή χρησιμοποιείται για την ενημέρωση των βαρών του δικτύου.

**Αλγόριθμοι Δράστη-Κριτή**

Στους αλγορίθμους αυτούς χρησιμοποιούνται δύο νευρωνικά δίκτυα, ο δράστης, υπεύθυνος για την επιλογή της ενέργειας και ο κριτής, υπεύθυνος για την εκτίμηση της αξίας της τρέχουσας κατάστασης ή της επιλεγμένης ενέργειας στην τρέχουσα κατάσταση.

- Έξοδος Δικτύων:
    - Δράστης: η έξοδος του δράστη αποτελεί την πολιτική του πράκτορα. Για την έξοδο του δικτύου και την τελική επιλογή ενέργειας, ισχύουν τα όσα περιγράφησαν νωρίτερα, στους αλγορίθμους βελτιστοποίησης πολιτικής.
    - Κριτής: η έξοδος του κριτή αποτελεί την εκτίμηση μίας αξίας. Σε ορισμένους αλγορίθμους, αυτή η αξία αναφέρεται στην τρέχουσα κατάσταση $V(s)$, ενώ σε άλλους αλγορίθμους αναφέρεται στο ζεύγος τρέχουσας κατάστασης-επιλεγμένης ενέργειας $Q(s,a)$.
- Εκπαίδευση Δικτύων: η εκπαίδευση γίνεται και σε αυτήν την περίπτωση, με βάση την ανταμοιβή του περιβάλλοντος. Η διαδικασία αποτυπώνεται παραστατικά στην *Εικόνα @fig:theory:deep_reinforcement_learning:actor_critic*. Συγκεκριμένα, βλέπουμε πως ο κριτής δέχεται ως είσοδο την τρέχουσα κατάσταση και ανταμοιβή του περιβάλλοντος. Έτσι, υπολογίζει το σφάλμα χρονικών διαφορών (*TD Error*), όπως περιγράφηκε νωρίτερα, στους αλγορίθμους εκτίμησης αξίας. Το σφάλμα χρονικών διαφορών χρησιμοποιείται για την ενημέρωση των βαρών του κριτή και του δράστη, όπως φαίνεται από τις καμπύλες που διαπερνούν τα δίκτυα αυτά στην *Εικόνα @fig:theory:deep_reinforcement_learning:actor_critic*. Με τον τρόπο αυτό, ο κριτής εκπαιδεύει το δίκτυο του, ενώ παρέχει και ανάδραση στον δράστη, η οποία χρησιμοποιείται για την εκπαίδευση του.

![Αλληλεπίδραση δικτύων Δράστη-Κριτή [@Sutton2018].](3-theory/figures/ActorCritic.png){#fig:theory:deep_reinforcement_learning:actor_critic width=60%}

$\\$
$\\$
$\\$
$\\$
$\\$
$\\$
$\\$
$\\$
$\\$
$\\$
$\\$
$\\$
$\\$

Έχοντας πλέον, αναλύσει τα βασικά χαρακτηριστικά της βαθιάς ενισχυτικής μάθησης, καθώς και τον τρόπο λειτουργίας των νευρωνικών δικτύων σε αυτό το πλαίσιο, ας εξετάσουμε τους αλγορίθμους βαθιάς ενισχυτικής μάθησης που χρησιμοποιήθηκαν στην παρούσα εργασία. Συγκεκριμένα, πραγματοποιήθηκαν εκπαιδεύσεις πρακτόρων με τους εξής αλγόριθμους:

- PPO
- DDPG
- TD3
- SAC

### Ο αλγόριθμος PPO {#sec:theory:deep_reinforcement_learning:ppo}

Ο αλγόριθμος PPO (*Proximal Policy Optimization*) είναι ένας αποτελεσματικός αλγόριθμος ενισχυτικής μάθησης, που ανήκει στην κατηγορία των αλγορίθμων βελτιστοποίησης πολιτικής. Αναπτύχθηκε το 2017 από την εταιρία OpenAI [@schulman2017] και ξεχωρίζει για την απλότητά του, τη σταθερότητα του και τις ισχυρές επιδόσεις που πετυχαίνει. Σήμερα, αποτελεί έναν από τους πιο δημοφιλείς αλγορίθμους ενισχυτικής μάθησης. Αυτό αποδεικνύεται και από την *Εικόνα @fig:theory:deep_reinforcement_learning:Algoritmhs-usage*, όπου παρουσιάζεται η συχνότητα των επιστημονικών δημοσιεύσεων των αλγορίθμων ενισχυτικής μάθησης που χρησιμοποιήθηκαν στην παρούσα εργασία, σε σχέση με τον χρόνο. Παρατηρούμε πως ο αλγόριθμος PPO έχει πολύ μεγαλύτερη συχνότητα χρήσης, σε σχέση με τους υπόλοιπους αλγορίθμους.

![Συχνότητα χρήσης αλγορίθμων ενισχυτικής μάθησης σε επιστημονικές δημοσιεύσεις [@paperswithcode].](3-theory/figures/Algorithms-usage.png){#fig:theory:deep_reinforcement_learning:Algoritmhs-usage width=100%}

Επίσης, στην *Εικόνα @fig:theory:deep_reinforcement_learning:PPO-usage* παρουσιάζονται οι τύποι των προβλημάτων στα οποία χρησιμοποιείται ο αλγόριθμος PPO. Παρατηρούμε πως ο πιο συνήθης τύπος είναι τα προβλήματα ενισχυτικής μάθησης, ακολουθούμενα από τα προβλήματα αυτόνομης οδήγησης και αυτόνομων οχημάτων. Άρα, καταλαβαίνουμε πως ο αλγόριθμος PPO θα είναι κατάλληλος για την επίλυση του προβλήματος αυτόματης στάθμευσης, που αντιμετωπίζουμε στην παρούσα εργασία.

![Τύποι προβλημάτων αλγορίθμου PPO [@paperswithcode] [^papersWithCode].](3-theory/figures/PPO-usage.png){#fig:theory:deep_reinforcement_learning:PPO-usage width=100%}

[^papersWithCode]: Αξίζει να σημειωθεί πως τα δύο προηγούμενα γραφήματα προέρχονται από τη σελίδα [paperswithcode](https://paperswithcode.com/), η οποία παρέχει χρήσιμες πληροφορίες σχετικά με αλγορίθμους μηχανικής μάθησης, όπως επιστημονικές δημοσιεύσεις, κώδικα, αποτελέσματα κ.ά.

Ο PPO βελτιστοποιεί την πολιτική του πράκτορα, μέσω της μεγιστοποίησης μιας αντικειμενικής συνάρτησης. Η αντικειμενική συνάρτηση αυτή είναι «περικομμένη» (*clipped*), δηλαδή έχει ένα κατώφλι που περιορίζει την αλλαγή της πολιτικής σε κάθε βήμα. Έτσι, αποφεύγονται μεγάλες, απότομες αλλαγές της πολιτικής και διασφαλίζεται η σταθερότητα της εκπαίδευσης. Η αντικειμενική συνάρτηση του αλγορίθμου PPO παρουσιάζεται στην εξίσωση \ref{eq:theory:deep_reinforcement_learning:ppo_objective}, και οι όροι της αναλύονται παρακάτω.
\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_{t} \left[ \min \left( r_{t}(\theta) \hat{A}_{t}, \text{clip}(r_{t}(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_{t} \right) \right]
\label{eq:theory:deep_reinforcement_learning:ppo_objective}
\end{equation}

- $r_{t}(\theta)$ αποτελεί το λόγο της νέας πολιτικής προς την παλιά και ισούται με $r_{t}(\theta) = \frac{\pi_{\theta}(a_{t} | s_{t})}{\pi_{\theta_{old}}(a_{t} | s_{t})}$.
- $\hat{A}_{t}$ είναι η συνάρτηση πλεονεκτήματος, η οποία εκτιμά το πόσο καλύτερη είναι μια ενέργεια σε μία συγκεκριμένη κατάσταση σε σχέση με τη μέση ενέργεια
- $\text{clip}(r_{t}(\theta), 1-\epsilon, 1+\epsilon)$ είναι η συνάρτηση περικοπής, η οποία διασφαλίζει ότι η πολιτική δεν θα υποστεί δραστικές αλλαγές, περιορίζοντας το $r_{t}(\theta)$ στο εύρος $[1-\epsilon, 1+\epsilon]$. 
- $\epsilon$ είναι το κατώφλι που περιορίζει την αλλαγή της πολιτικής. Αποτελεί υπερ-παράμετρο του αλγορίθμου και παίρνει μικρές θετικές τιμές (συνήθως 0.2).
- Η συνάρτηση $\min$ χρησιμοποιείται για την επιλογή του ελάχιστου μεταξύ της μη-περικομμένης και περικομμένης αντικειμενικής συνάρτησης. Έτσι, η τελική αντικειμενική συνάρτηση είναι ένα κάτω φράγμα (δηλαδή μία απαισιόδοξη εκτίμηση) της μη-περικομμένης αντικειμενικής συνάρτησης.

Επίσης, συχνά προστίθεται κι ένας όρος εντροπίας στην αντικειμενική συνάρτηση, ο οποίος ενθαρρύνει τον πράκτορα να εξερευνήσει. Έτσι, διατηρείται μία ποικιλία στις ενέργειες που επιλέγονται από την πολιτική, στα πρώτα στάδια της εκπαίδευσης.

Η μεθοδολογία του αλγορίθμου PPO δίνεται παράκατω υπό μορφή ψευδοκώδικα.

![](3-theory/figures/PPO-pseudocode.pdf){}
Ένα σημαντικό χαρακτηριστικό της εκπαίδευσης του αλγορίθμου PPO, είναι πως δεν εκτελεί μόνο μία ενημέρωση για κάθε δεδομένο που συλλέγεται. Αντίθετα, πραγματοποιεί πολλαπλές εποχές ενημερώσεων, χρησιμοποιώντας τα ίδια συλλεγμένα δεδομένα. Αυτό καθιστά τον αλγόριθμο πιο αποδοτικό στη χρήση δειγμάτων, σε σύγκριση με άλλες μεθόδους βελτιστοποίησης πολιτικής. Ακόμα, η εκπαίδευση γίνεται on-policy, ενώ χρησιμοποιείται μία στοχαστική πολιτική.

Όσον αφορά την αρχιτεκτονική του νευρωνικού δικτύου, στην αρχική δημοσίευση των [@schulman2017], χρησιμοποιήθηκε ένα πλήρως συνδεδεμένο MLP με δύο κρυφά επίπεδα των 64 νευρώνων και συνάρτηση ενεργοποίησης Tanh.

Με αυτούς τους τρόπους, ο PPO πετυχαίνει την ανθεκτικότητα και την αξιοπιστία παλαιοτέρων αλγορίθμων βελτιστοποίησης πολιτικής, όπως ο TRPO, αλλά είναι πολύ απλούστερος, ευκολότερος στην υλοποίηση και πετυχαίνει συνολικά, υψηλότερες επιδόσεις. Επιπλέον, είναι αποδοτικός στη χρήση δειγμάτων κι έτσι, μειώνεται το πλήθος των απαιτούμενων αλληλεπιδράσεων με το περιβάλλον. Τέλος, αξιοσημειώτη είναι η ευελιξία του αλγορίθμου, καθώς μπορεί να εφαρμοστεί τόσο σε διακριτούς, όσο και σε συνεχείς χώρους ενεργειών.

### Ο αλγόριθμος DDPG {#sec:theory:deep_reinforcement_learning:ddpg}

Ο αλγόριθμος DDPG (*Deep Deterministic Policy Gradient*) είναι ένας αλγόριθμος βαθιάς ενισχυτικής μάθησης, που ανήκει στην κατηγορία των αλγορίθμων δράστη-κριτή. Αναπτύχθηκε το 2015 από ερευνητές της εταιρίας DeepMind της Google [@lillicrap2015]. Ο DDPG συνδυάζει στοιχεία των αλγορίθμων βελτιστοποίησης πολιτικής, καθώς και του αλγορίθμου εκτίμησης αξίας DQN. Είναι σχεδιασμένος για περιβάλλοντα με συνεχείς χώρους καταστάσεων και ενεργειών και επομένως, μπορεί να θεωρηθεί ως η εφαρμογή του αλγορίθμου DQN σε συνεχείς χώρους ενεργειών.

Ο DDPG, χρησιμοποιεί ένα δίκτυο δράστη για την επιλογή της ενέργειας. Τα βάρη του δικτύου αυτού συμβολίζονται ως $\theta^{\mu}$. Ακόμα, όπως φανερώνει και το όνομα του αλγορίθμου, ο DDPG χρησιμοποιεί ντετερμινιστική πολιτική. Αυτό σημαίνει, πως από το συνεχές εύρος τιμών της κάθε ενέργειας, ο δράστης δεν επιλέγει μία μέση τιμή $\mu(s_{t})$ και μία τυπική απόκλιση $\sigma(s_{t})$, αλλά επιλέγει απευθείας μία τιμή $\mu(s_{t})$. Ωστόσο, ένα σημαντικό χαρακτηριστικό του αλγορίθμου DDPG, είναι πως προσθέτει θόρυβο ($N_{t}$) στην ενέργεια που επιλέγει ο δράστης, για να ενθαρρύνει την εξερεύνηση του περιβάλλοντος. Ο θόρυβος αυτός συνήθως ακολουθεί κατανομή Gauss ή Ornstein-Uhlenbeck. Επομένως, η ενέργεια που επιλέγει ο δράστης δίνεται από την εξίσωση \ref{eq:theory:deep_reinforcement_learning:ddpg_action}:
\begin{equation}
a_{t} = \mu(s_{t} | \theta^{\mu}) + N_{t}
\label{eq:theory:deep_reinforcement_learning:ddpg_action}
\end{equation}
Επιπλέον, χρησιμοποιείται ένα δίκτυο κριτή για την εκτίμηση της αξίας των ζευγών κατάστασης-ενέργειας, δηλαδή των τιμών $Q$. Τα βάρη του δικτύου αυτού συμβολίζονται ως $\theta^{Q}$. 

Ένα ακόμα χαρακτηριστικό του αλγορίθμου είναι η εκπαίδευση off policy. Συγκεκριμένα, αποθηκεύονται προηγούμενες εμπειρίες της μορφής $(s_{t}, a_{t}, R_{t}, s_{t+1})$ σε μία προσωρινή μνήμη που ονομάζεται *replay buffer*. Κατά την εκπαίδευση, μερικές φορές χρησιμοποιούνται τυχαία δείγματα από τον replay buffer, αντί για τα δεδομένα που συλλέγονται εκείνη τη χρονική στιγμή. Με αυτόν τον τρόπο, περιορίζονται οι συσχετίσεις μεταξύ διαδοχικών δεδομένων και επιτυγχάνεται μεγαλύτερη σταθερότητα στην εκπαίδευση.

Μία άλλη, σημαντική ιδιότητα του αλγορίθμου DDPG είναι η κανονικοποίηση τεμαχίων (*batch normalization*). Αυτή η τεχνική χρησιμοποιείται για την κανονικοποίηση των εισόδων στα δίκτυα δράστη και κριτή, έτσι ώστε να έχουν μονάδική μέση τιμή και διακύμανση. Αυτό βοηθά στην αύξηση της σταθερότητάς της εκπαίδευσης.

Τέλος, αξιοσημείωτη αποτελεί η χρήση δύο δικτύων στόχου (*target networks*), ένα για τον δράστη και ένα για τον κριτή. Τα δίκτυα αυτά έχουν βάρη $\theta^{\mu'}$ και $\theta^{Q'}$ αντίστοιχα, τα οποία ενημερώνονται αργά, με βάση τα βάρη των κύριων δικτύων. Μάλιστα, χρησιμοποιείται ένας παράγοντας $\tau$ που ελέγχει τον ρυθμό ενημέρωσης των βαρών των δικτύων στόχων και παίρνει μικρές τιμές (συνήθως $\tau=0.001$). Με τον τρόπο αυτό, οι ενημερώσεις γίνονται πιο ομαλές και αποφεύγονται μεγάλες ταλαντώσεις κατά την εκπαίδευση.

Τα δίκτυα στόχου χρησιμοποιούνται στον υπολογισμό της τιμής $Q$ στόχου ($y_{t}$), ο οποίος γίνεται μέσα από την εξίσωση \ref{eq:theory:deep_reinforcement_learning:ddpg_target_Q}:
\begin{equation}
y_{t} = R_{t} + \gamma Q'(s_{t+1}, \mu'(s_{t+1} | \theta^{\mu'}) | \theta^{Q'}) 
\label{eq:theory:deep_reinforcement_learning:ddpg_target_Q}
\end{equation}
Επομένως, το δίκτυο του κριτή ενημερώνεται μέσω της ελαχιστοποίησης του τετραγώνου της διαφοράς, μεταξύ της πρόβλεψης $Q$ του κριτή και της τιμής στόχου $y_{t}$. Η αντίστοιχη συνάρτηση σφάλματος, παρουσιάζεται στην εξίσωση \ref{eq:theory:deep_reinforcement_learning:ddpg_loss_Q}:
\begin{equation}
L(\theta^{Q}) = \frac{1}{N} \sum_{t} (y_{t} - Q(s_{t}, a_{t} | \theta^{Q}))^{2}
\label{eq:theory:deep_reinforcement_learning:ddpg_loss_Q}
\end{equation}
Αντίστοιχα, το δίκτυο του δράστη ενημερώνεται μέσω της μέγιστοποίησης της αναμενόμενης ανταμοιβής, όπως παρουσιάζεται στην εξίσωση \ref{eq:theory:deep_reinforcement_learning:ddpg_loss_mu}:
\begin{equation}
\nabla_{\theta^\mu} J \approx \frac{1}{N} \sum_{i} \nabla_a Q(s, a \mid \theta^Q) \bigg|_{s=s_i, a=\mu(s_i)} \nabla_{\theta^\mu} \mu(s \mid \theta^\mu) \bigg|_{s=s_i}
\label{eq:theory:deep_reinforcement_learning:ddpg_loss_mu}
\end{equation}
Με βάση αυτά, η μεθοδολογία του αλγορίθμου DDPG δίνεται και στην επόμενη σελίδα, υπό μορφή ψευδοκώδικα.

Συνολικά, η προσέγγιση του αλγορίθμου DDPG είναι απλή, εύκολη στην υλοποίηση και επεκτάσιμη σε δύσκολα προβλήματα, μεγάλων διαστάσεων. Ακόμα, οι τεχνικές που χρησιμοποιεί, όπως τα δίκτυα στόχων και ο replay buffer, βελτιώνουν σε μεγάλο βαθμό την ανθεκτικότητα της μάθησης.

Ωστόσο, οι ίδιες τεχνικές προκαλούν και μερικές αδυναμίες του αλγορίθμου, όπως η αργή σύγκλιση και η ανάγκη για μεγάλο αριθμό επαναλήψεων. Επιπλέον, πρόκληση αποτελεί η εξερεύνηση του περιβάλλοντος, καθώς η ντετερμινιστική πολιτική του αλγορίθμου μπορεί να οδηγήσει σε τοπικά ακρότατα. Ο προστιθέμενος θόρυβος βοηθάει σε αυτό το πρόβλημα, αλλά συχνά δεν αρκεί για να επιτευχθεί η απαιτούμενη εξερεύνηση. 

![Ψευδοκώδικας του αλγορίθμου DDPG.](3-theory/figures/DDPG-pseudocode.pdf){ #fig:theory:deep_reinforcement_learning:DDPGcode}

### Ο αλγόριθμος TD3 {#sec:theory:deep_reinforcement_learning:td3}

Ο αλγόριθμος TD3 (*Twin Delayed Deep Deterministic policy gradient*) αποτελεί μία βελτιωμένη έκδοση του αλγορίθμου DDPG, η οποία αναπτύχθηκε το 2018 από ερευνητές των πανεπιστημίου του Montreal και του Amsterdam [@fujimoto2018]. Είναι σχεδιασμένος για περιβάλλοντα με συνεχείς χώρους καταστάσεων και ενεργειών, ενώ στόχος του είναι να αντιμετωπίσει μερικά από τα μειονεκτήματα του DDPG, όπως την υπερεκτίμηση της συνάρτησης αξίας και την αστάθεια κατά την εκπαίδευση. Ο TD3 πετυχαίνει αυτόν τον στόχο, μέσω της εισαγωγής τριών βασικών τροποποιήσεων: της εξομάλυνσης της πολιτικής στόχου (*target policy smoothing*), της χρήσης μίας περικομμένης εκδοχής του αλγορίθμου Διπλού Q-learning (*clipped Double Q-learning*) και της καθυστέρησης της ενημέρωσης της πολιτικής (*delayed policy updates*).

Όπως και ο προκάτοχός του, ο TD3 ανήκει στην κατηγορία των αλγορίθμων δράστη-κριτή. Μάλιστα, χρησιμοποιεί ένα δίκτυο δράστη και δύο δίκτυα κριτή -εξού και ο χαρακτηρισμός *Twin* (Δίδυμος) στο όνομα του αλγορίθμου-. Οι παράμετροι του δικτύου δράστη συμβολίζονται ως $\phi$ και τα βάρη των δικτύων κριτή ως $\theta_{1}$ και $\theta_{2}$. 

Η πρώτη τροποποίηση του αλγορίθμου είναι η εξομάλυνση της πολιτικής στόχου (*target policy smoothing*). Η τεχνική αυτή εισάγεται στον υπολογισμό της ενέργειας στόχου $\tilde{a}$, η οποία θα χρησιμοποιηθεί στη συνέχεια, στον υπολογισμό της τιμής $Q$ στόχου. Συγκεκριμένα, στον υπολογισμό της $\tilde{a}$, προστίθεται ένας μικρός βαθμός θορύβου ($\epsilon$), στην ενέργεια που επέλεξε ο δράστης. Ο θόρυβος περιορίζεται σε ένα εύρος $[-c, c]$, όπου $c$ είναι μία σταθερά, ώστε η ενέργεια στόχου να μην αποκλίνει υπερβολικά από την ενέργεια που επέλεξε ο δράστης. Η διαδικασία αυτή φαίνεται στην εξίσωση \ref{eq:theory:deep_reinforcement_learning:td3_target_policy_smoothing}:
\begin{equation}
\tilde{a} \sim \pi_{\phi'}(s') + \epsilon, \epsilon \sim \text{clip}(N(0, \tilde{\sigma}), -c, c)
\label{eq:theory:deep_reinforcement_learning:td3_target_policy_smoothing}
\end{equation}
Με αυτόν τον τρόπο, μειώνεται η διασπορά στις εκτιμήσεις των τιμών $Q$ και αποτρέπεται η πολιτική από την εκμετάλλευση πιθανών αιχμών της συνάρτησης $Q$, που συνήθως οφείλονται σε σφάλματα υπερεκτίμησης από τα δίκτυα κριτή. 

Η δεύτερη τροποποίηση του αλγορίθμου αφορά το πρόβλημα της υπερεκτίμησης των τιμών $Q$ (*overestimation bias*). Το πρόβλημα αυτό, εμφανίζεται συχνά στους αλγορίθμους εκτίμησης αξίας και περισσότερο, όταν χρησιμοποιούνται συναρτήσεις προσέγγισης, όπως τα νευρωνικά δίκτυα. Το αποτέλεσμα του, είναι η υιοθέτηση υποβέλτιστων πολιτικών από τον πράκτορα. Τέτοια σφάλματα, εμφανίζονται και στην περίπτωση των αλγορίθμων δράστη-κριτή, αφού στους συγκεκριμένους αλγόριθμους, η πολιτική ενημερώνεται με βάση την εκτίμηση των τιμών $Q$. Ο αλγόριθμος TD3 επιδιώκει να ελαχιστοποιήσει το πρόβλημα της υπερεκτίμησης, χρησιμοποιώντας μία περικομμένη (*clipped*) εκδοχή του αλγορίθμου *Double Q-learning*.  Σύμφωνα με την εκδοχή αυτή, η τιμή $Q$ στόχου ($y$) υπολογίζεται από την εξίσωση \ref{eq:theory:deep_reinforcement_learning:td3_y}:
\begin{equation}
y = R + \gamma \min_{i=1,2} Q_{\theta'_i}(s', \tilde{a})
\label{eq:theory:deep_reinforcement_learning:td3_y}
\end{equation}
Επομένως, η τιμή $Q$ στόχου υπολογίζεται ως το ελάχιστο από τις τιμές $Q$ που επιστρέφουν τα δύο δίκτυα κριτή. Αυτός ο κανόνας, μπορεί να προκαλέσει το αντίθετο φαινόμενο, την υποεκτίμηση των τιμών $Q$. Ωστόσο, αυτό είναι σαφώς προτιμότερο από την υπερεκτίμηση, καθώς οι τιμές των υποεκτιμημένων ενεργειών δεν θα μεταδοθούν κατά τη μάθηση, αφού οι ενέργειες με μικρές τιμές $Q$ αποφεύγονται από την πολιτική.

Με βάση την τιμή $Q$ στόχου ($y$), τα δίκτυα κριτή ενημερώνουν τις παραμέτρους τους $\theta_{1}$ και $\theta_{2}$, μέσω της ελαχιστοποίησης του τετραγώνου της διαφοράς μεταξύ της πρόβλεψης του κριτή και της τιμής $Q$ στόχου. Η αντίστοιχη συνάρτηση σφάλματος παρουσιάζεται στην εξίσωση \ref{eq:theory:deep_reinforcement_learning:td3_loss_Q}:
\begin{equation}
\theta_{i} = \arg \min_{\theta_{i}} \frac{1}{N} \sum (y - Q_{\theta_{i}}(s, a))^{2}
\label{eq:theory:deep_reinforcement_learning:td3_loss_Q}
\end{equation}
Η τελευταία τροποποίηση του TD3 είναι η καθυστέρηση της  ενημέρωσης της πολιτικής. Συγκεκριμένα, το δίκτυο του δράστη (και κατ' επέκταση το δίκτυο στόχου του δράστη) δεν ενημερώνεται σε κάθε βήμα της εκπαίδευσης, αλλά ανά $d$ βήματα (προτείνεται $d=2$). Αυτή η καθυστέρηση βοηθά στη σταθεροποίηση της διαδικασίας μάθησης, επιτρέποντας στα δίκτυα κριτή να συγκλίνουν καλύτερα, πριν γίνει η ενημέρωση της πολιτικής. Έτσι, οι λιγότερο συχνές ενημερώσεις της πολιτικής. θα χρησιμοποιούν μία εκτίμηση της τιμής $Q$ με μικρότερη διακύμανση και συνεπώς, θα αποτελούν ενημερώσεις υψηλότερης ποιότητας. Οι ενημερώσεις αυτές της πολιτικής, δίνονται στην εξίσωση \ref{eq:theory:deep_reinforcement_learning:td3_policy_update}:
\begin{equation}
\nabla_{\phi} J(\phi) = \frac{1}{N} \sum \nabla_{a} Q_{\theta_1}(s, a)\big|_{a=\pi_{\phi}(s)} \nabla_{\phi} \pi_{\phi}(s)
\label{eq:theory:deep_reinforcement_learning:td3_policy_update}
\end{equation}
Με βάση τη διαδικασία που περιγράφηκε προηγουμένως, η μεθοδολογία του αλγορίθμου TD3 δίνεται παράκατω υπό μορφή ψευδοκώδικα.

![](3-theory/figures/TD3-pseudocode.pdf){}
Συνολικά, οι τροποποιήσεις του αλγορίθμου TD3 είναι απλές και εύκολες στην υλοποίηση, ενώ παίζουν καθοριστικό ρόλο στην αύξηση της απόδοσης σε σχέση με τον αλγόριθμο DDPG. Συγκεκριμένα, οι αλλαγές του TD3 μειώνουν την αστάθεια που μπορεί να εμφανιστεί στον DDPG, προωθούν την ανθεκτικότητα της εκπαίδευσης και τελικά, οδηγούν σε πιο αξιόπιστες πολιτικές.

Παρόλα αυτά, ο αλγόριθμος είναι πιο πολύπλοκος από τον DDPG, με χαρακτηριστικά παραδείγματα την χρήση δύο δικτύων κριτή και την καθυστέρηση της ενημέρωσης της πολιτικής. Επομένως, το υπολογιστικό κόστος του αλγορίθμου είναι υψηλότερο, ενώ η εκπαίδευση του μπορεί να απαιτεί περισσότερο χρόνο. Ακόμα, οι νέες τροποποιήσεις εισάγουν περισσότερες υπερ-παραμέτρους, οι οποίες πρέπει να ρυθμιστούν με προσοχή, ώστε να μην επηρεάσουν αρνητικά την απόδοση του αλγορίθμου. 

### Ο αλγόριθμος SAC {#sec:theory:deep_reinforcement_learning:sac}

Ο αλγόριθμος SAC (*Soft Actor-Critic*) είναι ένας αλγόριθμος βαθιάς ενισχυτικής μάθησης, που ανήκει στην κατηγορία των αλγορίθμων δράστη-κριτή. Αναπτύχθηκε το 2018 από ερευνητές του πανεπιστημιού του Berkeley και της Google [@haarnoja2018]. Αποτελεί μία επέκταση των παραδοσιακών μεθόδων δράστη-κριτή, που ενσωματώνει την κανονικοποίηση της εντροπίας στην αντικειμενική συνάρτηση, κάνοντας την πολιτική στοχαστική και ενθαρρύνοντας την εξερεύνηση. Εφαρμόζεται σε περιβάλλοντα με συνεχείς χώρους καταστάσεων και ενεργειών. Ξεχωρίζει για την αποδοτικότητα του στη χρήση δειγμάτων, την ανθεκτικότητα της εκπαίδευσης του και τις επιδόσεις του σε πολύπλοκα προβλήματα, όπως η πλοήγηση ρομπότ.

Ο SAC βασίζεται στο πλαίσιο της μέγιστης εντροπίας της ενισχυτικής μάθησης. Σε αυτό το πλαίσιο, ο δράστης στοχεύει στη μεγιστοποίηση της αναμενόμενης ανταμοιβής, ενώ ταυτόχρονα μεγιστοποιεί την εντροπία. Με άλλα λόγια, ο δράστης προσπαθεί να επιτύχει στην εργασία του, ενεργώντας όσο το δυνατόν πιο τυχαία. Με τον τρόπο αυτό, ο αλγόριθμος επιχειρεί να πετυχεί μία ισορροπία στο δίλημμα εξερεύνησης-εκμετάλλευσης. Το πλαίσιο αυτό, εμφανίζεται και στην αντικειμενική συνάρτηση του αλγορίθμου SAC, η οποία παρουσιάζεται στην εξίσωση \ref{eq:theory:deep_reinforcement_learning:sac_objective}:
\begin{equation}
J(\pi) = \mathbb{E}_{\pi} \left[ \sum_{t} R(s_t, a_t) - \alpha \log(\pi(a_t | s_t)) \right]
\label{eq:theory:deep_reinforcement_learning:sac_objective}
\end{equation}
Οι όροι της αντικειμενικής συνάρτησης αναλύονται παρακάτω:

- Ο όρος $\sum_{t} R(s_t, a_t)$ αποτελεί την αναμενόμενη αθροιστική ανταμοιβή του περιβάλλοντος. Παρατηρούμε ότι ο πράκτορας στοχεύει στη μεγιστοποίηση της.
- Ο όρος $- \alpha \log(\pi(a_t | s_t))$ αποτελεί την εντροπία της πολιτικής, την οποία ο πράκτορας επίσης στοχεύει να μεγιστοποιήσει.
- Ο όρος $\alpha$ είναι η παράμετρος θερμοκρασίας (*temperature parameter*), η οποία καθορίζει την επίδραση του όρου εντροπίας έναντι του όρου της ανταμοιβής και συνεπώς, ελέγχει τη στοχαστικότητα της βέλτιστης πολιτικής. Η κλασική, μέγιστη αναμενόμενη ανταμοιβή, των συμβατικών αλγορίθμων ενισχυτικής μάθησης, ανακτάται στο όριο όταν $\alpha \rightarrow 0$.

Προκειμένου να μεγιστοποιήσει την παραπάνω αντικειμενική συνάρτηση, ο αλγόριθμος SAC εκπαιδεύει πέντε διαφορετικά νευρωνικά δίκτυα:

- 1 δίκτυο δράστη, το οποίο αντιπροσωπεύει την πολιτική του πράκτορα $\pi(a_t | s_t)$,
- 2 δίκτυα κριτή, τα οποία εκτιμούν την αξία των ζευγών κατάστασης-ενέργειας, δηλαδή τις τιμές $Q(s_t, a_t)$ και
- 2 δίκτυα κριτή στόχου (*target networks*), τα οποία αποτελούν καθυστερημένα αντίγραφα των δικτύων κριτή και χρησιμοποιούνται στον υπολογισμό του σφάλματος, κατά την εκπαίδευση των δικτύων κριτή. Τα δίκτυα αυτά συμβάλουν σημαντικά στη σταθεροποίηση της εκπαίδευσης.

Ακόμα, ο αλγόριθμος SAC εκτελείται off-policy, δηλαδή τα δεδομένα που συλλέγονται από την αλληλεπίδραση με το περιβάλλον, αποθηκεύονται σε μία μνήμη (*replay buffer*) και μπορούν να χρησιμοποιηθούν πολλές φορές κατά την εκπαίδευση.

Αναλυτικότερα, η μεθοδολογία που ακολουθεί ο αλγορίθμος SAC δίνεται παράκατω υπό μορφή ψευδοκώδικα. Έχουν προστεθεί σχόλια στον ψευδοκώδικα, προκειμένου να γίνει πιο κατανοητή η λειτουργία του αλγορίθμου. 

![](3-theory/figures/SAC-pseudocode.pdf){}
Συνολικά, ο SAC διαθέτει πολλά στοιχεία στον τρόπο λειτουργίας του, τα οποία δημιουργούν σημαντικά πλεονεκτήματα σε σχέση με άλλους αλγορίθμους.

Αρχικά, η ιδιότητα του SAC ως off policy αλγορίθμου και η χρήση του replay buffer, τον καθιστά ιδιαίτερα αποδοτικό στη χρήση δειγμάτων και μειώνει τον αριθμό των απαιτούμενων αλληλεπιδράσεων με το περιβάλλον. Επιπλέον, η εκπαίδευση γίνεται με τη χρήση μιας στοχαστικής πολιτικής, η οποία ενθαρρύνει τον πράκτορα να εξερευνήσει το περιβάλλον.

Επίσης, όπως αναφέρθηκε και παραπάνω, η χρήση της εντροπίας στην αντικειμενική συνάρτηση, βοηθάει στην αποδοτική εξερεύνηση του περιβάλλοντος από τον πράκτορα και αποτρέπει την πρόωρη σύγκλιση της πολιτικής του. Αυτό είναι κρίσιμο σε περιβάλλοντα με αραιές ανταμοιβές, όπως το πρόβλημα της αυτόματης στάθμευσης.

Ακόμα, η χρήση διαφορετικών νευρωνικών δικτύων για την εκτιμήση των τιμών Q, συμβάλλει στην αποφυγή της υπερεκτίμησης των τιμών τους, το οποίο αποτελεί συχνό πρόβλημα σε τέτοιου είδους μεθόδους. Έτσι, ο αλγόριθμος επιτυγχάνει πιο σταθερή και αξιόπιστη μάθηση.

Ωστόσο, ο αλγόριθμος SAC παρουσιάζει και ορισμένα μειονέκτηματα. Το βασικότερο από αυτά αποτελεί η πολυπλοκότητα του, η οποία οδηγεί σε αυξημένο υπολογιστικό κόστος. Συγκεκριμένα, η ανάγκη ενημέρωσης πολλαπλών νευρωνικών δικτύων για τη συνάρτηση αξίας, επιβαρύνει σημαντικά το υπολογιστικό έργο.

Τέλος, αδυναμία του αλγορίθμου είναι η ευαισθησία στην κλίμακα των ανταμοιβών (*reward scale*).  Συγκεκριμένα, για μικρές τιμές των ανταμοιβών, ο πράκτορας αποτυγχάνει να αξιοποιήσει το σήμα ανταμοιβής κι έτσι, υποβαθμίζεται σημαντικά η απόδοση του. Αντίθετα, για μεγάλες τιμές ανταμοιβών, ο πράκτορας μαθαίνει γρήγορα στην αρχή, αλλά συχνά συγκλίνει σε τοπικά ελάχιστα, λόγω έλλειψης επαρκούς εξερεύνησης. Επομένως, είναι απαραίτητη η σωστή κλιμάκωση των ανταμοιβών, ώστε ο πράκτορας να ισορροπήσει την εξερεύνηση και την εκμετάλλευση και να πετύχει τελικά, καλύτερη απόδοση. Παρόλα αυτά, είναι θετικό πως η κλίμακα των ανταμοιβών αποτελεί τη μόνη υπερ-παράμετρο, που απαιτεί στην πράξη, προσεκτική ρύθμιση από τον σχεδιαστή του συστήματος.